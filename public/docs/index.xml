<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Docs on XAI</title><link>http://localhost:1313/docs/</link><description>Recent content in Docs on XAI</description><generator>Hugo</generator><language>en-us</language><atom:link href="http://localhost:1313/docs/index.xml" rel="self" type="application/rss+xml"/><item><title>CAM and SeCAM</title><link>http://localhost:1313/docs/groups/cam_and_secam/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/cam_and_secam/</guid><description>&lt;h1 id="cam-and-secam-explainable-ai-for-understanding-image-classification-models">
 CAM and SeCAM: Explainable AI for Understanding Image Classification Models
 &lt;a class="anchor" href="#cam-and-secam-explainable-ai-for-understanding-image-classification-models">#&lt;/a>
&lt;/h1>
&lt;p>&lt;strong>Tutorial by Yaroslav Sokolov and Iskander Ishkineev&lt;/strong>&lt;/p>
&lt;p>&lt;em>To see the implementation, visit our 
 &lt;a href="https://colab.research.google.com/drive/105adw-UyMxsmWiogAtSXC6xMjfC4423T?usp=sharing">colab project&lt;/a>.&lt;/em>&lt;/p>
&lt;h2 id="introduction">
 Introduction
 &lt;a class="anchor" href="#introduction">#&lt;/a>
&lt;/h2>
&lt;p>Explainable Artificial Intelligence (XAI) has emerged as a crucial aspect of AI research, aiming to enhance the transparency and interpretability of AI models. Understanding the decision-making process of AI systems is essential for ensuring trust, accountability, and safety in their applications.
In this tutorial, we focus on Class Activation Mapping (CAM) and Segmentation Class Activation Mapping (SeCAM). Specifically, we consider their application in explaining the decisions of the ResNet50 model, a pivotal architecture within the domain of deep CNNs that has significantly impacted image classification tasks.
CAM and SeCAM in particular aim to provide fast and intuitive explanations by identifying image regions most influential to the model&amp;rsquo;s prediction.&lt;/p></description></item><item><title>Diffusion Lens: Interpreting Text Encoders in Text-to-Image pipelines</title><link>http://localhost:1313/docs/groups/diffusion-lens-interpreting-text-encoders-in-text-to-image-pipelines-tuned-using-dreambooth/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/diffusion-lens-interpreting-text-encoders-in-text-to-image-pipelines-tuned-using-dreambooth/</guid><description>&lt;h1 id="diffusion-lens-interpreting-text-encoders-in-text-to-image-pipelines">
 &lt;strong>Diffusion Lens: Interpreting Text Encoders in Text-to-Image pipelines&lt;/strong>
 &lt;a class="anchor" href="#diffusion-lens-interpreting-text-encoders-in-text-to-image-pipelines">#&lt;/a>
&lt;/h1>
&lt;p>&lt;strong>Authors: Ivan Golov, Roman Makeev&lt;/strong>&lt;/p>
&lt;p>&lt;em>To see the implementation, visit our 
 &lt;a href="https://github.com/IVproger/GAI_course_project/tree/xai">github project&lt;/a>.&lt;/em>&lt;/p>
&lt;!-- Example of image loading -->
&lt;!-- ![Diffusion Lens Diagram](/Diffusion%20Lens:%20Interpreting%20Text%20Encoders%20in%20Text-to-Image%20pipelines%20tuned%20using%20DreamBooth/dreambooth.png) -->
&lt;hr>
&lt;h2 id="introduction">
 &lt;strong>Introduction&lt;/strong>
 &lt;a class="anchor" href="#introduction">#&lt;/a>
&lt;/h2>
&lt;p>In this work, we introduce an interpretable, end-to-end framework that enhances &lt;strong>Stable Diffusion v1.5 model&lt;/strong> fine‑tuned via the 
 &lt;a href="https://dreambooth.github.io">DreamBooth method&lt;/a> [1] to generate high‑fidelity, subject‑driven images from as few reference examples.&lt;/p>
&lt;p>While DreamBooth effectively personalizes generation by associating a unique rare token with the target concept, the internal process through which textual prompts are transformed into visual representations remains opaque. To bridge this gap, we integrate 
 &lt;a href="https://tokeron.github.io/DiffusionLensWeb/">Diffusion Lens&lt;/a> [2], a visualization technique that decodes the text encoder’s intermediate hidden states into images, producing a layer‑by‑layer sequence that illuminates how semantic concepts emerge and refine over the course of encoding.&lt;/p></description></item><item><title>Dimensionality Reduction in NLP: Visualizing Sentence Embeddings with UMAP and t-SNE</title><link>http://localhost:1313/docs/groups/dimensionality-reduction-in-nlp-visualizing-sentence-embeddings-with-umap-and-t-sne/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/dimensionality-reduction-in-nlp-visualizing-sentence-embeddings-with-umap-and-t-sne/</guid><description>&lt;h1 id="dimensionality-reduction-in-nlp-visualizing-sentence-embeddings-with-umap-and-t-sne">
 Dimensionality Reduction in NLP: Visualizing Sentence Embeddings with UMAP and t-SNE
 &lt;a class="anchor" href="#dimensionality-reduction-in-nlp-visualizing-sentence-embeddings-with-umap-and-t-sne">#&lt;/a>
&lt;/h1>
&lt;h2 id="students">
 Students:
 &lt;a class="anchor" href="#students">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>Yazan Kbaili (
 &lt;a href="mailto:y.kbaili@innopolis.university">y.kbaili@innopolis.university&lt;/a>)&lt;/li>
&lt;li>Hamada Salhab (
 &lt;a href="mailto:h.salhab@innopolis.university">h.salhab@innopolis.university&lt;/a>)&lt;/li>
&lt;/ul>
&lt;h2 id="introduction">
 Introduction
 &lt;a class="anchor" href="#introduction">#&lt;/a>
&lt;/h2>
&lt;p>This report delves into the visualization of sentence embeddings derived from the &lt;code>roberta-base&lt;/code> model fine-tuned on the &amp;ldquo;go-emotion&amp;rdquo; dataset using two prominent dimensionality reduction techniques: UMAP (Uniform Manifold Approximation and Projection) and t-SNE (t-distributed Stochastic Neighbor Embedding). The dataset used in the visualization consists of Twitter messages and is called “emotions”.&lt;/p></description></item><item><title>Example</title><link>http://localhost:1313/docs/groups/example/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/example/</guid><description>&lt;style> .markdown a{text-decoration: underline !important;} &lt;/style>
&lt;style> .markdown h2{font-weight: bold;} &lt;/style>
&lt;h1 id="presentation">
 &lt;strong>Presentation&lt;/strong>
 &lt;a class="anchor" href="#presentation">#&lt;/a>
&lt;/h1>
&lt;h1 id="about-us">
 &lt;strong>About us&lt;/strong>
 &lt;a class="anchor" href="#about-us">#&lt;/a>
&lt;/h1>
&lt;p>Examplary post here&lt;/p></description></item><item><title>Example</title><link>http://localhost:1313/docs/groups/shap_for_credit_risk/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/shap_for_credit_risk/</guid><description>&lt;style> .markdown a{text-decoration: underline !important;} &lt;/style>
&lt;style> .markdown h2{font-weight: bold;} &lt;/style>
&lt;h1 id="interpretable-shap-for-credit-risk-scoring">
 Interpretable SHAP for Credit Risk Scoring
 &lt;a class="anchor" href="#interpretable-shap-for-credit-risk-scoring">#&lt;/a>
&lt;/h1>
&lt;h2 id="table-of-contents">
 Table of Contents
 &lt;a class="anchor" href="#table-of-contents">#&lt;/a>
&lt;/h2>
&lt;ul>
&lt;li>
 &lt;a href="http://localhost:1313/#1-introduction">1. Introduction&lt;/a>&lt;/li>
&lt;li>
 &lt;a href="http://localhost:1313/#2-application-domain">2. Application Domain&lt;/a>&lt;/li>
&lt;li>
 &lt;a href="http://localhost:1313/#3-methodology">3. Methodology&lt;/a>
&lt;ul>
&lt;li>
 &lt;a href="http://localhost:1313/#31-dataset-and-model-overview">3.1 Dataset and Model Overview&lt;/a>&lt;/li>
&lt;li>
 &lt;a href="http://localhost:1313/#32-explainability-method-shap">3.2 Explainability Method: SHAP&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
 &lt;a href="http://localhost:1313/#4-implementation-applying-shap-to-randomforest">4. Implementation: Applying SHAP to RandomForest&lt;/a>
&lt;ul>
&lt;li>
 &lt;a href="http://localhost:1313/#41-implementation-overview">4.1 Implementation Overview&lt;/a>&lt;/li>
&lt;li>
 &lt;a href="http://localhost:1313/#42-how-to-use-it">4.2 How to Use It&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
 &lt;a href="http://localhost:1313/#5-experiments-and-analysis">5. Experiments and Analysis&lt;/a>
&lt;ul>
&lt;li>
 &lt;a href="http://localhost:1313/#51-evaluation-metrics">5.1 Evaluation Metrics&lt;/a>&lt;/li>
&lt;li>
 &lt;a href="http://localhost:1313/#52-explanation-techniques-and-visualizations">5.2 Explanation Techniques and Visualizations&lt;/a>&lt;/li>
&lt;li>
 &lt;a href="http://localhost:1313/#53-interpretations-and-findings">5.3 Interpretations and Findings&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
 &lt;a href="http://localhost:1313/#6-my-implementation">6. My Implementation&lt;/a>&lt;/li>
&lt;li>
 &lt;a href="http://localhost:1313/#7-conclusion-future-work-limitations-and-ethical-considerations">7. Conclusion, Future Work, Limitations and Ethical Considerations&lt;/a>&lt;/li>
&lt;li>
 &lt;a href="http://localhost:1313/#8-references">8. References&lt;/a>&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="1-introduction">
 1. Introduction
 &lt;a class="anchor" href="#1-introduction">#&lt;/a>
&lt;/h2>
&lt;p>This project explores how SHAP (SHapley Additive exPlanations) can be used to enhance interpretability in credit scoring. The core objective is to implement SHAP from scratch and apply it to a Random Forest model trained on a credit dataset.&lt;/p></description></item><item><title/><link>http://localhost:1313/docs/groups/ai-playing-geoguessr-explained/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/ai-playing-geoguessr-explained/</guid><description>&lt;h1 id="ai-playing-geoguessr-explained">
 AI playing GeoGuessr explained
 &lt;a class="anchor" href="#ai-playing-geoguessr-explained">#&lt;/a>
&lt;/h1>
&lt;p>Author: Pavel Roganin&lt;/p>
&lt;h2 id="prerequisites-to-read">
 Prerequisites to read
 &lt;a class="anchor" href="#prerequisites-to-read">#&lt;/a>
&lt;/h2>
&lt;p>None&lt;/p>
&lt;h2 id="introduction">
 Introduction
 &lt;a class="anchor" href="#introduction">#&lt;/a>
&lt;/h2>
&lt;p>Everyone has played 
 &lt;a href="https://www.geoguessr.com/">GeoGuessr&lt;/a> at least once in their life. This is how the game looks like:&lt;/p>
&lt;p>
 &lt;img src="https://exfai.xyz/docs/groups/AI%20playing%20GeoGuessr%20explained/Untitled.png" alt="Untitled" />&lt;/p>
&lt;p>If you do not remember this game, I will briefly explain the rules of the game.&lt;/p>
&lt;p>This is a simple browser game that selects a random location from around the world and shows the player interactive Google Street View panoramas of that location. The player can move through the streets and look around. The player’s task is to determine where he is and point the expected location on the world map. The closer the location guessed by the player is to the real one, the more points he receives.&lt;/p></description></item><item><title/><link>http://localhost:1313/docs/groups/contrastive-grad-cam-consistency/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/contrastive-grad-cam-consistency/</guid><description>&lt;h1 id="consistent-explanations-by-contrastive-learning">
 Consistent Explanations by Contrastive Learning
 &lt;a class="anchor" href="#consistent-explanations-by-contrastive-learning">#&lt;/a>
&lt;/h1>
&lt;h2 id="introduction-unveiling-the-black-box-of-deep-learning">
 Introduction: Unveiling the Black Box of Deep Learning
 &lt;a class="anchor" href="#introduction-unveiling-the-black-box-of-deep-learning">#&lt;/a>
&lt;/h2>
&lt;h3 id="demystifying-decisions-with-post-hoc-explanations">
 Demystifying Decisions with Post-hoc Explanations
 &lt;a class="anchor" href="#demystifying-decisions-with-post-hoc-explanations">#&lt;/a>
&lt;/h3>
&lt;p>Post-hoc explanation methods are techniques used to interpret and explain the decisions made by the model after they have been trained&lt;/p>
&lt;p>These methods, such as CAM, Grad-CAM, and FullGrad, typically generate heatmaps highlighting the image regions that were most influential for the model&amp;rsquo;s prediction. High values correspond to the regions that took important role in the network&amp;rsquo;s decision.&lt;/p></description></item><item><title/><link>http://localhost:1313/docs/groups/dndfs_shap/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/dndfs_shap/</guid><description>&lt;h1 id="deep-neural-decision-forests-dndfs-with-shap-values">
 Deep Neural Decision Forests (DNDFs) with SHAP Values
 &lt;a class="anchor" href="#deep-neural-decision-forests-dndfs-with-shap-values">#&lt;/a>
&lt;/h1>
&lt;h2 id="introduction">
 Introduction
 &lt;a class="anchor" href="#introduction">#&lt;/a>
&lt;/h2>
&lt;p>Deep Neural Decision Forests (DNDFs) combine the interpretability and robustness of decision trees with the power of neural networks to capture complex patterns in data. This integration allows DNDFs to perform well on various tasks, especially in high-dimensional spaces where traditional methods may struggle.&lt;/p>
&lt;p>The method is different from random forests in that it uses a principled, joint, and global optimization of split and leaf node parameters and from conventional deep networks because a decision forest provides the final predictions.&lt;/p></description></item><item><title/><link>http://localhost:1313/docs/groups/gradcam/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/gradcam/</guid><description>&lt;p>This work is made by &lt;strong>Andrei Markov&lt;/strong> and &lt;strong>Nikita Bogdankov&lt;/strong>&lt;/p>
&lt;h1 id="grad-cam">
 Grad-CAM
 &lt;a class="anchor" href="#grad-cam">#&lt;/a>
&lt;/h1>
&lt;h2 id="what-is-it">
 What is it?
 &lt;a class="anchor" href="#what-is-it">#&lt;/a>
&lt;/h2>
&lt;p>&lt;strong>Grad-CAM&lt;/strong> (Gradient-weighted Class Activation Mapping) is a technique used in deep learning, particularly with convolutional neural networks (CNNs), to understand which regions of an input image are important for the network&amp;rsquo;s prediction of a particular class.&lt;/p>
&lt;p>
 &lt;img src="http://localhost:1313/GradCAM/example4.png" alt="image" />&lt;/p>
&lt;p>This method can be used for understanding how a CNN has been driven to make a final classification decision. Grad-CAM is class-specific, which means that it can produce a separate visualizations on the image for each class. If happens that there is a classification error, than Grad-CAM can help you to see what it did wrong, so we can say that it makes the algorithm more transparent to the developers.&lt;/p></description></item><item><title/><link>http://localhost:1313/docs/groups/integrated-gradients/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/integrated-gradients/</guid><description>&lt;h1 id="integrated-gradients-method-for-image-classification">
 Integrated Gradients Method for Image Classification
 &lt;a class="anchor" href="#integrated-gradients-method-for-image-classification">#&lt;/a>
&lt;/h1>
&lt;p>&lt;strong>XAI Course Project | Anatoliy Pushkarev&lt;/strong>&lt;/p>
&lt;h2 id="goal">
 Goal
 &lt;a class="anchor" href="#goal">#&lt;/a>
&lt;/h2>
&lt;p>Develop a robust image classification model and analyze its behavior with the help of the integrated gradients method.&lt;/p>
&lt;p>
 &lt;a href="https://arxiv.org/abs/1703.01365">Integrated gradients paper&lt;/a>&lt;/p>
&lt;h2 id="integrated-gradients">
 Integrated Gradients
 &lt;a class="anchor" href="#integrated-gradients">#&lt;/a>
&lt;/h2>
&lt;p>Integrated Gradients is a technique for attributing a classification model&amp;rsquo;s prediction to its input features. It is a model interpretability technique: you can use it to visualize the relationship between input features and model predictions. It finds the importance of each pixel or feature in input data for a particular prediction of the model.&lt;/p></description></item><item><title/><link>http://localhost:1313/docs/groups/kernel-shap/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/kernel-shap/</guid><description>&lt;h1 id="unveiling-black-boxes-with-shap-values">
 Unveiling black boxes with SHAP Values
 &lt;a class="anchor" href="#unveiling-black-boxes-with-shap-values">#&lt;/a>
&lt;/h1>
&lt;p>Nowadays, correct interpretation of model predictions is crucial. It builds user confidence, helps to understand the process being modeled, and suggests how to improve the model. Sometimes simple models are preferred, e.g. in finance, because they are easy to interpret, but they usually do not achieve the same performance as complex ones.
Therefore, to overcome this trade-off between accuracy and interpretability, various methods have been developed. In this post I want to talk about one of them, the SHAP framework.&lt;/p></description></item><item><title/><link>http://localhost:1313/docs/groups/rag/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/rag/</guid><description>&lt;h1 id="retrieval-augmented-generation">
 Retrieval-augmented generation
 &lt;a class="anchor" href="#retrieval-augmented-generation">#&lt;/a>
&lt;/h1>
&lt;p>Project by:&lt;/p>
&lt;ul>
&lt;li>Vladislav Urzhumov&lt;/li>
&lt;li>Danil Timofeev&lt;/li>
&lt;/ul>
&lt;h2 id="what-is-rag">
 What is RAG?
 &lt;a class="anchor" href="#what-is-rag">#&lt;/a>
&lt;/h2>
&lt;p>Retrieval-Augmented Generation is a framework in the explainable artificial intelligence field for improving the quality of LLM-generated responses by grounding the model on external sources of knowledge to supplement the LLM’s internal representation of information.
By grounding an LLM on a set of external, verifiable facts, the model has fewer opportunities to pull information baked into its parameters. This reduces the chances that an LLM will leak sensitive data, or &amp;ldquo;hallucinate&amp;rdquo; incorrect or misleading information.&lt;/p></description></item><item><title/><link>http://localhost:1313/docs/groups/shap_darya_and_viktoria/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/shap_darya_and_viktoria/</guid><description>&lt;h1 id="shap-tree-explanations">
 SHAP. Tree Explanations
 &lt;a class="anchor" href="#shap-tree-explanations">#&lt;/a>
&lt;/h1>
&lt;h2 id="exploring-shap-tree-explainer-for-predictive-health-analytics-a-deep-dive-into-randomforest-models">
 Exploring SHAP Tree Explainer for Predictive Health Analytics: A Deep Dive into RandomForest Models
 &lt;a class="anchor" href="#exploring-shap-tree-explainer-for-predictive-health-analytics-a-deep-dive-into-randomforest-models">#&lt;/a>
&lt;/h2>
&lt;p>Predictive models are essential for managing healthcare resources, predicting disease outbreaks, and guiding public health policy in the quickly changing field of health analytics.
RandomForest is a standout model among these due to its solid performance, interpretability, and simplicity. But it can be difficult to comprehend how these models&amp;rsquo; complex internal workings operate.
Presenting SHAP (SHapley Additive exPlanations), a revolutionary tool that clarifies how machine learning models—like RandomForest—make decisions.
In-depth discussion of the SHAP Tree Explainer, its use in RandomForest models, and its importance in predictive health analytics are provided in this article.&lt;/p></description></item><item><title/><link>http://localhost:1313/docs/groups/sverl_tac_toe/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/sverl_tac_toe/</guid><description>&lt;h1 id="explaining-reinforcement-learning-with-shapley-values--tutorial">
 Explaining Reinforcement Learning with Shapley Values | Tutorial
 &lt;a class="anchor" href="#explaining-reinforcement-learning-with-shapley-values--tutorial">#&lt;/a>
&lt;/h1>
&lt;h2 id="introduction">
 Introduction
 &lt;a class="anchor" href="#introduction">#&lt;/a>
&lt;/h2>
&lt;div align="center">
 &lt;img src="http://localhost:1313/Sverl_Tac_Toe/rl.png">
 &lt;p>
 &lt;a href="https://www.toptal.com/machine-learning/deep-dive-into-reinforcement-learning">
 https://www.toptal.com/machine-learning/deep-dive-into-reinforcement-learning
 &lt;/a>
 &lt;/p>
&lt;/div>
&lt;p>Explainability and interpretability of AI models is a hot topic in the research community in recent times. With the growth of new technologies and methods in the neural networks field, the endeavour to understand the “black box” models is rising in popularity.&lt;/p>
&lt;p>In this tutorial, you will learn 2 methods for explaining the reinforcement learning models:&lt;/p></description></item><item><title/><link>http://localhost:1313/docs/groups/torchprism/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/torchprism/</guid><description>&lt;h1 id="torchprism">
 TorchPRISM
 &lt;a class="anchor" href="#torchprism">#&lt;/a>
&lt;/h1>
&lt;p>&lt;strong>Table of Contents&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>
 &lt;a href="http://localhost:1313/#torchprism">TorchPRISM&lt;/a>
&lt;ul>
&lt;li>
 &lt;a href="http://localhost:1313/#introduction-unlocking-cnns-with-prism">Introduction: Unlocking CNNs with PRISM&lt;/a>&lt;/li>
&lt;li>
 &lt;a href="http://localhost:1313/#understanding-cnns">Understanding CNNs&lt;/a>&lt;/li>
&lt;li>
 &lt;a href="http://localhost:1313/#introducing-prism-a-glimpse-into-cnn-decision-making">Introducing PRISM: A Glimpse into CNN Decision-Making&lt;/a>&lt;/li>
&lt;li>
 &lt;a href="http://localhost:1313/#implementation-of-prism">Implementation of PRISM&lt;/a>&lt;/li>
&lt;li>
 &lt;a href="http://localhost:1313/#how-to-use">How to use&lt;/a>&lt;/li>
&lt;li>
 &lt;a href="http://localhost:1313/#examples">Examples&lt;/a>
&lt;ul>
&lt;li>
 &lt;a href="http://localhost:1313/#vgg11">VGG11&lt;/a>&lt;/li>
&lt;li>
 &lt;a href="http://localhost:1313/#resnet101">ResNet101&lt;/a>&lt;/li>
&lt;li>
 &lt;a href="http://localhost:1313/#googlenet">GoogleNet&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="introduction-unlocking-cnns-with-prism">
 Introduction: Unlocking CNNs with PRISM
 &lt;a class="anchor" href="#introduction-unlocking-cnns-with-prism">#&lt;/a>
&lt;/h2>
&lt;p>Convolutional Neural Networks (CNNs) have revolutionized computer vision, powering innovations from facial recognition to autonomous vehicles. Yet, their decision-making process remains a mystery, hindering trust and understanding.&lt;/p>
&lt;p>Inspired by the paper &amp;ldquo;Unlocking the black box of CNNs: Visualising the decision-making process with PRISM,&amp;rdquo; our blog sets out to demystify CNNs&amp;rsquo; decisions.&lt;/p></description></item><item><title/><link>http://localhost:1313/docs/groups/xai_for_transformers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/xai_for_transformers/</guid><description>&lt;h1 id="xai-for-transformers-explanations-through-lrp">
 XAI for Transformers. Explanations through LRP
 &lt;a class="anchor" href="#xai-for-transformers-explanations-through-lrp">#&lt;/a>
&lt;/h1>
&lt;h2 id="introduction">
 Introduction
 &lt;a class="anchor" href="#introduction">#&lt;/a>
&lt;/h2>
&lt;p>Transformers are becoming more and more common these days. But transformers are based on DNN that makes it harder to explain than other models. However, more and more ordinary users are starting to work with LLMs and to have more questions and doubts for its&amp;rsquo; work and decisions. Thus, there is a need for some explanation of Transformers. The method presented in the article 
 &lt;a href="https://proceedings.mlr.press/v162/ali22a/ali22a.pdf">&amp;ldquo;XAI for Transformers: Better Explanations through Conservative Propagation&amp;rdquo;&lt;/a> by Ameen Ali et. al. serves this purpose.&lt;/p></description></item></channel></rss>