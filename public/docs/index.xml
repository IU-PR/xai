<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Docs on XAI</title><link>http://localhost:1313/docs/</link><description>Recent content in Docs on XAI</description><generator>Hugo</generator><language>en-us</language><atom:link href="http://localhost:1313/docs/index.xml" rel="self" type="application/rss+xml"/><item><title>CAM and SeCAM</title><link>http://localhost:1313/docs/groups/cam_and_secam/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/cam_and_secam/</guid><description>CAM and SeCAM: Explainable AI for Understanding Image Classification Models # Tutorial by Yaroslav Sokolov and Iskander Ishkineev
To see the implementation, visit our colab project.
Introduction # Explainable Artificial Intelligence (XAI) has emerged as a crucial aspect of AI research, aiming to enhance the transparency and interpretability of AI models. Understanding the decision-making process of AI systems is essential for ensuring trust, accountability, and safety in their applications. In this tutorial, we focus on Class Activation Mapping (CAM) and Segmentation Class Activation Mapping (SeCAM).</description></item><item><title>Counterfactual Explanations for Credit Risk Models: A Case Study</title><link>http://localhost:1313/docs/groups/counterfactual-explanations-for-credit-risk-models/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/counterfactual-explanations-for-credit-risk-models/</guid><description>Counterfactual Explanations for Credit Risk Models: A Case Study # TL;DR # In this case study, we implement and incrementally refine a gradient-based approach for generating counterfactual explanations in credit risk modeling. Beginning with a basic optimization procedure, we identify and resolve multiple real-world issues:
Immutable and semantically constrained features Inter-feature dependencies (e.g., derived ratios) Ordinal variables that require discrete treatment One-hot encoded categorical features that demand joint behavior We develop solutions involving gradient masking, differentiable approximations, manual feature injection, and the Gumbel-Softmax trick to ensure our counterfactuals are not only effective, but valid, interpretable, and realistic.</description></item><item><title>Diffusion Lens: Interpreting Text Encoders in Text-to-Image pipelines</title><link>http://localhost:1313/docs/groups/diffusion-lens-interpreting-text-encoders-in-text-to-image-pipelines-tuned-using-dreambooth/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/diffusion-lens-interpreting-text-encoders-in-text-to-image-pipelines-tuned-using-dreambooth/</guid><description>Diffusion Lens: Interpreting Text Encoders in Text-to-Image pipelines # Authors: Ivan Golov, Roman Makeev
To see the implementation, visit our github project.
Introduction # In this work, we introduce an interpretable, end-to-end framework that enhances Stable Diffusion v1.5 model fine‑tuned via the DreamBooth method [1] to generate high‑fidelity, subject‑driven images from as few reference examples.
While DreamBooth effectively personalizes generation by associating a unique rare token with the target concept, the internal process through which textual prompts are transformed into visual representations remains opaque.</description></item><item><title>Dimensionality Reduction in NLP: Visualizing Sentence Embeddings with UMAP and t-SNE</title><link>http://localhost:1313/docs/groups/dimensionality-reduction-in-nlp-visualizing-sentence-embeddings-with-umap-and-t-sne/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/dimensionality-reduction-in-nlp-visualizing-sentence-embeddings-with-umap-and-t-sne/</guid><description>Dimensionality Reduction in NLP: Visualizing Sentence Embeddings with UMAP and t-SNE # Students: # Yazan Kbaili ( y.kbaili@innopolis.university) Hamada Salhab ( h.salhab@innopolis.university) Introduction # This report delves into the visualization of sentence embeddings derived from the roberta-base model fine-tuned on the &amp;ldquo;go-emotion&amp;rdquo; dataset using two prominent dimensionality reduction techniques: UMAP (Uniform Manifold Approximation and Projection) and t-SNE (t-distributed Stochastic Neighbor Embedding). The dataset used in the visualization consists of Twitter messages and is called “emotions”.</description></item><item><title>Example</title><link>http://localhost:1313/docs/groups/example/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/example/</guid><description>Presentation # About us # Examplary post here</description></item><item><title/><link>http://localhost:1313/docs/groups/ai-playing-geoguessr-explained/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/ai-playing-geoguessr-explained/</guid><description>AI playing GeoGuessr explained # Author: Pavel Roganin
Prerequisites to read # None
Introduction # Everyone has played GeoGuessr at least once in their life. This is how the game looks like:
If you do not remember this game, I will briefly explain the rules of the game.
This is a simple browser game that selects a random location from around the world and shows the player interactive Google Street View panoramas of that location.</description></item><item><title/><link>http://localhost:1313/docs/groups/contrastive-grad-cam-consistency/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/contrastive-grad-cam-consistency/</guid><description>Consistent Explanations by Contrastive Learning # Introduction: Unveiling the Black Box of Deep Learning # Demystifying Decisions with Post-hoc Explanations # Post-hoc explanation methods are techniques used to interpret and explain the decisions made by the model after they have been trained
These methods, such as CAM, Grad-CAM, and FullGrad, typically generate heatmaps highlighting the image regions that were most influential for the model&amp;rsquo;s prediction. High values correspond to the regions that took important role in the network&amp;rsquo;s decision.</description></item><item><title/><link>http://localhost:1313/docs/groups/dndfs_shap/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/dndfs_shap/</guid><description>Deep Neural Decision Forests (DNDFs) with SHAP Values # Introduction # Deep Neural Decision Forests (DNDFs) combine the interpretability and robustness of decision trees with the power of neural networks to capture complex patterns in data. This integration allows DNDFs to perform well on various tasks, especially in high-dimensional spaces where traditional methods may struggle.
The method is different from random forests in that it uses a principled, joint, and global optimization of split and leaf node parameters and from conventional deep networks because a decision forest provides the final predictions.</description></item><item><title/><link>http://localhost:1313/docs/groups/gradcam/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/gradcam/</guid><description>This work is made by Andrei Markov and Nikita Bogdankov
Grad-CAM # What is it? # Grad-CAM (Gradient-weighted Class Activation Mapping) is a technique used in deep learning, particularly with convolutional neural networks (CNNs), to understand which regions of an input image are important for the network&amp;rsquo;s prediction of a particular class.
This method can be used for understanding how a CNN has been driven to make a final classification decision.</description></item><item><title/><link>http://localhost:1313/docs/groups/integrated-gradients/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/integrated-gradients/</guid><description>Integrated Gradients Method for Image Classification # XAI Course Project | Anatoliy Pushkarev
Goal # Develop a robust image classification model and analyze its behavior with the help of the integrated gradients method.
Integrated gradients paper
Integrated Gradients # Integrated Gradients is a technique for attributing a classification model&amp;rsquo;s prediction to its input features. It is a model interpretability technique: you can use it to visualize the relationship between input features and model predictions.</description></item><item><title/><link>http://localhost:1313/docs/groups/kernel-shap/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/kernel-shap/</guid><description>Unveiling black boxes with SHAP Values # Nowadays, correct interpretation of model predictions is crucial. It builds user confidence, helps to understand the process being modeled, and suggests how to improve the model. Sometimes simple models are preferred, e.g. in finance, because they are easy to interpret, but they usually do not achieve the same performance as complex ones. Therefore, to overcome this trade-off between accuracy and interpretability, various methods have been developed.</description></item><item><title/><link>http://localhost:1313/docs/groups/rag/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/rag/</guid><description>Retrieval-augmented generation # Project by:
Vladislav Urzhumov Danil Timofeev What is RAG? # Retrieval-Augmented Generation is a framework in the explainable artificial intelligence field for improving the quality of LLM-generated responses by grounding the model on external sources of knowledge to supplement the LLM’s internal representation of information. By grounding an LLM on a set of external, verifiable facts, the model has fewer opportunities to pull information baked into its parameters.</description></item><item><title/><link>http://localhost:1313/docs/groups/shap_darya_and_viktoria/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/shap_darya_and_viktoria/</guid><description>SHAP. Tree Explanations # Exploring SHAP Tree Explainer for Predictive Health Analytics: A Deep Dive into RandomForest Models # Predictive models are essential for managing healthcare resources, predicting disease outbreaks, and guiding public health policy in the quickly changing field of health analytics. RandomForest is a standout model among these due to its solid performance, interpretability, and simplicity. But it can be difficult to comprehend how these models&amp;rsquo; complex internal workings operate.</description></item><item><title/><link>http://localhost:1313/docs/groups/sverl_tac_toe/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/sverl_tac_toe/</guid><description>Explaining Reinforcement Learning with Shapley Values | Tutorial # Introduction # https://www.toptal.com/machine-learning/deep-dive-into-reinforcement-learning Explainability and interpretability of AI models is a hot topic in the research community in recent times. With the growth of new technologies and methods in the neural networks field, the endeavour to understand the “black box” models is rising in popularity.
In this tutorial, you will learn 2 methods for explaining the reinforcement learning models:
Applying Shapley values to policies Using SVERL-P method Before we start, this tutorial is based on the paper “Explaining Reinforcement Learning with Shapley Values” by Beechey et.</description></item><item><title/><link>http://localhost:1313/docs/groups/torchprism/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/torchprism/</guid><description>TorchPRISM # Table of Contents
TorchPRISM Introduction: Unlocking CNNs with PRISM Understanding CNNs Introducing PRISM: A Glimpse into CNN Decision-Making Implementation of PRISM How to use Examples VGG11 ResNet101 GoogleNet Introduction: Unlocking CNNs with PRISM # Convolutional Neural Networks (CNNs) have revolutionized computer vision, powering innovations from facial recognition to autonomous vehicles. Yet, their decision-making process remains a mystery, hindering trust and understanding.
Inspired by the paper &amp;ldquo;Unlocking the black box of CNNs: Visualising the decision-making process with PRISM,&amp;rdquo; our blog sets out to demystify CNNs&amp;rsquo; decisions.</description></item><item><title/><link>http://localhost:1313/docs/groups/xai_for_transformers/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>http://localhost:1313/docs/groups/xai_for_transformers/</guid><description>XAI for Transformers. Explanations through LRP # Introduction # Transformers are becoming more and more common these days. But transformers are based on DNN that makes it harder to explain than other models. However, more and more ordinary users are starting to work with LLMs and to have more questions and doubts for its&amp;rsquo; work and decisions. Thus, there is a need for some explanation of Transformers. The method presented in the article &amp;ldquo;XAI for Transformers: Better Explanations through Conservative Propagation&amp;rdquo; by Ameen Ali et.</description></item></channel></rss>