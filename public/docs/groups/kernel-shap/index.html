<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Unveiling black boxes with SHAP Values # Nowadays, correct interpretation of model predictions is crucial. It builds user confidence, helps to understand the process being modeled, and suggests how to improve the model. Sometimes simple models are preferred, e.g. in finance, because they are easy to interpret, but they usually do not achieve the same performance as complex ones. Therefore, to overcome this trade-off between accuracy and interpretability, various methods have been developed."><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:title" content><meta property="og:description" content="Unveiling black boxes with SHAP Values # Nowadays, correct interpretation of model predictions is crucial. It builds user confidence, helps to understand the process being modeled, and suggests how to improve the model. Sometimes simple models are preferred, e.g. in finance, because they are easy to interpret, but they usually do not achieve the same performance as complex ones. Therefore, to overcome this trade-off between accuracy and interpretability, various methods have been developed."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/docs/groups/kernel-shap/"><meta property="article:section" content="docs"><title>Kernel Shap | XAI</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.e832d4e94212199857473bcf13a450d089c3fcd54ccadedcfac84ed0feff83fb.css integrity="sha256-6DLU6UISGZhXRzvPE6RQ0InD/NVMyt7c+shO0P7/g/s=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/mathtex-script-type.min.js integrity=sha384-jiBVvJ8NGGj5n7kJaiWwWp9AjC+Yh8rhZY3GtAX8yU28azcLgoRo4oukO87g7zDT crossorigin=anonymous></script><script defer src=/flexsearch.min.js></script><script defer src=/en.search.min.c9008eb139ac970f1f53a56abe5cdb2f36fa54018801e9ce80aa550667d5f022.js integrity="sha256-yQCOsTmslw8fU6VqvlzbLzb6VAGIAenOgKpVBmfV8CI=" crossorigin=anonymous></script><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/YELLOW_BAR.png alt=Logo><span><b>XAI</b></span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=/docs/groups/cam_and_secam/>CAM and SeCAM</a></li><li><a href=/docs/groups/diffusion-lens-interpreting-text-encoders-in-text-to-image-pipelines-tuned-using-dreambooth/>Diffusion Lens: Interpreting Text Encoders in Text-to-Image pipelines</a></li><li><a href=/docs/groups/dimensionality-reduction-in-nlp-visualizing-sentence-embeddings-with-umap-and-t-sne/>Dimensionality Reduction in NLP: Visualizing Sentence Embeddings with UMAP and t-SNE</a></li><li><a href=/docs/groups/example/>Example</a></li><li><a href=/docs/groups/ai-playing-geoguessr-explained/>Ai Playing Geo Guessr Explained</a></li><li><a href=/docs/groups/contrastive-grad-cam-consistency/>Contrastive Grad Cam Consistency</a></li><li><a href=/docs/groups/contrastivrcredit/>Contrastivr Credit</a></li><li><a href=/docs/groups/dndfs_shap/>Dndfs Shap</a></li><li><a href=/docs/groups/gradcam/>Grad Cam</a></li><li><a href=/docs/groups/integrated-gradients/>Integrated Gradients</a></li><li><a href=/docs/groups/kernel-shap/ class=active>Kernel Shap</a></li><li><a href=/docs/groups/rag/>Rag</a></li><li><a href=/docs/groups/shap_darya_and_viktoria/>Shap Darya and Viktoria</a></li><li><a href=/docs/groups/sverl_tac_toe/>Sverl Tac Toe</a></li><li><a href=/docs/groups/torchprism/>Torch Prism</a></li><li><a href=/docs/groups/xai_for_transformers/>Xai for Transformers</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Kernel Shap</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#unveiling-black-boxes-with-shap-values>Unveiling black boxes with SHAP Values</a></li><li><a href=#what-are-shap-values>What are SHAP Values?</a></li><li><a href=#the-mathematics-behind-shap-values>The Mathematics Behind SHAP Values</a><ul><li><ul><li><a href=#formal-definition>Formal Definition:</a></li><li><a href=#extension-to-shap-values-and-properties>Extension to SHAP Values and Properties:</a></li></ul></li></ul></li><li><a href=#computation-of-shap-values>Computation of SHAP Values</a><ul><li><ul><li><a href=#kernel-shap>Kernel SHAP</a></li><li><a href=#illustrative-example>Illustrative example</a></li></ul></li></ul></li><li><a href=#interpreting-shap-values>Interpreting SHAP Values</a><ul><li><ul><li><a href=#individual-instance-interpretation>Individual Instance Interpretation:</a></li><li><a href=#global-feature-importance>Global Feature Importance:</a></li></ul></li></ul></li><li><a href=#applications-of-shap-values>Applications of SHAP Values</a></li><li><a href=#conclusion>Conclusion:</a><ul><li><ul><li><a href=#main-takeaways>Main takeaways:</a></li></ul></li></ul></li><li><a href=#references>References:</a></li></ul></nav></aside></header><article class=markdown><h1 id=unveiling-black-boxes-with-shap-values>Unveiling black boxes with SHAP Values
<a class=anchor href=#unveiling-black-boxes-with-shap-values>#</a></h1><p>Nowadays, correct interpretation of model predictions is crucial. It builds user confidence, helps to understand the process being modeled, and suggests how to improve the model. Sometimes simple models are preferred, e.g. in finance, because they are easy to interpret, but they usually do not achieve the same performance as complex ones.
Therefore, to overcome this trade-off between accuracy and interpretability, various methods have been developed. In this post I want to talk about one of them, the SHAP framework.</p><h1 id=what-are-shap-values>What are SHAP Values?
<a class=anchor href=#what-are-shap-values>#</a></h1><p>SHAP Values is a method that assigns each feature a value that reflects its contribution to the model prediction. These values are based on cooperative game theory,
<a href=https://en.wikipedia.org/wiki/Shapley_value>the concept of Shapley values</a>, introduced by Lloyd Shapley.
In this context, each attribute is treated as a player in the game, and the Shapley value measures the average marginal contribution of each attribute across all possible combinations of attributes</p><h1 id=the-mathematics-behind-shap-values>The Mathematics Behind SHAP Values
<a class=anchor href=#the-mathematics-behind-shap-values>#</a></h1><p>As already mentioned, Shapley values is a concept in the theory of cooperative games. For each such game, it specifies the distribution of the total payoff received by the coalition of all players.</p><h3 id=formal-definition>Formal Definition:
<a class=anchor href=#formal-definition>#</a></h3><p>The Shapley value for player
<link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\( i \)
</span>in a cooperative game is defined as the average marginal contribution of the player to the coalitions.
Formally, we have a set <span>\(N\)
</span>of players and a characteristic function <span>\(\mathcal{v}\)
</span>representing gains, which maps subset of players to real numbers(gain). Also, <span>\(\mathcal{v(\emptyset)}=0\)
</span>meaning that empty coaliation of players worths nothing.
Then, the Shapley value <span>\(\phi_i\)
</span>for player <span>\(i\)
</span>is given by:
<span>\[\phi_i{(\mathcal{v})}=\sum_{S \subset N \backslash \left\{i\right\}}{\frac{|S|!(n-|S|-1)!}{|N|!} (\mathcal{v}(S \cup \left\{i\right\})-\mathcal{v}(S))}\]</span></p><p>where:</p><ul><li><span>\(S\)
</span>is a subset of players excluding player <span>\(i\)
</span>.</li><li><span>\(|S|\)
</span>is the number of player in the coalition <span>\(S\)</span></li><li><span>\(\mathcal{v}(S)\)
</span>is a total gain of the coalition <span>\(S\)
</span>This formula calculates the marginal contribution of player <span>\(i\)
</span>to each possible coalition and then averages it.</li></ul><h3 id=extension-to-shap-values-and-properties>Extension to SHAP Values and Properties:
<a class=anchor href=#extension-to-shap-values-and-properties>#</a></h3><p>SHAP combines the local interpretability methods(Linear LIME, for example) and Shapley values. It results in desired properties:</p><ol><li>Local accuracy: <span>\[f(x)=g(x')=\phi_0+\sum^M_{i=1}{\phi_ix'_i}\]
</span>The explanation model <span>\(g(x')\)
</span>matches the original model <span>\(f(x)\)
</span>when <span>\(x=h_x(x')\)
</span>, where <span>\(\phi_0=f(h_x(0))\)
</span>represents the model output with all simplified inputs toggled off(missing)</li><li>Missingness:<span>
\[x'_i=0 \to \phi_i=0\]
</span>Missing features have no <strong>attributed</strong> impact</li><li>Consistency: Let <span>\(f_x(z')=f(h_x(z'))\)
</span>and <span>\(z'\backslash i\)
</span>denote setting <span>\(z'_i=0\)
</span>. For any two models <span>\(f\)
</span>and <span>\(f'\)
</span>, if <span>\[f'_x(z')-f'_x(z'\backslash i)\geq f_x(z')-f_x(z'\backslash i)\]
</span>for all inputs <span>\(z' \in \left\{0,1\right\}^M\)
</span>, then <span>\(\phi_i(f',x)\geq\phi_i(f,x)\)
</span>.
It means that if a model changes so that the marginal contribution of a feature value increases or stays the same (regardless of other features), the Shapley value also increases or stays the same.</li></ol><h1 id=computation-of-shap-values>Computation of SHAP Values
<a class=anchor href=#computation-of-shap-values>#</a></h1><h3 id=kernel-shap>Kernel SHAP
<a class=anchor href=#kernel-shap>#</a></h3><p>This is a model-agnostic method for approximating SHAP values. This method uses a Linear LIME to locally approximate the original model.</p><p>First, we need to heuristically choose the parameters for LIME:
<span>\[\Omega(g)=0,\]
</span><span>\[\pi_{x}(z')=\frac{(M-1)}{(M \text{ choose } |z'|)|z'|(M-|z'|)}\]
</span><span>\[L(f,g,\pi_{x})=\sum_{z'\in Z}{[f(h_x(z'))-g(z')]^2\pi_{x}(z')}\]</span></p><p>Then, since <span>\(g(z')\)
</span>is linear, <span>\(L\)
</span>is a squared loss, the objective function of LIME: <span>\(\xi=\underset{g \in \mathcal{G}}{\operatorname{argmin}}{L(f,g,\pi_{x'})+\Omega{(g)}}\)
</span>can be solved using linear regression.</p><h3 id=illustrative-example>Illustrative example
<a class=anchor href=#illustrative-example>#</a></h3><ol><li>Model and Instance:
Let&rsquo;s say we have a predictive model f and a dataset with three features. We want to understand how each feature contributes to the model&rsquo;s prediction for a specific data point <span>\(x = (x_1,x_2,x_3)\)
</span>by computing SHAP values.</li><li>Generating coalitions:
To do it we need to consider all possible coalitions of features that could be used to make a prediction. Each coalition is a subset of the features used for prediction. The set of coalitions in our case: {0,0,0},{0,0,1},{0,1,0},{0,1,1},{1,0,0},{1,0,1},{1,1,0},{1,1,1}.</li><li>Obtaining modeling results for coalitions:
For each of these coalitions, we compute the model output. The missing features must be imputed. We obtain the following outputs: <span>\(f(\emptyset), f(x_1),f(x_2),f(x_3),f(x_1,x_2),f(x_1,x_3),f(x_2,x_3),f(x_1,x_2,x_3)\)</span></li><li>Obtaining weights for coalitions:
<span>\[\pi_{x}(z')=\frac{(M-1)}{(M \text{ choose } |z'|)|z'|(M-|z'|)}\]
</span>For example, <span>\(\pi_x({0,0,1})=\frac{3-1}{\frac{3!}{1!(3-1)!}1(3-1)}=\frac{2}{6}=\frac{1}{3}\)</span></li><li>We obtain the model <span>\(g\)
</span>:
Finally, we train a linear model (explanation model) <span>\(g\)
</span>. This model is trained to match the outputs of our original model <span>\(f\)
</span>. The weights of the model <span>\(g\)
</span>are obtained by optimizing the following loss function <span>\[L(f,g,\pi_{x})=\sum_{z'\in Z}{[f(h_x(z'))-g(z')]^2\pi_{x}(z')}\]
</span>The weights of model <span>\(g\)
</span>are the Shapley values.</li></ol><h1 id=interpreting-shap-values>Interpreting SHAP Values
<a class=anchor href=#interpreting-shap-values>#</a></h1><h3 id=individual-instance-interpretation>Individual Instance Interpretation:
<a class=anchor href=#individual-instance-interpretation>#</a></h3><ul><li><strong>Feature Contribution:</strong> SHAP values give us the ability to measure how badly or good a feature is in making model predictions about an individual, instance. A positive value implies that the feature contributes towards increasing the prediction of the model while a negative value suggests that it reduces the prediction.</li><li><strong>Magnitude</strong>: The magnitude or absolute value of a SHAP number impacts on how much influence a particular attribute has in our model prediction, Larger numbers indicate more importance in shaping up the final outcome.</li></ul><h3 id=global-feature-importance>Global Feature Importance:
<a class=anchor href=#global-feature-importance>#</a></h3><ul><li><strong>Feature Importance Ranking:</strong> The average of the absolute SHAP values for each feature across all instances gives us a global ranking of feature importance. This ranking helps identify the features that consistently have the most significant impact on model predictions.</li><li><strong>Understanding Model Behavior:</strong> An insight into how well our model reacts to different levels of a given characteristic is only possible by studying distributions of its corresponding Shapley Values. In doing so, an expose can be made about any prejudices or non-linear aspects within.</li></ul><h1 id=applications-of-shap-values>Applications of SHAP Values
<a class=anchor href=#applications-of-shap-values>#</a></h1><p>SHAP values is a powerful tool that has several applications:</p><p><strong>1. Model Debugging:</strong> we can identify features which cause problems in predictions, that can indicate, for example, data leakage or correlations.</p><p><strong>2. Fairness and Bias Analysis:</strong> we can identify biases when the model making different unfair predictions based on some attributes like race, gender, and etc. Understanding the impact of these feature can help us develop fairer models.</p><h1 id=conclusion>Conclusion:
<a class=anchor href=#conclusion>#</a></h1><h3 id=main-takeaways>Main takeaways:
<a class=anchor href=#main-takeaways>#</a></h3><ul><li><strong>Shapley values is a measure of the average marginal contribution of each feature across all possible subsets of features.</strong></li><li><strong>SHAP values are connecting Shapley values to the local interpretability methods, providing properties such as locality, missingness, and consistency.</strong></li><li><strong>SHAP values enable us to better undestand both individual instance and global model behavior, as well as providing feature contribution analysis, feature importance ranking, and model behavior understanding.</strong></li><li><strong>They are commonly used for model debugging allowing identifying problematic features and developing more unbiased and fair model</strong></li></ul><h1 id=references>References:
<a class=anchor href=#references>#</a></h1><p>The blog post is mainly based on the original paper introducing SHAP values -
<a href=https://papers.nips.cc/paper_files/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html>A Unified Approach to Interpreting Model Predictions (nips.cc)</a></p><p>The original implementation of SHAP -
<a href=https://github.com/shap/shap>shap/shap: A game theoretic approach to explain the output of any machine learning model. (github.com)</a></p><p>My implementation of Kernel SHAP -
<a href="https://colab.research.google.com/drive/1TPHvns2psDNKknwubxCTHZprw3UGB-w1?usp=sharing">https://colab.research.google.com/drive/1TPHvns2psDNKknwubxCTHZprw3UGB-w1?usp=sharing</a></p></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/IU-PR/Capstone_project/tree/master//content/docs/Groups/Kernel%20SHAP.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#unveiling-black-boxes-with-shap-values>Unveiling black boxes with SHAP Values</a></li><li><a href=#what-are-shap-values>What are SHAP Values?</a></li><li><a href=#the-mathematics-behind-shap-values>The Mathematics Behind SHAP Values</a><ul><li><ul><li><a href=#formal-definition>Formal Definition:</a></li><li><a href=#extension-to-shap-values-and-properties>Extension to SHAP Values and Properties:</a></li></ul></li></ul></li><li><a href=#computation-of-shap-values>Computation of SHAP Values</a><ul><li><ul><li><a href=#kernel-shap>Kernel SHAP</a></li><li><a href=#illustrative-example>Illustrative example</a></li></ul></li></ul></li><li><a href=#interpreting-shap-values>Interpreting SHAP Values</a><ul><li><ul><li><a href=#individual-instance-interpretation>Individual Instance Interpretation:</a></li><li><a href=#global-feature-importance>Global Feature Importance:</a></li></ul></li></ul></li><li><a href=#applications-of-shap-values>Applications of SHAP Values</a></li><li><a href=#conclusion>Conclusion:</a><ul><li><ul><li><a href=#main-takeaways>Main takeaways:</a></li></ul></li></ul></li><li><a href=#references>References:</a></li></ul></nav></div></aside></main></body></html>