<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="
  Deep Neural Decision Forests (DNDFs) with SHAP Values
  #


  Introduction
  #

Deep Neural Decision Forests (DNDFs) combine the interpretability and robustness of decision trees with the power of neural networks to capture complex patterns in data. This integration allows DNDFs to perform well on various tasks, especially in high-dimensional spaces where traditional methods may struggle.
The method is different from random forests in that it uses a principled, joint, and global optimization of split and leaf node parameters and from conventional deep networks because a decision forest provides the final predictions."><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/groups/dndfs_shap/"><meta property="og:site_name" content="XAI"><meta property="og:title" content="XAI"><meta property="og:description" content="Deep Neural Decision Forests (DNDFs) with SHAP Values # Introduction # Deep Neural Decision Forests (DNDFs) combine the interpretability and robustness of decision trees with the power of neural networks to capture complex patterns in data. This integration allows DNDFs to perform well on various tasks, especially in high-dimensional spaces where traditional methods may struggle.
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.e832d4e94212199857473bcf13a450d089c3fcd54ccadedcfac84ed0feff83fb.css integrity="sha256-6DLU6UISGZhXRzvPE6RQ0InD/NVMyt7c+shO0P7/g/s=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/mathtex-script-type.min.js integrity=sha384-jiBVvJ8NGGj5n7kJaiWwWp9AjC+Yh8rhZY3GtAX8yU28azcLgoRo4oukO87g7zDT crossorigin=anonymous></script><script defer src=/flexsearch.min.js></script><script defer src=/en.search.min.ad436edd829ec592525c968cb38b5379be6117b5639c053bb9908a9c0a469c15.js integrity="sha256-rUNu3YKexZJSXJaMs4tTeb5hF7VjnAU7uZCKnApGnBU=" crossorigin=anonymous></script><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/YELLOW_BAR.png alt=Logo><span><b>XAI</b></span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=/docs/groups/cam_and_secam/>CAM and SeCAM</a></li><li><a href=/docs/groups/counterfactual-explanations-for-credit-risk-models/>Counterfactual Explanations for Credit Risk Models: A Case Study</a></li><li><a href=/docs/groups/diffusion-lens-interpreting-text-encoders-in-text-to-image-pipelines-tuned-using-dreambooth/>Diffusion Lens: Interpreting Text Encoders in Text-to-Image pipelines</a></li><li><a href=/docs/groups/dimensionality-reduction-in-nlp-visualizing-sentence-embeddings-with-umap-and-t-sne/>Dimensionality Reduction in NLP: Visualizing Sentence Embeddings with UMAP and t-SNE</a></li><li><a href=/docs/groups/example/>Example</a></li><li><a href=/docs/groups/ai-playing-geoguessr-explained/>Ai Playing Geo Guessr Explained</a></li><li><a href=/docs/groups/contrastive-grad-cam-consistency/>Contrastive Grad Cam Consistency</a></li><li><a href=/docs/groups/dndfs_shap/ class=active>Dndfs Shap</a></li><li><a href=/docs/groups/gradcam/>Grad Cam</a></li><li><a href=/docs/groups/integrated-gradients/>Integrated Gradients</a></li><li><a href=/docs/groups/kernel-shap/>Kernel Shap</a></li><li><a href=/docs/groups/rag/>Rag</a></li><li><a href=/docs/groups/shap_darya_and_viktoria/>Shap Darya and Viktoria</a></li><li><a href=/docs/groups/sverl_tac_toe/>Sverl Tac Toe</a></li><li><a href=/docs/groups/torchprism/>Torch Prism</a></li><li><a href=/docs/groups/xai_for_transformers/>Xai for Transformers</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Dndfs Shap</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#deep-neural-decision-forests-dndfs-with-shap-values>Deep Neural Decision Forests (DNDFs) with SHAP Values</a><ul><li><a href=#introduction>Introduction</a></li><li><a href=#formulas-involved>Formulas Involved</a></li><li><a href=#learning-process>Learning Process</a></li><li><a href=#dataset>Dataset</a></li><li><a href=#model-description>Model Description</a><ul><li><a href=#model-components>Model Components</a></li></ul></li><li><a href=#implementation>Implementation</a><ul><li><a href=#dataset-class>Dataset Class</a></li><li><a href=#feature-layer>Feature Layer</a></li><li><a href=#tree-and-forest-classes>Tree and Forest Classes</a></li><li><a href=#neural-decision-forest-class>Neural Decision Forest Class</a></li><li><a href=#training-and-evaluation-functions>Training and Evaluation Functions</a></li><li><a href=#evaluation-and-prediction-functions>Evaluation and Prediction Functions</a></li><li><a href=#shap-explanations>SHAP Explanations</a></li><li><a href=#main-function>Main Function</a></li></ul></li><li><a href=#results>Results</a><ul><li><a href=#model-performance>Model Performance</a></li><li><a href=#shap-values>SHAP Values</a></li><li><a href=#challenges>Challenges</a></li><li><a href=#insights-from-shap>Insights from SHAP</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#references>References</a></li></ul></li></ul></nav></aside></header><article class=markdown><h1 id=deep-neural-decision-forests-dndfs-with-shap-values>Deep Neural Decision Forests (DNDFs) with SHAP Values
<a class=anchor href=#deep-neural-decision-forests-dndfs-with-shap-values>#</a></h1><h2 id=introduction>Introduction
<a class=anchor href=#introduction>#</a></h2><p>Deep Neural Decision Forests (DNDFs) combine the interpretability and robustness of decision trees with the power of neural networks to capture complex patterns in data. This integration allows DNDFs to perform well on various tasks, especially in high-dimensional spaces where traditional methods may struggle.</p><p>The method is different from random forests in that it uses a principled, joint, and global optimization of split and leaf node parameters and from conventional deep networks because a decision forest provides the final predictions.</p><h2 id=formulas-involved>Formulas Involved
<a class=anchor href=#formulas-involved>#</a></h2><p>The final probability of an observation belonging to a class is the aggregated probability of that observation belonging to a class in each leaf node. The aggregation is done using a weighted sum, where the probability of the observation reaching the corresponding leaf is taken as weight. From the paper, the actual formula is as below:</p><p align=center><img src=/DNDFs_SHAP/1.jpg alt="Probability of an observation x belonging to class y. Source: ref Deep Neural Decision Forest"><p align=center><h5><font color=DimGray><center>Probability of an observation x belonging to class y</center></font></h5><h2 id=learning-process>Learning Process
<a class=anchor href=#learning-process>#</a></h2><p>The training of the model is done in two stages. Starting from a randomly initiated set of class probabilities for each node, iteratively update ùúã and ¬µ for a predefined number of epochs.</p><h2 id=dataset>Dataset
<a class=anchor href=#dataset>#</a></h2><p><strong>Data Description:</strong>
There are three types of input features:</p><ul><li><strong>Objective</strong>: factual information</li><li><strong>Examination</strong>: results of medical examination</li><li><strong>Subjective</strong>: information given by the patient</li></ul><p><strong>Features:</strong></p><ul><li>Age | int (days)</li><li>Height | int (cm)</li><li>Weight | float (kg)</li><li>Gender | categorical code</li><li>Systolic blood pressure | int</li><li>Diastolic blood pressure | int</li><li>Cholesterol | 1: normal, 2: above normal, 3: well above normal</li><li>Glucose | 1, 2, 3</li><li>Smoking | binary</li><li>Alcohol intake | binary</li><li>Physical activity | binary</li><li>Presence or absence of cardiovascular disease | binary</li></ul><p>Dataset can be found
<a href=https://www.kaggle.com/datasets/sulianova/cardiovascular-disease-dataset>here</a>.</p><h2 id=model-description>Model Description
<a class=anchor href=#model-description>#</a></h2><p>We based our implementation on the DeepNeuralForest model available on
<a href=https://github.com/jingxil/Neural-Decision-Forests/blob/master/README.md>GitHub</a>, which is inspired by a paper on neural decision forests. We adapted the model and the training loop from this source. Specifically, we modified one of the layers in the model to better suit our dataset and added a custom class to handle our specific dataset.</p><p>The paper can be accessed
<a href=https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Kontschieder_Deep_Neural_Decision_ICCV_2015_paper.pdf>here</a>.</p><h3 id=model-components>Model Components
<a class=anchor href=#model-components>#</a></h3><ol><li><strong>Feature Extraction Layer</strong>: This layer is built using fully connected neural networks with ReLU activations and dropout for regularization.</li><li><strong>Decision Forest</strong>: This component consists of multiple decision trees, each trained on a random subset of the features.</li></ol><p align=center><img src=/DNDFs_SHAP/2.jpg alt="Training Process. Source: ref Deep Neural Decision Forest"><p align=center><h5><font color=DimGray><center>Model structure for tabular data</center></font></h5><h2 id=implementation>Implementation
<a class=anchor href=#implementation>#</a></h2><h3 id=dataset-class>Dataset Class
<a class=anchor href=#dataset-class>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>CardioDataset</span>(Dataset):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Custom dataset class for handling the cardiovascular disease dataset.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, df):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            df (pd.DataFrame): DataFrame containing the dataset.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>data <span style=color:#f92672>=</span> df
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>X <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>iloc[:, <span style=color:#ae81ff>1</span>:<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>values<span style=color:#f92672>.</span>astype(np<span style=color:#f92672>.</span>float32)  <span style=color:#75715e># Features</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>y <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>iloc[:, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>values<span style=color:#f92672>.</span>astype(np<span style=color:#f92672>.</span>int64)      <span style=color:#75715e># Labels</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Normalize the features</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>X <span style=color:#f92672>=</span> (self<span style=color:#f92672>.</span>X <span style=color:#f92672>-</span> self<span style=color:#f92672>.</span>X<span style=color:#f92672>.</span>mean(axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)) <span style=color:#f92672>/</span> (self<span style=color:#f92672>.</span>X<span style=color:#f92672>.</span>std(axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>) <span style=color:#f92672>+</span> <span style=color:#ae81ff>1e-6</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __len__(self):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;Returns the total number of samples in the dataset.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> len(self<span style=color:#f92672>.</span>data)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __getitem__(self, idx):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Retrieves the feature and label for a given index.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            idx (int): Index of the sample to retrieve.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            tuple: (feature, label)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>X[idx], self<span style=color:#f92672>.</span>y[idx]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>prepare_db</span>(csv_file):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Prepares the training, validation, and test datasets.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        csv_file (str): Path to the CSV file containing the dataset.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        dict: Dictionary containing the training, validation, and test datasets.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(csv_file, sep<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;;&#39;</span>)
</span></span><span style=display:flex><span>    train_df, test_df <span style=color:#f92672>=</span> train_test_split(df, test_size<span style=color:#f92672>=</span><span style=color:#ae81ff>0.2</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>    train_df, val_df <span style=color:#f92672>=</span> train_test_split(train_df, test_size<span style=color:#f92672>=</span><span style=color:#ae81ff>0.25</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)  <span style=color:#75715e># 0.25 * 0.8 = 0.2</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    train_dataset <span style=color:#f92672>=</span> CardioDataset(train_df)
</span></span><span style=display:flex><span>    val_dataset <span style=color:#f92672>=</span> CardioDataset(val_df)
</span></span><span style=display:flex><span>    test_dataset <span style=color:#f92672>=</span> CardioDataset(test_df)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> {<span style=color:#e6db74>&#39;train&#39;</span>: train_dataset, <span style=color:#e6db74>&#39;val&#39;</span>: val_dataset, <span style=color:#e6db74>&#39;test&#39;</span>: test_dataset, <span style=color:#e6db74>&#39;test_df&#39;</span>: test_df}
</span></span></code></pre></div><h3 id=feature-layer>Feature Layer
<a class=anchor href=#feature-layer>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>CardioFeatureLayer</span>(nn<span style=color:#f92672>.</span>Sequential):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Feature extraction layer using fully connected neural networks with dropout.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, dropout_rate, shallow<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            dropout_rate (float): Dropout rate for regularization.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            shallow (bool, optional): Whether to use a shallow network. Defaults to False.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        super(CardioFeatureLayer, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>add_module(<span style=color:#e6db74>&#39;linear1&#39;</span>, nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>11</span>, <span style=color:#ae81ff>1024</span>))
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>add_module(<span style=color:#e6db74>&#39;relu1&#39;</span>, nn<span style=color:#f92672>.</span>ReLU())
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>add_module(<span style=color:#e6db74>&#39;dropout1&#39;</span>, nn<span style=color:#f92672>.</span>Dropout(dropout_rate))
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>add_module(<span style=color:#e6db74>&#39;linear2&#39;</span>, nn<span style=color:#f92672>.</span>Linear(<span style=color:#ae81ff>1024</span>, <span style=color:#ae81ff>1024</span>))
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>add_module(<span style=color:#e6db74>&#39;relu2&#39;</span>, nn<span style=color:#f92672>.</span>ReLU())
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>add_module(<span style=color:#e6db74>&#39;dropout2&#39;</span>, nn<span style=color:#f92672>.</span>Dropout(dropout_rate))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_out_feature_size</span>(self):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;Returns the output feature size of the layer.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>1024</span>
</span></span></code></pre></div><h3 id=tree-and-forest-classes>Tree and Forest Classes
<a class=anchor href=#tree-and-forest-classes>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Tree</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Tree class for building a single decision tree.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, depth, n_in_feature, used_feature_rate, n_class, jointly_training<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            depth (int): Depth of the tree.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            n_in_feature (int): Number of input features.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            used_feature_rate (float): Fraction of features to use.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            n_class (int): Number of classes.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            jointly_training (bool, optional): Whether to use joint training. Defaults to True.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        super(Tree, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>depth <span style=color:#f92672>=</span> depth
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>n_leaf <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>**</span> depth
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>n_class <span style=color:#f92672>=</span> n_class
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>jointly_training <span style=color:#f92672>=</span> jointly_training
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        n_used_feature <span style=color:#f92672>=</span> int(n_in_feature <span style=color:#f92672>*</span> used_feature_rate)
</span></span><span style=display:flex><span>        onehot <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>eye(n_in_feature)
</span></span><span style=display:flex><span>        using_idx <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>choice(np<span style=color:#f92672>.</span>arange(n_in_feature), n_used_feature, replace<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>feature_mask <span style=color:#f92672>=</span> onehot[using_idx]<span style=color:#f92672>.</span>T
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>feature_mask <span style=color:#f92672>=</span> Parameter(torch<span style=color:#f92672>.</span>from_numpy(self<span style=color:#f92672>.</span>feature_mask)<span style=color:#f92672>.</span>type(torch<span style=color:#f92672>.</span>FloatTensor), requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> jointly_training:
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>pi <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>rand(self<span style=color:#f92672>.</span>n_leaf, n_class)
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>pi <span style=color:#f92672>=</span> Parameter(torch<span style=color:#f92672>.</span>from_numpy(self<span style=color:#f92672>.</span>pi)<span style=color:#f92672>.</span>type(torch<span style=color:#f92672>.</span>FloatTensor), requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>pi <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>ones((self<span style=color:#f92672>.</span>n_leaf, n_class)) <span style=color:#f92672>/</span> n_class
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>pi <span style=color:#f92672>=</span> Parameter(torch<span style=color:#f92672>.</span>from_numpy(self<span style=color:#f92672>.</span>pi)<span style=color:#f92672>.</span>type(torch<span style=color:#f92672>.</span>FloatTensor), requires_grad<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>decision <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>Sequential(OrderedDict([
</span></span><span style=display:flex><span>            (<span style=color:#e6db74>&#39;linear1&#39;</span>, nn<span style=color:#f92672>.</span>Linear(n_used_feature, self<span style=color:#f92672>.</span>n_leaf)),
</span></span><span style=display:flex><span>            (<span style=color:#e6db74>&#39;sigmoid&#39;</span>, nn<span style=color:#f92672>.</span>Sigmoid()),
</span></span><span style=display:flex><span>        ]))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Forward pass for the tree.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            x (torch.Tensor): Input tensor.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            torch.Tensor: Output tensor after passing through the tree.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> x<span style=color:#f92672>.</span>is_cuda <span style=color:#f92672>and</span> <span style=color:#f92672>not</span> self<span style=color:#f92672>.</span>feature_mask<span style=color:#f92672>.</span>is_cuda:
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>feature_mask <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>feature_mask<span style=color:#f92672>.</span>cuda()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        feats <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>mm(x, self<span style=color:#f92672>.</span>feature_mask)
</span></span><span style=display:flex><span>        decision <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>decision(feats)
</span></span><span style=display:flex><span>        decision <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>unsqueeze(decision, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        decision_comp <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> decision
</span></span><span style=display:flex><span>        decision <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cat((decision, decision_comp), dim<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        batch_size <span style=color:#f92672>=</span> x<span style=color:#f92672>.</span>size()[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        _mu <span style=color:#f92672>=</span> Variable(x<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>new(batch_size, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>fill_(<span style=color:#ae81ff>1.</span>))
</span></span><span style=display:flex><span>        begin_idx <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        end_idx <span style=color:#f92672>=</span> <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> n_layer <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>0</span>, self<span style=color:#f92672>.</span>depth):
</span></span><span style=display:flex><span>            _mu <span style=color:#f92672>=</span> _mu<span style=color:#f92672>.</span>view(batch_size, <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>repeat(<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>            _decision <span style=color:#f92672>=</span> decision[:, begin_idx:end_idx, :]
</span></span><span style=display:flex><span>            _mu <span style=color:#f92672>=</span> _mu <span style=color:#f92672>*</span> _decision
</span></span><span style=display:flex><span>            begin_idx <span style=color:#f92672>=</span> end_idx
</span></span><span style=display:flex><span>            end_idx <span style=color:#f92672>=</span> begin_idx <span style=color:#f92672>+</span> <span style=color:#ae81ff>2</span> <span style=color:#f92672>**</span> (n_layer <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        mu <span style=color:#f92672>=</span> _mu<span style=color:#f92672>.</span>view(batch_size, self<span style=color:#f92672>.</span>n_leaf)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> mu
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>get_pi</span>(self):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;Returns the class probabilities for the leaf nodes.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> self<span style=color:#f92672>.</span>jointly_training:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> F<span style=color:#f92672>.</span>softmax(self<span style=color:#f92672>.</span>pi, dim<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> self<span style=color:#f92672>.</span>pi
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>cal_prob</span>(self, mu, pi):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;Calculates the probability of the input belonging to each class.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        p <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>mm(mu, pi)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> p
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>update_pi</span>(self, new_pi):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;Updates the class probabilities for the leaf nodes.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>pi<span style=color:#f92672>.</span>data <span style=color:#f92672>=</span> new_pi
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>Forest</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Forest class for building a decision forest.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, n_tree, tree_depth, n_in_feature, tree_feature_rate, n_class, jointly_training):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            n_tree (int): Number of trees in the forest.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            tree_depth (int): Depth of each tree.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            n_in_feature (int): Number of input features.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            tree_feature_rate (float): Fraction of features to use for each tree.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            n_class (int): Number of classes.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            jointly_training (bool): Whether to use joint training.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        super(Forest, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>trees <span style=color:#f92672>=</span> nn<span style=color:#f92672>.</span>ModuleList()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>n_tree <span style=color:#f92672>=</span> n_tree
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(n_tree):
</span></span><span style=display:flex><span>            tree <span style=color:#f92672>=</span> Tree(tree_depth, n_in_feature, tree_feature_rate, n_class, jointly_training)
</span></span><span style=display:flex><span>            self<span style=color:#f92672>.</span>trees<span style=color:#f92672>.</span>append(tree)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Forward pass for the forest.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            x (torch.Tensor): Input tensor.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            torch.Tensor: Output tensor after passing through the forest.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        probs <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> tree <span style=color:#f92672>in</span> self<span style=color:#f92672>.</span>trees:
</span></span><span style=display:flex><span>            mu <span style=color:#f92672>=</span> tree(x)
</span></span><span style=display:flex><span>            p <span style=color:#f92672>=</span> tree<span style=color:#f92672>.</span>cal_prob(mu, tree<span style=color:#f92672>.</span>get_pi())
</span></span><span style=display:flex><span>            probs<span style=color:#f92672>.</span>append(p<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>2</span>))
</span></span><span style=display:flex><span>        probs <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>cat(probs, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>        prob <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>sum(probs, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>) <span style=color:#f92672>/</span> self<span style=color:#f92672>.</span>n_tree
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> prob
</span></span></code></pre></div><h3 id=neural-decision-forest-class>Neural Decision Forest Class
<a class=anchor href=#neural-decision-forest-class>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>class</span> <span style=color:#a6e22e>NeuralDecisionForest</span>(nn<span style=color:#f92672>.</span>Module):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;Neural Decision Forest class combining the feature extraction layer and the forest.&#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> __init__(self, feature_layer, forest):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            feature_layer (nn.Module): Feature extraction layer.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            forest (nn.Module): Decision forest.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        super(NeuralDecisionForest, self)<span style=color:#f92672>.</span>__init__()
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>feature_layer <span style=color:#f92672>=</span> feature_layer
</span></span><span style=display:flex><span>        self<span style=color:#f92672>.</span>forest <span style=color:#f92672>=</span> forest
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>forward</span>(self, x):
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Forward pass for the neural decision forest.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            x (torch.Tensor): Input tensor.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>            torch.Tensor: Output tensor after passing through the neural decision forest.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>        out <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>feature_layer(x)
</span></span><span style=display:flex><span>        out <span style=color:#f92672>=</span> out<span style=color:#f92672>.</span>view(x<span style=color:#f92672>.</span>size()[<span style=color:#ae81ff>0</span>], <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        out <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>forest(out)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> out
</span></span></code></pre></div><h3 id=training-and-evaluation-functions>Training and Evaluation Functions
<a class=anchor href=#training-and-evaluation-functions>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>prepare_model</span>(opt):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Prepares the neural decision forest model.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        opt (dict): Dictionary containing model options.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        nn.Module: Neural decision forest model.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    feat_layer <span style=color:#f92672>=</span> CardioFeatureLayer(opt[<span style=color:#e6db74>&#39;feat_dropout&#39;</span>])
</span></span><span style=display:flex><span>    forest <span style=color:#f92672>=</span> Forest(n_tree<span style=color:#f92672>=</span>opt[<span style=color:#e6db74>&#39;n_tree&#39;</span>], tree_depth<span style=color:#f92672>=</span>opt[<span style=color:#e6db74>&#39;tree_depth&#39;</span>], n_in_feature<span style=color:#f92672>=</span>feat_layer<span style=color:#f92672>.</span>get_out_feature_size(),
</span></span><span style=display:flex><span>                    tree_feature_rate<span style=color:#f92672>=</span>opt[<span style=color:#e6db74>&#39;tree_feature_rate&#39;</span>], n_class<span style=color:#f92672>=</span>opt[<span style=color:#e6db74>&#39;n_class&#39;</span>],
</span></span><span style=display:flex><span>                    jointly_training<span style=color:#f92672>=</span>opt[<span style=color:#e6db74>&#39;jointly_training&#39;</span>])
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> NeuralDecisionForest(feat_layer, forest)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> opt[<span style=color:#e6db74>&#39;cuda&#39;</span>]:
</span></span><span style=display:flex><span>        model <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>cuda()
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        model <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>cpu()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> model
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>prepare_optim</span>(model, opt):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Prepares the optimizer for training.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        model (nn.Module): Neural decision forest model.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        opt (dict): Dictionary containing optimization options.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        torch.optim.Optimizer: Optimizer.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    params <span style=color:#f92672>=</span> [p <span style=color:#66d9ef>for</span> p <span style=color:#f92672>in</span> model<span style=color:#f92672>.</span>parameters() <span style=color:#66d9ef>if</span> p<span style=color:#f92672>.</span>requires_grad]
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> torch<span style=color:#f92672>.</span>optim<span style=color:#f92672>.</span>Adam(params, lr<span style=color:#f92672>=</span>opt[<span style=color:#e6db74>&#39;lr&#39;</span>], weight_decay<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-5</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>train</span>(model, optim, db, opt):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Trains the neural decision forest model.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        model (nn.Module): Neural decision forest model.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        optim (torch.optim.Optimizer): Optimizer.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        db (dict): Dictionary containing the datasets.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        opt (dict): Dictionary containing training options.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    best_val_loss <span style=color:#f92672>=</span> float(<span style=color:#e6db74>&#39;inf&#39;</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> epoch <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>1</span>, opt[<span style=color:#e6db74>&#39;epochs&#39;</span>] <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> opt[<span style=color:#e6db74>&#39;jointly_training&#39;</span>]:
</span></span><span style=display:flex><span>            print(<span style=color:#e6db74>&#34;Epoch </span><span style=color:#e6db74>%d</span><span style=color:#e6db74> : Two Stage Learning - Update PI&#34;</span> <span style=color:#f92672>%</span> (epoch))
</span></span><span style=display:flex><span>            cls_onehot <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>eye(opt[<span style=color:#e6db74>&#39;n_class&#39;</span>])
</span></span><span style=display:flex><span>            feat_batches <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>            target_batches <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>            train_loader <span style=color:#f92672>=</span> DataLoader(db[<span style=color:#e6db74>&#39;train&#39;</span>], batch_size<span style=color:#f92672>=</span>opt[<span style=color:#e6db74>&#39;batch_size&#39;</span>], shuffle<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>for</span> batch_idx, (data, target) <span style=color:#f92672>in</span> tqdm(enumerate(train_loader), total<span style=color:#f92672>=</span>len(train_loader), desc<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Updating PI&#34;</span>):
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>if</span> opt[<span style=color:#e6db74>&#39;cuda&#39;</span>]:
</span></span><span style=display:flex><span>                        data, target, cls_onehot <span style=color:#f92672>=</span> data<span style=color:#f92672>.</span>cuda(), target<span style=color:#f92672>.</span>cuda(), cls_onehot<span style=color:#f92672>.</span>cuda()
</span></span><span style=display:flex><span>                    data <span style=color:#f92672>=</span> Variable(data)
</span></span><span style=display:flex><span>                    feats <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>feature_layer(data)
</span></span><span style=display:flex><span>                    feats <span style=color:#f92672>=</span> feats<span style=color:#f92672>.</span>view(feats<span style=color:#f92672>.</span>size()[<span style=color:#ae81ff>0</span>], <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>                    feat_batches<span style=color:#f92672>.</span>append(feats)
</span></span><span style=display:flex><span>                    target_batches<span style=color:#f92672>.</span>append(cls_onehot[target])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>for</span> tree <span style=color:#f92672>in</span> model<span style=color:#f92672>.</span>forest<span style=color:#f92672>.</span>trees:
</span></span><span style=display:flex><span>                    mu_batches <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>for</span> feats <span style=color:#f92672>in</span> feat_batches:
</span></span><span style=display:flex><span>                        mu <span style=color:#f92672>=</span> tree(feats)
</span></span><span style=display:flex><span>                        mu_batches<span style=color:#f92672>.</span>append(mu)
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>20</span>):
</span></span><span style=display:flex><span>                        new_pi <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>zeros((tree<span style=color:#f92672>.</span>n_leaf, tree<span style=color:#f92672>.</span>n_class))
</span></span><span style=display:flex><span>                        <span style=color:#66d9ef>if</span> opt[<span style=color:#e6db74>&#39;cuda&#39;</span>]:
</span></span><span style=display:flex><span>                            new_pi <span style=color:#f92672>=</span> new_pi<span style=color:#f92672>.</span>cuda()
</span></span><span style=display:flex><span>                        <span style=color:#66d9ef>for</span> mu, target <span style=color:#f92672>in</span> zip(mu_batches, target_batches):
</span></span><span style=display:flex><span>                            pi <span style=color:#f92672>=</span> tree<span style=color:#f92672>.</span>get_pi()
</span></span><span style=display:flex><span>                            prob <span style=color:#f92672>=</span> tree<span style=color:#f92672>.</span>cal_prob(mu, pi)
</span></span><span style=display:flex><span>                            pi <span style=color:#f92672>=</span> pi<span style=color:#f92672>.</span>data
</span></span><span style=display:flex><span>                            prob <span style=color:#f92672>=</span> prob<span style=color:#f92672>.</span>data
</span></span><span style=display:flex><span>                            mu <span style=color:#f92672>=</span> mu<span style=color:#f92672>.</span>data
</span></span><span style=display:flex><span>                            _target <span style=color:#f92672>=</span> target<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>                            _pi <span style=color:#f92672>=</span> pi<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>                            _mu <span style=color:#f92672>=</span> mu<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>                            _prob <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>clamp(prob<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>1</span>), min<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-6</span>, max<span style=color:#f92672>=</span><span style=color:#ae81ff>1.</span>)
</span></span><span style=display:flex><span>                            _new_pi <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>mul(torch<span style=color:#f92672>.</span>mul(_target, _pi), _mu) <span style=color:#f92672>/</span> _prob
</span></span><span style=display:flex><span>                            new_pi <span style=color:#f92672>+=</span> torch<span style=color:#f92672>.</span>sum(_new_pi, dim<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>                        new_pi <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>softmax(Variable(new_pi), dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>data
</span></span><span style=display:flex><span>                        tree<span style=color:#f92672>.</span>update_pi(new_pi)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        model<span style=color:#f92672>.</span>train()
</span></span><span style=display:flex><span>        train_loader <span style=color:#f92672>=</span> DataLoader(db[<span style=color:#e6db74>&#39;train&#39;</span>], batch_size<span style=color:#f92672>=</span>opt[<span style=color:#e6db74>&#39;batch_size&#39;</span>], shuffle<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>        running_loss <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>
</span></span><span style=display:flex><span>        correct <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        total <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>with</span> tqdm(total<span style=color:#f92672>=</span>len(train_loader), desc<span style=color:#f92672>=</span><span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;Epoch </span><span style=color:#e6db74>{</span>epoch<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>) <span style=color:#66d9ef>as</span> pbar:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> batch_idx, (data, target) <span style=color:#f92672>in</span> enumerate(train_loader):
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> opt[<span style=color:#e6db74>&#39;cuda&#39;</span>]:
</span></span><span style=display:flex><span>                    data, target <span style=color:#f92672>=</span> data<span style=color:#f92672>.</span>cuda(), target<span style=color:#f92672>.</span>cuda()
</span></span><span style=display:flex><span>                data, target <span style=color:#f92672>=</span> Variable(data), Variable(target)
</span></span><span style=display:flex><span>                optim<span style=color:#f92672>.</span>zero_grad()
</span></span><span style=display:flex><span>                output <span style=color:#f92672>=</span> model(data)
</span></span><span style=display:flex><span>                loss <span style=color:#f92672>=</span> F<span style=color:#f92672>.</span>nll_loss(torch<span style=color:#f92672>.</span>log(output <span style=color:#f92672>+</span> <span style=color:#ae81ff>1e-6</span>), target)  <span style=color:#75715e># Add small epsilon to prevent NaNs</span>
</span></span><span style=display:flex><span>                loss<span style=color:#f92672>.</span>backward()
</span></span><span style=display:flex><span>                optim<span style=color:#f92672>.</span>step()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                running_loss <span style=color:#f92672>+=</span> loss<span style=color:#f92672>.</span>item()
</span></span><span style=display:flex><span>                pred <span style=color:#f92672>=</span> output<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>max(<span style=color:#ae81ff>1</span>, keepdim<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>                correct <span style=color:#f92672>+=</span> pred<span style=color:#f92672>.</span>eq(target<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>view_as(pred))<span style=color:#f92672>.</span>cpu()<span style=color:#f92672>.</span>sum()<span style=color:#f92672>.</span>item()
</span></span><span style=display:flex><span>                total <span style=color:#f92672>+=</span> target<span style=color:#f92672>.</span>size(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>                pbar<span style=color:#f92672>.</span>set_postfix({<span style=color:#e6db74>&#39;loss&#39;</span>: running_loss <span style=color:#f92672>/</span> (batch_idx <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>), <span style=color:#e6db74>&#39;accuracy&#39;</span>: correct <span style=color:#f92672>/</span> total})
</span></span><span style=display:flex><span>                pbar<span style=color:#f92672>.</span>update(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        val_loss, val_accuracy <span style=color:#f92672>=</span> evaluate(model, db[<span style=color:#e6db74>&#39;val&#39;</span>], opt, desc<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Validating&#34;</span>)
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Validation set: Average loss: </span><span style=color:#e6db74>{</span>val_loss<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>, Accuracy: </span><span style=color:#e6db74>{</span>val_accuracy<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Log metrics to wandb</span>
</span></span><span style=display:flex><span>        wandb<span style=color:#f92672>.</span>log({
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#39;epoch&#39;</span>: epoch,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#39;train_loss&#39;</span>: running_loss <span style=color:#f92672>/</span> len(train_loader),
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#39;train_accuracy&#39;</span>: <span style=color:#ae81ff>100.</span> <span style=color:#f92672>*</span> correct <span style=color:#f92672>/</span> total,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#39;val_loss&#39;</span>: val_loss,
</span></span><span style=display:flex><span>            <span style=color:#e6db74>&#39;val_accuracy&#39;</span>: val_accuracy
</span></span><span style=display:flex><span>        })
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># Save the best model</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> val_loss <span style=color:#f92672>&lt;</span> best_val_loss:
</span></span><span style=display:flex><span>            best_val_loss <span style=color:#f92672>=</span> val_loss
</span></span><span style=display:flex><span>            torch<span style=color:#f92672>.</span>save(model<span style=color:#f92672>.</span>state_dict(), <span style=color:#e6db74>&#39;best_model_7.pth&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Load the best model and make predictions on the test set</span>
</span></span><span style=display:flex><span>    model<span style=color:#f92672>.</span>load_state_dict(torch<span style=color:#f92672>.</span>load(<span style=color:#e6db74>&#39;best_model_7.pth&#39;</span>))
</span></span><span style=display:flex><span>    test_loss, test_accuracy <span style=color:#f92672>=</span> evaluate(model, db[<span style=color:#e6db74>&#39;test&#39;</span>], opt, desc<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Testing&#34;</span>)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>Test set: Average loss: </span><span style=color:#e6db74>{</span>test_loss<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>, Accuracy: </span><span style=color:#e6db74>{</span>test_accuracy<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74>&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Log test metrics to wandb</span>
</span></span><span style=display:flex><span>    wandb<span style=color:#f92672>.</span>log({
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;test_loss&#39;</span>: test_loss,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;test_accuracy&#39;</span>: test_accuracy
</span></span><span style=display:flex><span>    })
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Predict on the test set and print results</span>
</span></span><span style=display:flex><span>    predictions <span style=color:#f92672>=</span> predict(model, db[<span style=color:#e6db74>&#39;test&#39;</span>], opt)
</span></span><span style=display:flex><span>    print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#39;Test set predictions:</span><span style=color:#ae81ff>\n</span><span style=color:#e6db74> </span><span style=color:#e6db74>{</span>predictions<span style=color:#e6db74>}</span><span style=color:#e6db74>&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># SHAP explanations</span>
</span></span><span style=display:flex><span>    shap_explainer(model, db[<span style=color:#e6db74>&#39;test_df&#39;</span>], opt)
</span></span></code></pre></div><h3 id=evaluation-and-prediction-functions>Evaluation and Prediction Functions
<a class=anchor href=#evaluation-and-prediction-functions>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>evaluate</span>(model, dataset, opt, desc<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;Evaluating&#34;</span>):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Evaluates the model on the given dataset.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        model (nn.Module): Neural decision forest model.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        dataset (Dataset): Dataset to evaluate on.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        opt (dict): Dictionary containing evaluation options.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        desc (str): Description for the progress bar.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        tuple: (average loss, accuracy)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    model<span style=color:#f92672>.</span>eval()
</span></span><span style=display:flex><span>    loader <span style=color:#f92672>=</span> DataLoader(dataset, batch_size<span style=color:#f92672>=</span>opt[<span style=color:#e6db74>&#39;batch_size&#39;</span>], shuffle<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>    test_loss <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    correct <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> tqdm(total<span style=color:#f92672>=</span>len(loader), desc<span style=color:#f92672>=</span>desc) <span style=color:#66d9ef>as</span> pbar:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>for</span> data, target <span style=color:#f92672>in</span> loader:
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>if</span> opt[<span style=color:#e6db74>&#39;cuda&#39;</span>]:
</span></span><span style=display:flex><span>                    data, target <span style=color:#f92672>=</span> data<span style=color:#f92672>.</span>cuda(), target<span style=color:#f92672>.</span>cuda()
</span></span><span style=display:flex><span>                data, target <span style=color:#f92672>=</span> Variable(data), Variable(target)
</span></span><span style=display:flex><span>                output <span style=color:#f92672>=</span> model(data)
</span></span><span style=display:flex><span>                test_loss <span style=color:#f92672>+=</span> F<span style=color:#f92672>.</span>nll_loss(torch<span style=color:#f92672>.</span>log(output <span style=color:#f92672>+</span> <span style=color:#ae81ff>1e-6</span>), target, reduction<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;sum&#39;</span>)<span style=color:#f92672>.</span>item()  <span style=color:#75715e># Add small epsilon to prevent NaNs</span>
</span></span><span style=display:flex><span>                pred <span style=color:#f92672>=</span> output<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>max(<span style=color:#ae81ff>1</span>, keepdim<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>                correct <span style=color:#f92672>+=</span> pred<span style=color:#f92672>.</span>eq(target<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>view_as(pred))<span style=color:#f92672>.</span>cpu()<span style=color:#f92672>.</span>sum()<span style=color:#f92672>.</span>item()
</span></span><span style=display:flex><span>                pbar<span style=color:#f92672>.</span>set_postfix({<span style=color:#e6db74>&#39;val_loss&#39;</span>: test_loss <span style=color:#f92672>/</span> len(loader<span style=color:#f92672>.</span>dataset), <span style=color:#e6db74>&#39;val_accuracy&#39;</span>: correct <span style=color:#f92672>/</span> len(loader<span style=color:#f92672>.</span>dataset)})
</span></span><span style=display:flex><span>                pbar<span style=color:#f92672>.</span>update(<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    test_loss <span style=color:#f92672>/=</span> len(loader<span style=color:#f92672>.</span>dataset)
</span></span><span style=display:flex><span>    accuracy <span style=color:#f92672>=</span> correct <span style=color:#f92672>/</span> len(loader<span style=color:#f92672>.</span>dataset)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> test_loss, accuracy
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict</span>(model, dataset, opt):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Predicts the class labels for the given dataset.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        model (nn.Module): Neural decision forest model.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        dataset (Dataset): Dataset to predict on.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        opt (dict): Dictionary containing prediction options.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Returns:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        list: List of predicted class labels.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    model<span style=color:#f92672>.</span>eval()
</span></span><span style=display:flex><span>    loader <span style=color:#f92672>=</span> DataLoader(dataset, batch_size<span style=color:#f92672>=</span>opt[<span style=color:#e6db74>&#39;batch_size&#39;</span>], shuffle<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>    predictions <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> data, _ <span style=color:#f92672>in</span> loader:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> opt[<span style=color:#e6db74>&#39;cuda&#39;</span>]:
</span></span><span style=display:flex><span>                data <span style=color:#f92672>=</span> data<span style=color:#f92672>.</span>cuda()
</span></span><span style=display:flex><span>            data <span style=color:#f92672>=</span> Variable(data)
</span></span><span style=display:flex><span>            output <span style=color:#f92672>=</span> model(data)
</span></span><span style=display:flex><span>            pred <span style=color:#f92672>=</span> output<span style=color:#f92672>.</span>data<span style=color:#f92672>.</span>max(<span style=color:#ae81ff>1</span>, keepdim<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>            predictions<span style=color:#f92672>.</span>extend(pred<span style=color:#f92672>.</span>cpu()<span style=color:#f92672>.</span>numpy()<span style=color:#f92672>.</span>flatten())
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> predictions
</span></span></code></pre></div><h3 id=shap-explanations>SHAP Explanations
<a class=anchor href=#shap-explanations>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>shap_explainer</span>(model, dataset, opt):
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Generates SHAP explanations for the model predictions.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Args:
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        model (nn.Module): Neural decision forest model.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        dataset (dict): Dictionary containing the test dataset.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>        opt (dict): Dictionary containing SHAP options.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>def</span> <span style=color:#a6e22e>model_predict</span>(data):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>array(predict(model, data, opt))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    X_test <span style=color:#f92672>=</span> dataset[<span style=color:#e6db74>&#34;test&#34;</span>]<span style=color:#f92672>.</span>X  <span style=color:#75715e># Use the dataset&#39;s features for SHAP values</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    X_sample <span style=color:#f92672>=</span> X_test[np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>, len(X_test), <span style=color:#ae81ff>100</span>)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    explainer <span style=color:#f92672>=</span> shap<span style=color:#f92672>.</span>KernelExplainer(model_predict, X_sample)
</span></span><span style=display:flex><span>    shap_values <span style=color:#f92672>=</span> explainer<span style=color:#f92672>.</span>shap_values(X_sample, nsamples<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Plot the summary plot</span>
</span></span><span style=display:flex><span>    shap<span style=color:#f92672>.</span>summary_plot(shap_values, X_sample, plot_type<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;bar&#34;</span>, 
</span></span><span style=display:flex><span>                      feature_names<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;age&#39;</span>, <span style=color:#e6db74>&#39;gender&#39;</span>, <span style=color:#e6db74>&#39;height&#39;</span>, <span style=color:#e6db74>&#39;weight&#39;</span>, <span style=color:#e6db74>&#39;ap_hi&#39;</span>, <span style=color:#e6db74>&#39;ap_lo&#39;</span>, <span style=color:#e6db74>&#39;cholesterol&#39;</span>, <span style=color:#e6db74>&#39;gluc&#39;</span>, <span style=color:#e6db74>&#39;smoke&#39;</span>, <span style=color:#e6db74>&#39;alco&#39;</span>, <span style=color:#e6db74>&#39;active&#39;</span>])
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    shap<span style=color:#f92672>.</span>summary_plot(shap_values, features<span style=color:#f92672>=</span>X_sample, class_inds<span style=color:#f92672>=</span>[<span style=color:#ae81ff>1</span>], max_display<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>, 
</span></span><span style=display:flex><span>                      feature_names<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;age&#39;</span>, <span style=color:#e6db74>&#39;gender&#39;</span>, <span style=color:#e6db74>&#39;height&#39;</span>, <span style=color:#e6db74>&#39;weight&#39;</span>, <span style=color:#e6db74>&#39;ap_hi&#39;</span>, <span style=color:#e6db74>&#39;ap_lo&#39;</span>, <span style=color:#e6db74>&#39;cholesterol&#39;</span>, <span style=color:#e6db74>&#39;gluc&#39;</span>, <span style=color:#e6db74>&#39;smoke&#39;</span>, <span style=color:#e6db74>&#39;alco&#39;</span>, <span style=color:#e6db74>&#39;active&#39;</span>])
</span></span></code></pre></div><h3 id=main-function>Main Function
<a class=anchor href=#main-function>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>main</span>():
</span></span><span style=display:flex><span>    <span style=color:#e6db74>&#34;&#34;&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    Main function to initialize wandb, prepare data, model, optimizer, and start training.
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;&#34;&#34;</span>
</span></span><span style=display:flex><span>    wandb<span style=color:#f92672>.</span>init(project<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;cardio_prediction&#34;</span>, config<span style=color:#f92672>=</span>{
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;batch_size&#39;</span>: <span style=color:#ae81ff>128</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;feat_dropout&#39;</span>: <span style=color:#ae81ff>0.3</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;n_tree&#39;</span>: <span style=color:#ae81ff>5</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;tree_depth&#39;</span>: <span style=color:#ae81ff>3</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;n_class&#39;</span>: <span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;tree_feature_rate&#39;</span>: <span style=color:#ae81ff>0.5</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;lr&#39;</span>: <span style=color:#ae81ff>0.001</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;gpuid&#39;</span>: <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;jointly_training&#39;</span>: <span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;epochs&#39;</span>: <span style=color:#ae81ff>20</span>,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;report_every&#39;</span>: <span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>    })
</span></span><span style=display:flex><span>    config <span style=color:#f92672>=</span> wandb<span style=color:#f92672>.</span>config
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># Manually define arguments</span>
</span></span><span style=display:flex><span>    opt <span style=color:#f92672>=</span> {
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;batch_size&#39;</span>: config<span style=color:#f92672>.</span>batch_size,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;feat_dropout&#39;</span>: config<span style=color:#f92672>.</span>feat_dropout,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;n_tree&#39;</span>: config<span style=color:#f92672>.</span>n_tree,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;tree_depth&#39;</span>: config<span style=color:#f92672>.</span>tree_depth,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;n_class&#39;</span>: config<span style=color:#f92672>.</span>n_class,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;tree_feature_rate&#39;</span>: config<span style=color:#f92672>.</span>tree_feature_rate,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;lr&#39;</span>: config<span style=color:#f92672>.</span>lr,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;gpuid&#39;</span>: config<span style=color:#f92672>.</span>gpuid,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;jointly_training&#39;</span>: config<span style=color:#f92672>.</span>jointly_training,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;epochs&#39;</span>: config<span style=color:#f92672>.</span>epochs,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;report_every&#39;</span>: config<span style=color:#f92672>.</span>report_every,
</span></span><span style=display:flex><span>        <span style=color:#e6db74>&#39;cuda&#39;</span>: torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>is_available()
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> opt[<span style=color:#e6db74>&#39;gpuid&#39;</span>] <span style=color:#f92672>&gt;=</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>        torch<span style=color:#f92672>.</span>cuda<span style=color:#f92672>.</span>set_device(opt[<span style=color:#e6db74>&#39;gpuid&#39;</span>])
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;WARNING: RUN WITHOUT GPU&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    db <span style=color:#f92672>=</span> prepare_db(<span style=color:#e6db74>&#39;cardio_train.csv&#39;</span>)
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> prepare_model(opt)
</span></span><span style=display:flex><span>    optim <span style=color:#f92672>=</span> prepare_optim(model, opt)
</span></span><span style=display:flex><span>    train(model, optim, db, opt)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>if</span> __name__ <span style=color:#f92672>==</span> <span style=color:#e6db74>&#39;__main__&#39;</span>:
</span></span><span style=display:flex><span>    main()
</span></span></code></pre></div><h2 id=results>Results
<a class=anchor href=#results>#</a></h2><h3 id=model-performance>Model Performance
<a class=anchor href=#model-performance>#</a></h3><p>The model demonstrated a steady improvement in training accuracy, reaching approximately 74% by the 20th epoch. Validation accuracy showed a similar trend, indicating that the model was effectively learning from the training data without overfitting.</p><h4 id=training-accuracy>Training Accuracy
<a class=anchor href=#training-accuracy>#</a></h4><p align=center><img src=/DNDFs_SHAP/training_accuracy.jpg alt="Training Accuracy"><p align=center><h5><font color=DimGray><center>Training Accuracy</center></font></h5><h4 id=training-and-validation-metrics>Training and Validation Metrics
<a class=anchor href=#training-and-validation-metrics>#</a></h4><p align=center><img src=/DNDFs_SHAP/validation_metrics.jpg alt="Validation Accuracy, Training Loss, Validation Loss"><p align=center><h5><font color=DimGray><center>Validation Accuracy, Training Loss, Validation Loss</center></font></h5><h3 id=shap-values>SHAP Values
<a class=anchor href=#shap-values>#</a></h3><p>Tree SHAP is an algorithm used to compute exact SHAP values for Decision Tree-based models. SHAP (SHapley Additive exPlanation) is a game-theoretic approach to explain the output of any machine learning model. The goal of SHAP is to explain the prediction for any instance ( x_i ) as a sum of contributions from its individual feature values.</p><p align=center><img src=/DNDFs_SHAP/box.jpg alt="Black and White Boxs"><p align=center><p>As explained in the first article, SHAP values are obtained from the following equation:</p><p align=center><img src=/DNDFs_SHAP/3.jpg alt="SHAP Equation"><p align=center><h5><font color=DimGray><center>SHAP Equation</center></font></h5><p>This method is part of the additive feature attribution methods class; feature attribution refers to the fact that the change of an outcome to be explained (e.g., a class probability in a classification problem) with respect to a baseline (e.g., average prediction probability for that class in the training set) can be attributed in different proportions to the model input features.</p><h4 id=shap-interaction-values>SHAP Interaction Values
<a class=anchor href=#shap-interaction-values>#</a></h4><p>SHAP allows us to compute interaction effects by considering pairwise feature attributions. This leads to a matrix of attribution values representing the impact of all pairs of features on a given model prediction. SHAP interaction effect is based on the Shapley interaction index from game theory and is given by:</p><p align=center><img src=/DNDFs_SHAP/4.jpg alt="SHAP Interaction Values"><p align=center><h5><font color=DimGray><center>SHAP Interaction Values</center></font></h5><p>The above equation indicates that the SHAP interaction value of the ( i )-th feature with respect to the ( j )-th feature can be interpreted as the difference between SHAP values of the ( i )-th feature with and without the ( j )-th feature. This allows us to use the algorithm for computing SHAP values to compute SHAP interaction values.</p><h3 id=challenges>Challenges
<a class=anchor href=#challenges>#</a></h3><p>Training on a real dataset presented several additional challenges:</p><ul><li><strong>Computational Resources</strong>: Training the model on a larger real-world dataset required substantial computational resources. Efficient use of GPU acceleration and parallel processing was necessary to manage training times.</li><li><strong>SHAP Integration</strong>: Integrating SHAP for model explainability was challenging due to the custom architecture of the neural decision forest. Ensuring compatibility and efficient computation of SHAP values required careful handling.</li></ul><h3 id=insights-from-shap>Insights from SHAP
<a class=anchor href=#insights-from-shap>#</a></h3><p>The integration of SHAP provided valuable insights into the model&rsquo;s predictions. The SHAP summary plot highlighted the most influential features in the model&rsquo;s decision-making process.</p><p>From the plot, we can observe that the top features impacting the model&rsquo;s predictions are:</p><ol><li>Systolic Blood Pressure (ap_hi)</li><li>Cholesterol Levels</li><li>Age</li><li>Weight</li><li>Diastolic Blood Pressure (ap_lo)</li></ol><p>These features have the highest mean absolute SHAP values, indicating their significant influence on the prediction of cardiovascular disease.</p><h4 id=shap-summary-plot>SHAP Summary Plot
<a class=anchor href=#shap-summary-plot>#</a></h4><p align=center><img src=/DNDFs_SHAP/5.png alt="SHAP Summary Plot"><p align=center><h5><font color=DimGray><center>SHAP Summary Plot</center></font></h5><h4 id=shap-beeswarm-plot>SHAP Beeswarm Plot
<a class=anchor href=#shap-beeswarm-plot>#</a></h4><p align=center><img src=/DNDFs_SHAP/6.png alt="SHAP Beeswarm Plot"><p align=center><h5><font color=DimGray><center>SHAP Beeswarm Plot</center></font></h5><h2 id=conclusion>Conclusion
<a class=anchor href=#conclusion>#</a></h2><p>Deep Neural Decision Forests offer a powerful combination of decision trees and neural networks, providing both high performance and interpretability. The addition of SHAP values further enhances the model&rsquo;s transparency, making it easier to understand and trust the predictions.</p><p>For those interested in further exploration, the full implementation is provided, including the training process and SHAP integration for interpretability.</p><p>Happy coding!</p><h2 id=references>References
<a class=anchor href=#references>#</a></h2><ul><li><a href=https://www.kaggle.com/datasets/sulianova/cardiovascular-disease-dataset>Kaggle Dataset: Cardiovascular Disease Dataset</a></li><li><a href=https://github.com/jingxil/Neural-Decision-Forests/blob/master/README.md>GitHub: Neural Decision Forests</a></li><li><a href=https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Kontschieder_Deep_Neural_Decision_ICCV_2015_paper.pdf>Paper: Deep Neural Decision Forests</a></li><li><a href=https://medium.com/analytics-vidhya/shap-part-3-tree-shap-3af9bcd7cd9b>Tree Shap</a></li><li><a href=https://kushalmukherjee.medium.com/deep-neural-decision-forest-in-keras-60134d270bfe>Deep Neural Decision Forest</a></li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/IU-PR/Capstone_project/tree/master//content/docs/Groups/DNDFs_SHAP.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#deep-neural-decision-forests-dndfs-with-shap-values>Deep Neural Decision Forests (DNDFs) with SHAP Values</a><ul><li><a href=#introduction>Introduction</a></li><li><a href=#formulas-involved>Formulas Involved</a></li><li><a href=#learning-process>Learning Process</a></li><li><a href=#dataset>Dataset</a></li><li><a href=#model-description>Model Description</a><ul><li><a href=#model-components>Model Components</a></li></ul></li><li><a href=#implementation>Implementation</a><ul><li><a href=#dataset-class>Dataset Class</a></li><li><a href=#feature-layer>Feature Layer</a></li><li><a href=#tree-and-forest-classes>Tree and Forest Classes</a></li><li><a href=#neural-decision-forest-class>Neural Decision Forest Class</a></li><li><a href=#training-and-evaluation-functions>Training and Evaluation Functions</a></li><li><a href=#evaluation-and-prediction-functions>Evaluation and Prediction Functions</a></li><li><a href=#shap-explanations>SHAP Explanations</a></li><li><a href=#main-function>Main Function</a></li></ul></li><li><a href=#results>Results</a><ul><li><a href=#model-performance>Model Performance</a></li><li><a href=#shap-values>SHAP Values</a></li><li><a href=#challenges>Challenges</a></li><li><a href=#insights-from-shap>Insights from SHAP</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#references>References</a></li></ul></li></ul></nav></div></aside></main></body></html>