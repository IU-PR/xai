<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="TorchPRISM # Table of Contents
TorchPRISM Introduction: Unlocking CNNs with PRISM Understanding CNNs Introducing PRISM: A Glimpse into CNN Decision-Making Implementation of PRISM How to use Examples VGG11 ResNet101 GoogleNet Introduction: Unlocking CNNs with PRISM # Convolutional Neural Networks (CNNs) have revolutionized computer vision, powering innovations from facial recognition to autonomous vehicles. Yet, their decision-making process remains a mystery, hindering trust and understanding.
Inspired by the paper &ldquo;Unlocking the black box of CNNs: Visualising the decision-making process with PRISM,&rdquo; our blog sets out to demystify CNNs&rsquo; decisions."><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:title" content><meta property="og:description" content="TorchPRISM # Table of Contents
TorchPRISM Introduction: Unlocking CNNs with PRISM Understanding CNNs Introducing PRISM: A Glimpse into CNN Decision-Making Implementation of PRISM How to use Examples VGG11 ResNet101 GoogleNet Introduction: Unlocking CNNs with PRISM # Convolutional Neural Networks (CNNs) have revolutionized computer vision, powering innovations from facial recognition to autonomous vehicles. Yet, their decision-making process remains a mystery, hindering trust and understanding.
Inspired by the paper “Unlocking the black box of CNNs: Visualising the decision-making process with PRISM,” our blog sets out to demystify CNNs’ decisions."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>Torch Prism | XAI</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.e832d4e94212199857473bcf13a450d089c3fcd54ccadedcfac84ed0feff83fb.css integrity="sha256-6DLU6UISGZhXRzvPE6RQ0InD/NVMyt7c+shO0P7/g/s=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/mathtex-script-type.min.js integrity=sha384-jiBVvJ8NGGj5n7kJaiWwWp9AjC+Yh8rhZY3GtAX8yU28azcLgoRo4oukO87g7zDT crossorigin=anonymous></script><script defer src=/flexsearch.min.js></script><script defer src=/en.search.min.7d9f26c474a08432d3b0cb4f498e61f362a17ce5ef9d59f294f4d08b34fc8ae8.js integrity="sha256-fZ8mxHSghDLTsMtPSY5h82KhfOXvnVnylPTQizT8iug=" crossorigin=anonymous></script><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/YELLOW_BAR.png alt=Logo><span><b>XAI</b></span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=/docs/groups/cam_and_secam/>CAM and SeCAM</a></li><li><a href=/docs/groups/diffusion-lens-interpreting-text-encoders-in-text-to-image-pipelines-tuned-using-dreambooth/>Diffusion Lens: Interpreting Text Encoders in Text-to-Image pipelines</a></li><li><a href=/docs/groups/dimensionality-reduction-in-nlp-visualizing-sentence-embeddings-with-umap-and-t-sne/>Dimensionality Reduction in NLP: Visualizing Sentence Embeddings with UMAP and t-SNE</a></li><li><a href=/docs/groups/example/>Example</a></li><li><a href=/docs/groups/ai-playing-geoguessr-explained/>Ai Playing Geo Guessr Explained</a></li><li><a href=/docs/groups/contrastive-grad-cam-consistency/>Contrastive Grad Cam Consistency</a></li><li><a href=/docs/groups/dndfs_shap/>Dndfs Shap</a></li><li><a href=/docs/groups/gradcam/>Grad Cam</a></li><li><a href=/docs/groups/integrated-gradients/>Integrated Gradients</a></li><li><a href=/docs/groups/kernel-shap/>Kernel Shap</a></li><li><a href=/docs/groups/rag/>Rag</a></li><li><a href=/docs/groups/shap_darya_and_viktoria/>Shap Darya and Viktoria</a></li><li><a href=/docs/groups/sverl_tac_toe/>Sverl Tac Toe</a></li><li><a href=/docs/groups/torchprism/ class=active>Torch Prism</a></li><li><a href=/docs/groups/xai_for_transformers/>Xai for Transformers</a></li><li><a href=/docs/groups/scorecam_for_cyclegan/>Score-CAM for CycleGAN</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Torch Prism</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#torchprism>TorchPRISM</a><ul><li><a href=#introduction-unlocking-cnns-with-prism>Introduction: Unlocking CNNs with PRISM</a></li><li><a href=#understanding-cnns>Understanding CNNs</a></li><li><a href=#introducing-prism-a-glimpse-into-cnn-decision-making>Introducing PRISM: A Glimpse into CNN Decision-Making</a></li><li><a href=#implementation-of-prism>Implementation of PRISM</a></li><li><a href=#how-to-use>How to use</a></li><li><a href=#code>Code</a></li><li><a href=#examples>Examples</a><ul><li><a href=#vgg11>VGG11</a></li><li><a href=#resnet101>ResNet101</a></li><li><a href=#googlenet>GoogleNet</a></li></ul></li></ul></li></ul></nav></aside></header><article class=markdown><h1 id=torchprism>TorchPRISM
<a class=anchor href=#torchprism>#</a></h1><p><strong>Table of Contents</strong></p><ul><li><a href=/#torchprism>TorchPRISM</a><ul><li><a href=/#introduction-unlocking-cnns-with-prism>Introduction: Unlocking CNNs with PRISM</a></li><li><a href=/#understanding-cnns>Understanding CNNs</a></li><li><a href=/#introducing-prism-a-glimpse-into-cnn-decision-making>Introducing PRISM: A Glimpse into CNN Decision-Making</a></li><li><a href=/#implementation-of-prism>Implementation of PRISM</a></li><li><a href=/#how-to-use>How to use</a></li><li><a href=/#examples>Examples</a><ul><li><a href=/#vgg11>VGG11</a></li><li><a href=/#resnet101>ResNet101</a></li><li><a href=/#googlenet>GoogleNet</a></li></ul></li></ul></li></ul><h2 id=introduction-unlocking-cnns-with-prism>Introduction: Unlocking CNNs with PRISM
<a class=anchor href=#introduction-unlocking-cnns-with-prism>#</a></h2><p>Convolutional Neural Networks (CNNs) have revolutionized computer vision, powering innovations from facial recognition to autonomous vehicles. Yet, their decision-making process remains a mystery, hindering trust and understanding.</p><p>Inspired by the paper &ldquo;Unlocking the black box of CNNs: Visualising the decision-making process with PRISM,&rdquo; our blog sets out to demystify CNNs&rsquo; decisions.</p><p>In this short intro, we&rsquo;ll touch on CNN basics, the need for transparency, and introduce PRISM as our tool of choice for visualizing CNN decisions. Get ready to see CNNs in a new light!</p><h2 id=understanding-cnns>Understanding CNNs
<a class=anchor href=#understanding-cnns>#</a></h2><p>CNNs are the backbone of modern computer vision, mimicking the human visual system to recognize patterns and features in images. At their core, CNNs consist of layers of neurons organized in a hierarchical fashion, each layer extracting increasingly complex features from the input data.</p><p><strong>Why Interpretability Matters:</strong> While CNNs excel at tasks like image classification and object detection, their inner workings often remain inscrutable. This lack of transparency raises concerns about bias, fairness, and reliability in AI systems. Understanding how CNNs arrive at their decisions is crucial for ensuring accountability and trust.</p><p><strong>Visualizing CNNs:</strong> Techniques like PRISM offer a window into CNN decision-making. By visualizing the activations of individual neurons and feature maps across different layers of the network, PRISM helps unravel the thought process behind CNN predictions.</p><p><img src=/TorchPRISM/cnn_visualization.webp alt="CNN Visualization"></p><h2 id=introducing-prism-a-glimpse-into-cnn-decision-making>Introducing PRISM: A Glimpse into CNN Decision-Making
<a class=anchor href=#introducing-prism-a-glimpse-into-cnn-decision-making>#</a></h2><p><strong>Meet PRISM:</strong> Predictive, Interactive, Summarisation, and Modelling. PRISM isn&rsquo;t just a tool; it&rsquo;s a key to unlocking the black box of CNN decision-making.</p><p><strong>Predictive:</strong> PRISM enables us to predict and understand how CNNs arrive at their decisions by visualizing the activation patterns within the network.</p><p><strong>Interactive:</strong> With PRISM, exploring CNN decision-making is not a passive experience. It&rsquo;s an interactive journey where we can manipulate inputs, observe neuron activations, and gain insights into the network&rsquo;s inner workings.</p><p><strong>Summarisation:</strong> PRISM doesn&rsquo;t overwhelm us with complex data. Instead, it distills the essence of CNN decision-making into intuitive visualizations that highlight the most influential features and neurons.</p><p><strong>Modelling:</strong> Through PRISM, we model and interpret the decision-making process of CNNs, shedding light on their behavior and paving the way for more transparent and accountable AI systems.</p><h2 id=implementation-of-prism>Implementation of PRISM
<a class=anchor href=#implementation-of-prism>#</a></h2><p>Implementing PRISM brings us closer to understanding the intricate decision-making processes of Convolutional Neural Networks (CNNs). Let&rsquo;s explore how to harness the power of PRISM to visualize CNN activations and gain valuable insights.</p><p><strong>Step 1: Data Preparation:</strong>
Prepare the input data and the trained CNN model you want to analyze. Ensure that the data is in a format compatible with your chosen deep learning framework.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># crop image for the model input</span>
</span></span><span style=display:flex><span>_crop <span style=color:#f92672>=</span> transforms<span style=color:#f92672>.</span>Compose([
</span></span><span style=display:flex><span>    transforms<span style=color:#f92672>.</span>ToPILImage(),
</span></span><span style=display:flex><span>    transforms<span style=color:#f92672>.</span>Resize((<span style=color:#ae81ff>224</span>, <span style=color:#ae81ff>224</span>))
</span></span><span style=display:flex><span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># normalize image for model input on which it was trained</span>
</span></span><span style=display:flex><span>_normalize <span style=color:#f92672>=</span> transforms<span style=color:#f92672>.</span>Compose([
</span></span><span style=display:flex><span>    transforms<span style=color:#f92672>.</span>ToTensor(),
</span></span><span style=display:flex><span>    transforms<span style=color:#f92672>.</span>Normalize([<span style=color:#ae81ff>0.485</span>, <span style=color:#ae81ff>0.456</span>, <span style=color:#ae81ff>0.406</span>], [<span style=color:#ae81ff>0.229</span>, <span style=color:#ae81ff>0.224</span>, <span style=color:#ae81ff>0.225</span>])
</span></span><span style=display:flex><span>])
</span></span></code></pre></div><p>If you have different data preprocessing steps add them here.</p><p><strong>Step 2: Model Loading:</strong>
Load the pre-trained CNN model into your preferred deep learning framework, such as PyTorch. This model will serve as the basis for visualizing activations with PRISM.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;vgg11&#39;</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> models<span style=color:#f92672>.</span>get_model(model_name, weights<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span></code></pre></div><p><strong>Step 3: PRISM:</strong></p><p>The proposed technique uses PCA of features detected by neural network models to create an RGB coloured image mask that highlights the features identified by the model. PRISM can be used for better human interpretation of neural network representations and to automate the identification of ambiguous class features. The combination of PRISM with another method, Gradual Extrapolation, results in an image showing each segment of a classified object in different colours. PRISM can help identify indistinct classes and improve the real-world application of the model.</p><p>Generating PRISM results consists of simple matrix manipulation and computation of the PCA (Fig. 1). First, we transform the output from the chosen layer of the model into a two-dimensional matrix. Each channel becomes a single column in the resulting matrix. In this matrix, we computed the PCA and cut off all PCs beyond the first three. In the last step, we transform these three PCs back into channel matrices to assign later colours red, green, and blue to make them visually distinguishable.</p><p><img src=/TorchPRISM/prism_algorithm.png alt="alt text"></p><p><img src=/TorchPRISM/prism1.png alt="alt text"></p><p><strong>Step 4: PRISM Implementation:</strong></p><p>Get top three PC&rsquo;s for RGB color.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_get_pc</span>(self, final_excitation):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    final_layer_input <span style=color:#f92672>=</span> final_excitation<span style=color:#f92672>.</span>permute(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>reshape(
</span></span><span style=display:flex><span>        <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>, final_excitation<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>1</span>]
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    normalized_final_layer_input <span style=color:#f92672>=</span> final_layer_input <span style=color:#f92672>-</span> final_layer_input<span style=color:#f92672>.</span>mean(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    u, s, v <span style=color:#f92672>=</span> normalized_final_layer_input<span style=color:#f92672>.</span>svd(compute_uv<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    self<span style=color:#f92672>.</span>_variances <span style=color:#f92672>=</span> s<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span><span style=color:#f92672>/</span>sum(s<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span>) <span style=color:#75715e># save the variance</span>
</span></span><span style=display:flex><span>    raw_features <span style=color:#f92672>=</span> u[:, :<span style=color:#ae81ff>3</span>]<span style=color:#f92672>.</span>matmul(s[:<span style=color:#ae81ff>3</span>]<span style=color:#f92672>.</span>diag())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> raw_features<span style=color:#f92672>.</span>view(
</span></span><span style=display:flex><span>        final_excitation<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>],
</span></span><span style=display:flex><span>        final_excitation<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>2</span>],
</span></span><span style=display:flex><span>        final_excitation<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>3</span>],
</span></span><span style=display:flex><span>        <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>    )<span style=color:#f92672>.</span>permute(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>2</span>)
</span></span></code></pre></div><p>Use PC to perform PRISM.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>prism</span>(self, grad_extrap<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> <span style=color:#f92672>not</span> self<span style=color:#f92672>.</span>_excitations:
</span></span><span style=display:flex><span>        print(<span style=color:#e6db74>&#34;No data in hooks. Have You used `register_hooks(model)` method?&#34;</span>)
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>with</span> torch<span style=color:#f92672>.</span>no_grad():
</span></span><span style=display:flex><span>        rgb_features_map <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>_get_pc(self<span style=color:#f92672>.</span>_excitations<span style=color:#f92672>.</span>pop())
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> grad_extrap:
</span></span><span style=display:flex><span>            rgb_features_map <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>_upsampling(
</span></span><span style=display:flex><span>                rgb_features_map, self<span style=color:#f92672>.</span>_excitations
</span></span><span style=display:flex><span>            )
</span></span><span style=display:flex><span>        rgb_features_map <span style=color:#f92672>=</span> self<span style=color:#f92672>.</span>_normalize_to_rgb(rgb_features_map)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> rgb_features_map
</span></span></code></pre></div><p><img src=/TorchPRISM/basic_prism.png alt="Basic PRISM"></p><p>Basic PRISM outputs RGB image according to last layer. To get accurate output we do upsampling the output to original image size. With upsampling it is called Gradual Extrapolated PRISM.</p><p><strong>Gradual Extrapolation</strong> is based on the concept that a map considers the size of the preceding layer. This result is then multiplied by a matrix denoting the weights of the contributions from the current layer. When used on PRISM, this approach generates a sharp heat map focused on an object instead of the area where the object is present.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>_upsampling</span>(self, extracted_features, pre_excitations):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> e <span style=color:#f92672>in</span> pre_excitations[::<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]:
</span></span><span style=display:flex><span>        extracted_features <span style=color:#f92672>=</span> interpolate(
</span></span><span style=display:flex><span>            extracted_features,
</span></span><span style=display:flex><span>            size<span style=color:#f92672>=</span>(e<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>2</span>], e<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>3</span>]),
</span></span><span style=display:flex><span>            mode<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;bilinear&#34;</span>,
</span></span><span style=display:flex><span>            align_corners<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>,
</span></span><span style=display:flex><span>        )
</span></span><span style=display:flex><span>        extracted_features <span style=color:#f92672>*=</span> e<span style=color:#f92672>.</span>mean(dim<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, keepdim<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> extracted_features
</span></span></code></pre></div><p><img src=/TorchPRISM/ge_prism.png alt="Gradual Extrapolation PRISM"></p><p>To use Gradual Extrapolation PRISM set parameter <code>grad_extrap=True</code> (default True).</p><h2 id=how-to-use>How to use
<a class=anchor href=#how-to-use>#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># load images into batch</span>
</span></span><span style=display:flex><span>input_batch <span style=color:#f92672>=</span> load_images()
</span></span><span style=display:flex><span>prism <span style=color:#f92672>=</span> TorchPRISM()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># choose your prefered model</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> models<span style=color:#f92672>.</span>vgg11(weights<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)<span style=color:#f92672>.</span>eval()
</span></span><span style=display:flex><span>prism<span style=color:#f92672>.</span>register_hooks(model)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model(input_batch)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>prism_maps_batch <span style=color:#f92672>=</span> prism<span style=color:#f92672>.</span>prism()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>drawable_input_batch <span style=color:#f92672>=</span> input_batch<span style=color:#f92672>.</span>permute(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>detach()<span style=color:#f92672>.</span>cpu()<span style=color:#f92672>.</span>numpy()
</span></span><span style=display:flex><span>drawable_prism_maps_batch <span style=color:#f92672>=</span> prism_maps_batch<span style=color:#f92672>.</span>permute(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>2</span>, <span style=color:#ae81ff>3</span>, <span style=color:#ae81ff>1</span>)<span style=color:#f92672>.</span>detach()<span style=color:#f92672>.</span>cpu()<span style=color:#f92672>.</span>numpy()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>draw_input_n_prism(drawable_input_batch, drawable_prism_maps_batch)
</span></span></code></pre></div><h2 id=code>Code
<a class=anchor href=#code>#</a></h2><p>You can find source code for this tutorial in this
<a href="https://colab.research.google.com/drive/1U-QzMQM2xGwrf4Xr2trx4ZcnHO0Ztvvy?usp=sharing">Colab Notebook</a>.</p><p>Here is the presentation for the tutorial in
<a href="https://docs.google.com/presentation/d/1m7RB_MWnMS45woR52BADDlEYawd4SDiAb7pcj_jD3Q0/edit?usp=sharing">Google Slides</a>.</p><h2 id=examples>Examples
<a class=anchor href=#examples>#</a></h2><h3 id=vgg11>VGG11
<a class=anchor href=#vgg11>#</a></h3><p><img src=/TorchPRISM/vgg11_example1.png alt="alt text">
<img src=/TorchPRISM/vgg11_example2.png alt="alt text"></p><h3 id=resnet101>ResNet101
<a class=anchor href=#resnet101>#</a></h3><p><img src=/TorchPRISM/resnet101_example1.png alt="alt text">
<img src=/TorchPRISM/resnet101_example2.png alt="alt text"></p><h3 id=googlenet>GoogleNet
<a class=anchor href=#googlenet>#</a></h3><p><img src=/TorchPRISM/googlenet_example1.png alt="alt text">
<img src=/TorchPRISM/googlenet_example2.png alt="alt text"></p></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/IU-PR/Capstone_project/tree/master//content/docs/Groups/TorchPRISM.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#torchprism>TorchPRISM</a><ul><li><a href=#introduction-unlocking-cnns-with-prism>Introduction: Unlocking CNNs with PRISM</a></li><li><a href=#understanding-cnns>Understanding CNNs</a></li><li><a href=#introducing-prism-a-glimpse-into-cnn-decision-making>Introducing PRISM: A Glimpse into CNN Decision-Making</a></li><li><a href=#implementation-of-prism>Implementation of PRISM</a></li><li><a href=#how-to-use>How to use</a></li><li><a href=#code>Code</a></li><li><a href=#examples>Examples</a><ul><li><a href=#vgg11>VGG11</a></li><li><a href=#resnet101>ResNet101</a></li><li><a href=#googlenet>GoogleNet</a></li></ul></li></ul></li></ul></nav></div></aside></main></body></html>