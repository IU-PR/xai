<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="


  Interpretable SHAP for Credit Risk Scoring
  #


  Table of Contents
  #



  1. Introduction

  2. Application Domain

  3. Methodology


  3.1 Dataset and Model Overview

  3.2 Explainability Method: SHAP



  4. Implementation: Applying SHAP to RandomForest


  4.1 Implementation Overview

  4.2 How to Use It



  5. Experiments and Analysis


  5.1 Evaluation Metrics

  5.2 Explanation Techniques and Visualizations

  5.3 Interpretations and Findings



  6. My Implementation

  7. Conclusion, Future Work, Limitations and Ethical Considerations

  8. References



  1. Introduction
  #

This project explores how SHAP (SHapley Additive exPlanations) can be used to enhance interpretability in credit scoring. The core objective is to implement SHAP from scratch and apply it to a Random Forest model trained on a credit dataset."><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/groups/shap_for_credit_risk/"><meta property="og:site_name" content="XAI"><meta property="og:title" content="Example"><meta property="og:description" content="Interpretable SHAP for Credit Risk Scoring # Table of Contents # 1. Introduction 2. Application Domain 3. Methodology 3.1 Dataset and Model Overview 3.2 Explainability Method: SHAP 4. Implementation: Applying SHAP to RandomForest 4.1 Implementation Overview 4.2 How to Use It 5. Experiments and Analysis 5.1 Evaluation Metrics 5.2 Explanation Techniques and Visualizations 5.3 Interpretations and Findings 6. My Implementation 7. Conclusion, Future Work, Limitations and Ethical Considerations 8. References 1. Introduction # This project explores how SHAP (SHapley Additive exPlanations) can be used to enhance interpretability in credit scoring. The core objective is to implement SHAP from scratch and apply it to a Random Forest model trained on a credit dataset."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>Example | XAI</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.e832d4e94212199857473bcf13a450d089c3fcd54ccadedcfac84ed0feff83fb.css integrity="sha256-6DLU6UISGZhXRzvPE6RQ0InD/NVMyt7c+shO0P7/g/s=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/mathtex-script-type.min.js integrity=sha384-jiBVvJ8NGGj5n7kJaiWwWp9AjC+Yh8rhZY3GtAX8yU28azcLgoRo4oukO87g7zDT crossorigin=anonymous></script><script defer src=/flexsearch.min.js></script><script defer src=/en.search.min.01b6b493e552379ed80b068779bd60a21ea3356bee5ba07de31fc95722807451.js integrity="sha256-Aba0k+VSN57YCwaHeb1goh6jNWvuW6B94x/JVyKAdFE=" crossorigin=anonymous></script><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/YELLOW_BAR.png alt=Logo><span><b>XAI</b></span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=/docs/groups/cam_and_secam/>CAM and SeCAM</a></li><li><a href=/docs/groups/diffusion-lens-interpreting-text-encoders-in-text-to-image-pipelines-tuned-using-dreambooth/>Diffusion Lens: Interpreting Text Encoders in Text-to-Image pipelines</a></li><li><a href=/docs/groups/dimensionality-reduction-in-nlp-visualizing-sentence-embeddings-with-umap-and-t-sne/>Dimensionality Reduction in NLP: Visualizing Sentence Embeddings with UMAP and t-SNE</a></li><li><a href=/docs/groups/example/>Example</a></li><li><a href=/docs/groups/shap_for_credit_risk/ class=active>Example</a></li><li><a href=/docs/groups/ai-playing-geoguessr-explained/>Ai Playing Geo Guessr Explained</a></li><li><a href=/docs/groups/contrastive-grad-cam-consistency/>Contrastive Grad Cam Consistency</a></li><li><a href=/docs/groups/dndfs_shap/>Dndfs Shap</a></li><li><a href=/docs/groups/gradcam/>Grad Cam</a></li><li><a href=/docs/groups/integrated-gradients/>Integrated Gradients</a></li><li><a href=/docs/groups/kernel-shap/>Kernel Shap</a></li><li><a href=/docs/groups/rag/>Rag</a></li><li><a href=/docs/groups/shap_darya_and_viktoria/>Shap Darya and Viktoria</a></li><li><a href=/docs/groups/sverl_tac_toe/>Sverl Tac Toe</a></li><li><a href=/docs/groups/torchprism/>Torch Prism</a></li><li><a href=/docs/groups/xai_for_transformers/>Xai for Transformers</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Example</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#interpretable-shap-for-credit-risk-scoring>Interpretable SHAP for Credit Risk Scoring</a><ul><li><a href=#table-of-contents>Table of Contents</a></li><li><a href=#1-introduction>1. Introduction</a></li><li><a href=#2-application-domain>2. Application Domain</a></li><li><a href=#3-methodology>3. Methodology</a><ul><li><a href=#31-dataset-and-model-overview>3.1 Dataset and Model Overview</a></li><li><a href=#32-explainability-method-shap>3.2 Explainability Method: SHAP</a></li><li><a href=#33-kernel-shap-approximation>3.3 Kernel SHAP Approximation</a></li></ul></li><li><a href=#4-implementation-applying-shap-to-randomforest>4. Implementation: Applying SHAP to RandomForest</a><ul><li><a href=#41-implementation-overview>4.1 Implementation Overview</a></li><li><a href=#42-how-to-use-it>4.2 How to Use It</a></li><li><a href=#43-shap-implementation-steps-with-code-snippets>4.3 SHAP Implementation Steps with Code Snippets</a></li></ul></li><li><a href=#5-results-and-visualizations>5. Results and Visualizations</a><ul><li><a href=#51-explanation-techniques-and-visualizations>5.1 Explanation Techniques and Visualizations</a></li><li><a href=#52-interpretations-and-findings>5.2 Interpretations and Findings</a></li><li><a href=#visual-explanations-in-detatils>Visual Explanations in detatils</a></li><li><a href=#1-shap-value-bar-plot-interpretation-one-prediction>1. SHAP Value Bar Plot Interpretation (One Prediction)</a></li><li><a href=#2-shap-force-plot-interpretation>2. SHAP Force Plot Interpretation</a></li><li><a href=#3-interpretation-of-shap-bar-plot--top-10-features>3. Interpretation of SHAP Bar Plot – Top 10 Features</a></li><li><a href=#4-shap-waterfall-chart-interpretation>4. SHAP Waterfall Chart Interpretation</a></li><li><a href=#5-shap-summary-plot-interpretation>5. SHAP Summary Plot Interpretation</a></li><li><a href=#6-shap-interaction-plot-age-vs-shapage-colored-by-duration>6. SHAP Interaction Plot: <code>age</code> vs SHAP(<code>age</code>), colored by <code>duration</code></a></li><li><a href=#7-shap-feature-interaction-credit_history--other_payment_plans>7. SHAP Feature Interaction: <code>credit_history</code> × <code>other_payment_plans</code></a></li><li><a href=#8-shap-interaction-value-matrix--interpretation>8. SHAP Interaction Value Matrix — Interpretation</a></li><li><a href=#9-shap-interaction-comparison-risky-client-vs-safe-client>9. SHAP Interaction Comparison: Risky Client vs Safe Client</a></li><li><a href=#10-shap-waterfall-comparison-risky-vs-safe-client>10. SHAP Waterfall Comparison: Risky vs Safe Client</a></li><li><a href=#11-12-shap-value-comparison-custom-vs-shapkernelexplainer>11-12. SHAP Value Comparison: Custom vs shap.KernelExplainer</a></li></ul></li><li><a href=#6-my-implementation>6. My Implementation</a></li><li><a href=#7-conclusion>7. Conclusion</a><ul><li><a href=#future-work>Future Work</a></li><li><a href=#limitations>Limitations</a></li><li><a href=#ethical-considerations>Ethical Considerations</a></li></ul></li><li><a href=#8-references>8. References</a></li></ul></li></ul></nav></aside></header><article class=markdown><style>.markdown a{text-decoration:underline!important}</style><style>.markdown h2{font-weight:700}</style><h1 id=interpretable-shap-for-credit-risk-scoring>Interpretable SHAP for Credit Risk Scoring
<a class=anchor href=#interpretable-shap-for-credit-risk-scoring>#</a></h1><h2 id=table-of-contents>Table of Contents
<a class=anchor href=#table-of-contents>#</a></h2><ul><li><a href=/#1-introduction>1. Introduction</a></li><li><a href=/#2-application-domain>2. Application Domain</a></li><li><a href=/#3-methodology>3. Methodology</a><ul><li><a href=/#31-dataset-and-model-overview>3.1 Dataset and Model Overview</a></li><li><a href=/#32-explainability-method-shap>3.2 Explainability Method: SHAP</a></li></ul></li><li><a href=/#4-implementation-applying-shap-to-randomforest>4. Implementation: Applying SHAP to RandomForest</a><ul><li><a href=/#41-implementation-overview>4.1 Implementation Overview</a></li><li><a href=/#42-how-to-use-it>4.2 How to Use It</a></li></ul></li><li><a href=/#5-experiments-and-analysis>5. Experiments and Analysis</a><ul><li><a href=/#51-evaluation-metrics>5.1 Evaluation Metrics</a></li><li><a href=/#52-explanation-techniques-and-visualizations>5.2 Explanation Techniques and Visualizations</a></li><li><a href=/#53-interpretations-and-findings>5.3 Interpretations and Findings</a></li></ul></li><li><a href=/#6-my-implementation>6. My Implementation</a></li><li><a href=/#7-conclusion-future-work-limitations-and-ethical-considerations>7. Conclusion, Future Work, Limitations and Ethical Considerations</a></li><li><a href=/#8-references>8. References</a></li></ul><hr><h2 id=1-introduction>1. Introduction
<a class=anchor href=#1-introduction>#</a></h2><p>This project explores how SHAP (SHapley Additive exPlanations) can be used to enhance interpretability in credit scoring. The core objective is to implement SHAP from scratch and apply it to a Random Forest model trained on a credit dataset.</p><p><strong>Research Question:</strong> <em>Can we accurately replicate SHAP value explanations without relying on libraries, and how useful are these explanations for understanding credit risk decisions made by machine learning models?</em></p><p>SHAP was chosen due to its strong theoretical grounding in cooperative game theory and its ability to produce consistent, locally accurate feature attributions. This makes it particularly suitable for credit scoring applications where decisions have significant human impact.</p><p>By replicating SHAP manually, we aim to:</p><ul><li>Gain deeper insight into the mechanics of explainable AI</li><li>Validate model decisions through transparent breakdowns</li><li>Provide stakeholders and regulators with interpretable model behavior</li></ul><p>This research bridges the gap between theoretical fairness requirements and practical implementation in real-world credit assessment systems.</p><hr><h2 id=2-application-domain>2. Application Domain
<a class=anchor href=#2-application-domain>#</a></h2><p>Credit scoring is a critical application within financial services, where institutions assess an applicant&rsquo;s ability to repay a loan. An accurate and interpretable model helps mitigate financial risk and ensures transparency in lending decisions.</p><p>By applying SHAP to a Random Forest model on the German Credit dataset, this project addresses the need for explainability in high-stakes domains. Regulatory frameworks (like the EU GDPR or U.S. Equal Credit Opportunity Act) increasingly require that decisions be interpretable and non-discriminatory. Hence, this research not only contributes technically but also aligns with real-world financial compliance needs.</p><p>In this application domain, local explanations can help loan officers understand individual decisions, while global explanations support model auditability and fairness assessments.</p><hr><h2 id=3-methodology>3. Methodology
<a class=anchor href=#3-methodology>#</a></h2><h3 id=31-dataset-and-model-overview>3.1 Dataset and Model Overview
<a class=anchor href=#31-dataset-and-model-overview>#</a></h3><p>We used the
<a href=https://archive.ics.uci.edu/ml/datasets/statlog+%28german+credit+data%29><strong>German Credit Dataset</strong></a>, a structured dataset frequently applied in credit risk modeling research. It contains 1000 samples with 20 features and a binary target variable indicating creditworthiness (good or bad credit risk). The features include a mix of categorical and numerical variables such as:</p><ul><li><code>checking_status</code>: status of existing checking account</li><li><code>duration</code>: duration in months</li><li><code>credit_history</code>: credit history records</li><li><code>purpose</code>: purpose of the loan (e.g., car, education)</li><li><code>credit_amount</code>: amount of credit requested</li><li><code>savings_status</code>: status of savings account/bonds</li><li><code>employment</code>: years of current employment</li><li><code>installment_commitment</code>: installment rate as a percentage of income</li><li><code>personal_status</code>: personal and sex status</li><li><code>other_parties</code>: other debtors/guarantors</li><li><code>residence_since</code>: years living at current residence</li><li><code>property_magnitude</code>: value of assets</li><li><code>age</code>: applicant&rsquo;s age</li><li><code>other_payment_plans</code>: presence of other installment plans</li><li><code>housing</code>: housing situation</li><li><code>existing_credits</code>: number of existing credits</li><li><code>job</code>: type of job</li><li><code>num_dependents</code>: number of dependents</li><li><code>own_telephone</code>: ownership of a telephone</li><li><code>foreign_worker</code>: foreign worker status</li></ul><p>These features offer a comprehensive view of an individual&rsquo;s financial and personal profile relevant to credit decision-making.</p><p>We selected the <strong>Random Forest classifier</strong> due to its ensemble-based structure that combines multiple decision trees to improve predictive performance and reduce overfitting. Random Forest is particularly suitable for tabular data with mixed data types (categorical and numerical), and it performs implicit feature selection by evaluating feature importance across many trees. Each tree in the forest is trained on a bootstrap sample of the data and considers a random subset of features at each split, which enhances diversity and robustness. Furthermore, Random Forest supports probability estimation and has interpretable decision paths, making it a practical and explainable choice for credit risk modeling.</p><p><strong>Why Random Forest?</strong></p><ul><li><strong>Robust performance on tabular data</strong> — It effectively handles both numerical and categorical variables.</li><li><strong>Resistance to overfitting</strong> — Uses bootstrap aggregation and feature randomness.</li><li><strong>Feature importance</strong> — Naturally provides importance rankings useful for interpretability.</li><li><strong>Interpretability</strong> — Easier to interpret than black-box models like neural networks.</li><li><strong>Practicality</strong> — Widely supported, scalable, and requires minimal hyperparameter tuning.</li></ul><h3 id=32-explainability-method-shap>3.2 Explainability Method: SHAP
<a class=anchor href=#32-explainability-method-shap>#</a></h3><p>SHAP (SHapley Additive exPlanations) is an explainability framework based on Shapley values from cooperative game theory. It provides consistent and locally accurate attributions of a model’s output to each input feature.</p><p>In SHAP, the prediction task is treated as a game in which the &ldquo;players&rdquo; are the input features. The goal is to fairly allocate the model output (gain) among the features based on their contribution to the prediction.</p><h4 id=how-shap-works>How SHAP Works
<a class=anchor href=#how-shap-works>#</a></h4><ul><li>It considers all possible feature combinations (subsets) and computes the marginal contribution of a feature across these combinations.</li><li>Each SHAP value represents how much a feature contributes to the difference between the actual prediction and the mean prediction (baseline).</li><li>The final explanation is a sum of these SHAP values and the base value.</li></ul><p>Mathematically, for a model (f), the SHAP value for feature (i) is:</p><p>$$
\phi_i = \sum_{S \subseteq F \setminus {i}} \frac{|S|!(|F| - |S| - 1)!}{|F|!} \left[ f(S \cup {i}) - f(S) \right]
$$</p><p>Where:</p><ul><li>(F): the set of all features</li><li>(S): a subset of features excluding (i)</li></ul><p>This formulation ensures fairness (symmetry), efficiency (conservation of output), and consistency across feature attributions.</p><p>SHAP is model-agnostic and provides a unified measure of feature importance for both global and local interpretability.</p><h3 id=33-kernel-shap-approximation>3.3 Kernel SHAP Approximation
<a class=anchor href=#33-kernel-shap-approximation>#</a></h3><p>Kernel SHAP is a model-agnostic approximation algorithm that estimates SHAP values using a weighted linear regression. It is especially useful when the exact computation of Shapley values is computationally infeasible, such as in models with many input features.</p><p>Key aspects of Kernel SHAP:</p><ul><li>It treats the original model as a black box.</li><li>It samples feature subsets and queries the model to observe how predictions change.</li><li>It solves a weighted least squares problem to estimate each feature&rsquo;s SHAP value.</li><li>Weights are chosen such that smaller subsets (i.e., those missing more features) have higher influence.</li></ul><p>Kernel SHAP combines the strengths of LIME (local surrogate models) and Shapley theory, and provides theoretically grounded explanations even when the model internals are not accessible.</p><p>Since computing all ( 2^n ) feature coalitions is infeasible for large ( n ), Kernel SHAP uses a model-agnostic, sampling-based approach that approximates SHAP values via weighted linear regression.</p><p><strong>Steps:</strong></p><ol><li>Generate random binary masks ( z \in {0, 1}^n )</li><li>For each mask, compute a masked input instance:
[ x_z = z \odot x + (1 - z) \odot \bar{x}_{bg} ]</li><li>Evaluate the model output ( f(x_z) )</li><li>Solve:
[ \hat{f}(z) = \phi_0 + \sum_{i=1}^n \phi_i z_i ]
using weighted ridge regression, where the weights are given by the SHAP kernel:
[ w(z) = \frac{(n - 1)}{{n \choose |z|} \cdot |z|(n - |z|)} ]</li></ol><p>This formulation ensures that explanations are consistent and robust, even when internal model logic is inaccessible.</p><hr><h2 id=4-implementation-applying-shap-to-randomforest>4. Implementation: Applying SHAP to RandomForest
<a class=anchor href=#4-implementation-applying-shap-to-randomforest>#</a></h2><h3 id=41-implementation-overview>4.1 Implementation Overview
<a class=anchor href=#41-implementation-overview>#</a></h3><p>Our custom SHAP implementation includes:</p><ul><li><strong>Expected value</strong> computation: model prediction averaged over the training data.</li><li><strong>Marginal contribution</strong> estimation: for each feature, we average the effect across multiple permutations.</li><li><strong>SHAP value aggregation</strong>: computes a final breakdown per instance.</li></ul><p>This was done without using the <code>shap</code> package.</p><p>Implementing SHAP from scratch for a Random Forest model presented several practical challenges. Kernel SHAP assumes feature independence and requires repeated sampling and model evaluations, making it computationally expensive. Additionally, since Random Forests are not inherently differentiable or linear, accurate approximation depends on well-designed masking strategies and sufficient sampling.</p><p>To address these, we:</p><ul><li>Used a <strong>mean-masked baseline</strong> over multiple background samples to better approximate realistic counterfactuals.</li><li>Tuned the <strong>SHAP kernel weights</strong> and <strong>regularization strength</strong> in ridge regression to improve numerical stability.</li><li>Increased the number of sampled coalitions (up to 2000) to improve approximation accuracy.</li></ul><p>These design choices helped us replicate SHAP explanations with high fidelity, while keeping the implementation self-contained and interpretable.</p><h3 id=42-how-to-use-it>4.2 How to Use It
<a class=anchor href=#42-how-to-use-it>#</a></h3><p>The notebook:</p><ol><li>Trains a Random Forest model.</li><li>Applies SHAP using our method.</li><li>Visualizes outputs using:<ul><li>Custom force plots (HTML with SVG)</li><li>Waterfall charts</li><li>Top-K bar plots</li><li>SHAP summary scatter plots</li></ul></li></ol><hr><h3 id=43-shap-implementation-steps-with-code-snippets>4.3 SHAP Implementation Steps with Code Snippets
<a class=anchor href=#43-shap-implementation-steps-with-code-snippets>#</a></h3><ol><li><strong>Sample feature masks:</strong></li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>masks <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>2</span>, size<span style=color:#f92672>=</span>(nsamples, n_features))
</span></span></code></pre></div><ol start=2><li><strong>Mask inputs and generate predictions:</strong></li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> mask <span style=color:#f92672>in</span> masks:
</span></span><span style=display:flex><span>    x_masked <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>where(mask <span style=color:#f92672>==</span> <span style=color:#ae81ff>1</span>, x, X_bg<span style=color:#f92672>.</span>mean(axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>))
</span></span><span style=display:flex><span>    pred <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict_proba([x_masked])[<span style=color:#ae81ff>0</span>][<span style=color:#ae81ff>1</span>]
</span></span></code></pre></div><ol start=3><li><strong>Compute kernel weights:</strong></li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>shap_kernel_weight</span>(mask):
</span></span><span style=display:flex><span>    z <span style=color:#f92672>=</span> mask<span style=color:#f92672>.</span>sum()
</span></span><span style=display:flex><span>    n <span style=color:#f92672>=</span> len(mask)
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> (n <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>) <span style=color:#f92672>/</span> (comb(n, z) <span style=color:#f92672>*</span> z <span style=color:#f92672>*</span> (n <span style=color:#f92672>-</span> z)) <span style=color:#66d9ef>if</span> <span style=color:#ae81ff>0</span> <span style=color:#f92672>&lt;</span> z <span style=color:#f92672>&lt;</span> n <span style=color:#66d9ef>else</span> <span style=color:#ae81ff>1e-6</span>
</span></span></code></pre></div><ol start=4><li><strong>Fit weighted ridge regression:</strong></li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>reg <span style=color:#f92672>=</span> Ridge(alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-3</span>)
</span></span><span style=display:flex><span>reg<span style=color:#f92672>.</span>fit(X_masked, preds, sample_weight<span style=color:#f92672>=</span>weights)
</span></span><span style=display:flex><span>shap_values <span style=color:#f92672>=</span> reg<span style=color:#f92672>.</span>coef_
</span></span></code></pre></div><ol start=5><li><strong>Return SHAP values and base value:</strong></li></ol><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>base_value <span style=color:#f92672>=</span> reg<span style=color:#f92672>.</span>intercept_
</span></span><span style=display:flex><span><span style=color:#66d9ef>return</span> shap_values, base_value
</span></span></code></pre></div><hr><h2 id=5-results-and-visualizations>5. Results and Visualizations
<a class=anchor href=#5-results-and-visualizations>#</a></h2><h3 id=51-explanation-techniques-and-visualizations>5.1 Explanation Techniques and Visualizations
<a class=anchor href=#51-explanation-techniques-and-visualizations>#</a></h3><p>To understand both individual predictions and overall model behavior, we employed a combination of <strong>local</strong> and <strong>global</strong> explanation methods using SHAP.</p><ul><li><strong>Local explanations</strong> (force plots, bar plots, waterfall charts) clarify how specific input features influenced the outcome for one applicant.</li><li><strong>Global explanations</strong> (summary plots, interaction matrices) reveal patterns across the entire dataset, highlighting which features are most impactful and how they interact.</li></ul><p>These complementary techniques allow for both micro-level decision inspection and macro-level model transparency, making them essential in domains such as credit scoring where interpretability is a regulatory and ethical necessity.</p><p>We generated 11 distinct SHAP visualizations including:</p><ul><li><strong>Local explanation</strong>: force plots, bar plots, and waterfall charts for individual predictions</li><li><strong>Global explanation</strong>: summary plots, interaction matrices, and comparison between risky and safe applicants</li></ul><h3 id=52-interpretations-and-findings>5.2 Interpretations and Findings
<a class=anchor href=#52-interpretations-and-findings>#</a></h3><p><strong>Summary of key insights from visualizations:</strong></p><ul><li><code>checking_status</code> consistently appears as the most influential feature in both local and global plots.</li><li><code>age</code>, <code>duration</code>, and <code>credit_history</code> are strong negative contributors for younger applicants or long loans.</li><li>Positive factors such as <code>purpose</code>, <code>property_magnitude</code>, and <code>employment</code> help increase predicted creditworthiness.</li><li>Interaction visualizations reveal that features like <code>checking_status</code> and <code>duration</code> compound their effects in risky applicants.</li><li>Our custom SHAP implementation approximates SHAP values with high fidelity (mean absolute difference ~0.017 vs KernelExplainer).</li></ul><h3 id=visual-explanations-in-detatils>Visual Explanations in detatils
<a class=anchor href=#visual-explanations-in-detatils>#</a></h3><h3 id=1-shap-value-bar-plot-interpretation-one-prediction>1. SHAP Value Bar Plot Interpretation (One Prediction)
<a class=anchor href=#1-shap-value-bar-plot-interpretation-one-prediction>#</a></h3><p><img src=/SHAP_for_credit_risk/1_Bar_plot_for_1_prediction.png alt="Fig. 1 SHAP Value Bar Plot for 1 Prediction"></p><ul><li>The most influential negative contributors are <code>checking_status</code> and <code>age</code>, indicating perceived financial and demographic risks.</li><li>Positive contributions from <code>purpose</code>, <code>installment_commitment</code>, and <code>residence_since</code> help balance the prediction.</li></ul><h3 id=2-shap-force-plot-interpretation>2. SHAP Force Plot Interpretation
<a class=anchor href=#2-shap-force-plot-interpretation>#</a></h3><p><img src=/SHAP_for_credit_risk/force_plot_custom.html alt="Link to the SHAP Force Plot"></p><ul><li>The force plot shows how the model moves from the average output to the final prediction based on individual features.</li><li>Key drivers include <code>credit_history</code>, <code>savings_status</code>, and <code>checking_status</code>.</li></ul><h3 id=3-interpretation-of-shap-bar-plot--top-10-features>3. Interpretation of SHAP Bar Plot – Top 10 Features
<a class=anchor href=#3-interpretation-of-shap-bar-plot--top-10-features>#</a></h3><p><img src=/SHAP_for_credit_risk/3_Top_10_Important_Features_for_Prediction.png alt="Fig. 3 SHAP Bar Plot – Top 10 Features"></p><ul><li><code>checking_status</code>, <code>age</code>, and <code>personal_status</code> are the top negative contributors.</li><li>Features like <code>purpose</code>, <code>installment_commitment</code>, and <code>property_magnitude</code> help raise the prediction.</li></ul><h3 id=4-shap-waterfall-chart-interpretation>4. SHAP Waterfall Chart Interpretation
<a class=anchor href=#4-shap-waterfall-chart-interpretation>#</a></h3><p><img src=/SHAP_for_credit_risk/4_SHAP_Waterfall_Chart.png alt="Fig. 4 SHAP Waterfall Chart"></p><ul><li>The prediction decreases from the base value due to strong negative SHAP values for <code>checking_status</code> and <code>age</code>.</li><li>Positive contributions include <code>purpose</code> and <code>property_magnitude</code>, but do not fully offset the negatives.</li></ul><h3 id=5-shap-summary-plot-interpretation>5. SHAP Summary Plot Interpretation
<a class=anchor href=#5-shap-summary-plot-interpretation>#</a></h3><p><img src=/SHAP_for_credit_risk/5_Summary_plot.png alt="Fig. 5 SHAP Summary Plot"></p><ul><li>Globally, <code>checking_status</code>, <code>duration</code>, and <code>age</code> are the most influential features.</li><li>Low values for <code>age</code> and high values for <code>duration</code> push predictions lower.</li></ul><h3 id=6-shap-interaction-plot-age-vs-shapage-colored-by-duration>6. SHAP Interaction Plot: <code>age</code> vs SHAP(<code>age</code>), colored by <code>duration</code>
<a class=anchor href=#6-shap-interaction-plot-age-vs-shapage-colored-by-duration>#</a></h3><p><img src=/SHAP_for_credit_risk/6_Interaction_age_by_duration.png alt="Fig. 6 SHAP Interaction Plot"></p><ul><li>Younger age consistently decreases predictions.</li><li>Longer loan durations amplify the negative impact of age.</li></ul><h3 id=7-shap-feature-interaction-credit_history--other_payment_plans>7. SHAP Feature Interaction: <code>credit_history</code> × <code>other_payment_plans</code>
<a class=anchor href=#7-shap-feature-interaction-credit_history--other_payment_plans>#</a></h3><p><img src=/SHAP_for_credit_risk/7_Interaction_credit_history_by_other_payment_plans.png alt="Fig. 7 SHAP Feature Interaction: credit_history × other_payment_plans"></p><ul><li>Applicants with no credit history and no other plans are penalized most.</li><li>Good credit history matters more when not undermined by additional obligations.</li></ul><h3 id=8-shap-interaction-value-matrix--interpretation>8. SHAP Interaction Value Matrix — Interpretation
<a class=anchor href=#8-shap-interaction-value-matrix--interpretation>#</a></h3><p><img src=/SHAP_for_credit_risk/8_Full_SHAP_Interation_Value_matrix.png alt="Fig. 8 SHAP Interaction Value Matrix"></p><ul><li><code>checking_status</code> has strong independent influence.</li><li>Features like <code>purpose</code> and <code>credit_amount</code> show notable interactions.</li></ul><h3 id=9-shap-interaction-comparison-risky-client-vs-safe-client>9. SHAP Interaction Comparison: Risky Client vs Safe Client
<a class=anchor href=#9-shap-interaction-comparison-risky-client-vs-safe-client>#</a></h3><p><img src=/SHAP_for_credit_risk/9_SHAP_interaction_comparison_risk_vs_safe_client.png alt="Fig. 9 SHAP Interaction Comparison: Risky Client vs Safe Client"></p><ul><li>Risky clients are more impacted by interactions combining weak financial signals.</li><li>Safe clients benefit from combinations of stable signals.</li></ul><h3 id=10-shap-waterfall-comparison-risky-vs-safe-client>10. SHAP Waterfall Comparison: Risky vs Safe Client
<a class=anchor href=#10-shap-waterfall-comparison-risky-vs-safe-client>#</a></h3><p><img src=/SHAP_for_credit_risk/10_Waterfall_Comparison_of_SHAP_Contributions_Risky_vs_Safe_client.png alt="Fig. 10 SHAP Waterfall Comparison: Risky vs Safe Client"></p><ul><li>Risky client is penalized by multiple negative features with no major positives.</li><li>Safe client gains modest positive contributions and avoids major penalties.</li></ul><h3 id=11-12-shap-value-comparison-custom-vs-shapkernelexplainer>11-12. SHAP Value Comparison: Custom vs shap.KernelExplainer
<a class=anchor href=#11-12-shap-value-comparison-custom-vs-shapkernelexplainer>#</a></h3><p><img src=/SHAP_for_credit_risk/11_SHAP_Value_Difference_Ours_vs_shap.KernelExplainer.png alt="Fig. 11 SHAP Value Difference: Custom vs shap.KernelExplainer">
<img src=/SHAP_for_credit_risk/12_SHAP_Value_Comparison.png alt="Fig. 12 SHAP Value Comparison: Custom vs shap.KernelExplainer"></p><ul><li>Our custom SHAP implementation shows strong agreement with KernelExplainer.</li><li>Mean absolute SHAP difference is low, confirming the accuracy of our method.</li></ul><hr><h2 id=6-my-implementation>6. My Implementation
<a class=anchor href=#6-my-implementation>#</a></h2><p>All code, visualizations, and notebooks are available in the GitHub repository (to be added):</p><blockquote><p>📎 <strong>Link to GitHub repo:</strong> <strong>coming soon</strong><br>Contains code, SHAP implementation logic, and interactive visualizations.</p></blockquote><hr><h2 id=7-conclusion>7. Conclusion
<a class=anchor href=#7-conclusion>#</a></h2><p>This project demonstrates how manual implementation of SHAP can:</p><ul><li>Make ML model decisions more transparent</li><li>Offer causal-like reasoning for features</li><li>Improve trust and accountability in credit scoring</li></ul><h3 id=future-work>Future Work
<a class=anchor href=#future-work>#</a></h3><ul><li>Test with other models (e.g., XGBoost, Logistic Regression)</li><li>Add comparative analysis with the SHAP library</li><li>Create an interactive dashboard for model inspection</li><li>Measure and mitigate fairness issues</li></ul><h3 id=limitations>Limitations
<a class=anchor href=#limitations>#</a></h3><ul><li>Manual SHAP is slower and more fragile than library versions.</li><li>Assumes feature independence — not always realistic.</li><li>Findings may not generalize due to dataset size and sampling.</li></ul><h3 id=ethical-considerations>Ethical Considerations
<a class=anchor href=#ethical-considerations>#</a></h3><ul><li>Credit scoring affects real lives — transparency and fairness are essential.</li><li>Explainable models can empower both users and regulators.</li></ul><hr><h2 id=8-references>8. References
<a class=anchor href=#8-references>#</a></h2><ul><li>Lundberg, S. M., & Lee, S. I. (2017). A Unified Approach to Interpreting Model Predictions. <em>NeurIPS</em>.
<a href=https://arxiv.org/abs/1705.07874>https://arxiv.org/abs/1705.07874</a></li><li>Lundberg, S. M., Erion, G. G., & Lee, S. I. (2020). From Local Explanations to Global Understanding with Explainable AI for Trees. <em>Nature Machine Intelligence</em>.
<a href=https://www.nature.com/articles/s42256-019-0138-9>https://www.nature.com/articles/s42256-019-0138-9</a></li><li>Wachter, S., Mittelstadt, B., & Russell, C. (2018). Counterfactual Explanations without Opening the Black Box. <em>Harvard Journal of Law & Technology</em>, 31(2)</li><li>UCI Machine Learning Repository. German Credit Dataset.
<a href=https://archive.ics.uci.edu/ml/datasets/statlog+%28german+credit+data%29>https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)</a></li><li>SHAP GitHub Repository.
<a href=https://github.com/shap/shap>https://github.com/shap/shap</a></li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/IU-PR/Capstone_project/tree/master//content/docs/Groups/SHAP_for_credit_risk.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#interpretable-shap-for-credit-risk-scoring>Interpretable SHAP for Credit Risk Scoring</a><ul><li><a href=#table-of-contents>Table of Contents</a></li><li><a href=#1-introduction>1. Introduction</a></li><li><a href=#2-application-domain>2. Application Domain</a></li><li><a href=#3-methodology>3. Methodology</a><ul><li><a href=#31-dataset-and-model-overview>3.1 Dataset and Model Overview</a></li><li><a href=#32-explainability-method-shap>3.2 Explainability Method: SHAP</a></li><li><a href=#33-kernel-shap-approximation>3.3 Kernel SHAP Approximation</a></li></ul></li><li><a href=#4-implementation-applying-shap-to-randomforest>4. Implementation: Applying SHAP to RandomForest</a><ul><li><a href=#41-implementation-overview>4.1 Implementation Overview</a></li><li><a href=#42-how-to-use-it>4.2 How to Use It</a></li><li><a href=#43-shap-implementation-steps-with-code-snippets>4.3 SHAP Implementation Steps with Code Snippets</a></li></ul></li><li><a href=#5-results-and-visualizations>5. Results and Visualizations</a><ul><li><a href=#51-explanation-techniques-and-visualizations>5.1 Explanation Techniques and Visualizations</a></li><li><a href=#52-interpretations-and-findings>5.2 Interpretations and Findings</a></li><li><a href=#visual-explanations-in-detatils>Visual Explanations in detatils</a></li><li><a href=#1-shap-value-bar-plot-interpretation-one-prediction>1. SHAP Value Bar Plot Interpretation (One Prediction)</a></li><li><a href=#2-shap-force-plot-interpretation>2. SHAP Force Plot Interpretation</a></li><li><a href=#3-interpretation-of-shap-bar-plot--top-10-features>3. Interpretation of SHAP Bar Plot – Top 10 Features</a></li><li><a href=#4-shap-waterfall-chart-interpretation>4. SHAP Waterfall Chart Interpretation</a></li><li><a href=#5-shap-summary-plot-interpretation>5. SHAP Summary Plot Interpretation</a></li><li><a href=#6-shap-interaction-plot-age-vs-shapage-colored-by-duration>6. SHAP Interaction Plot: <code>age</code> vs SHAP(<code>age</code>), colored by <code>duration</code></a></li><li><a href=#7-shap-feature-interaction-credit_history--other_payment_plans>7. SHAP Feature Interaction: <code>credit_history</code> × <code>other_payment_plans</code></a></li><li><a href=#8-shap-interaction-value-matrix--interpretation>8. SHAP Interaction Value Matrix — Interpretation</a></li><li><a href=#9-shap-interaction-comparison-risky-client-vs-safe-client>9. SHAP Interaction Comparison: Risky Client vs Safe Client</a></li><li><a href=#10-shap-waterfall-comparison-risky-vs-safe-client>10. SHAP Waterfall Comparison: Risky vs Safe Client</a></li><li><a href=#11-12-shap-value-comparison-custom-vs-shapkernelexplainer>11-12. SHAP Value Comparison: Custom vs shap.KernelExplainer</a></li></ul></li><li><a href=#6-my-implementation>6. My Implementation</a></li><li><a href=#7-conclusion>7. Conclusion</a><ul><li><a href=#future-work>Future Work</a></li><li><a href=#limitations>Limitations</a></li><li><a href=#ethical-considerations>Ethical Considerations</a></li></ul></li><li><a href=#8-references>8. References</a></li></ul></li></ul></nav></div></aside></main></body></html>