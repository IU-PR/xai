<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Abstract # We introduce Integrated Hessians, the first scalable second-order attribution method for Transformer models, applying it to a RoBERTa-based sentiment classifier on the IMDb dataset. Our approach quantifies pairwise token interactions via a path-integrated Hessian, extending the axiomatic foundation of Integrated Gradients. We provide a reproducible PyTorch implementation, Dockerized environment, and visualizations (heatmaps of token–token interactions) that reveal linguistic phenomena (e.g., negation and contrast) inaccessible to first-order methods. All code is publicly available for community use and extension."><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:title" content><meta property="og:description" content="Abstract # We introduce Integrated Hessians, the first scalable second-order attribution method for Transformer models, applying it to a RoBERTa-based sentiment classifier on the IMDb dataset. Our approach quantifies pairwise token interactions via a path-integrated Hessian, extending the axiomatic foundation of Integrated Gradients. We provide a reproducible PyTorch implementation, Dockerized environment, and visualizations (heatmaps of token–token interactions) that reveal linguistic phenomena (e.g., negation and contrast) inaccessible to first-order methods. All code is publicly available for community use and extension."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/docs/groups/integrated_hessians/"><meta property="article:section" content="docs"><title>Integrated Hessians | XAI</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.e832d4e94212199857473bcf13a450d089c3fcd54ccadedcfac84ed0feff83fb.css integrity="sha256-6DLU6UISGZhXRzvPE6RQ0InD/NVMyt7c+shO0P7/g/s=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/mathtex-script-type.min.js integrity=sha384-jiBVvJ8NGGj5n7kJaiWwWp9AjC+Yh8rhZY3GtAX8yU28azcLgoRo4oukO87g7zDT crossorigin=anonymous></script><script defer src=/flexsearch.min.js></script><script defer src=/en.search.min.a040731689cd19718a91f9c6750d976d00425e6cc042ec907df8749f70d883ee.js integrity="sha256-oEBzFonNGXGKkfnGdQ2XbQBCXmzAQuyQffh0n3DYg+4=" crossorigin=anonymous></script><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/YELLOW_BAR.png alt=Logo><span><b>XAI</b></span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=/docs/groups/cam_and_secam/>CAM and SeCAM</a></li><li><a href=/docs/groups/diffusion-lens-interpreting-text-encoders-in-text-to-image-pipelines-tuned-using-dreambooth/>Diffusion Lens: Interpreting Text Encoders in Text-to-Image pipelines</a></li><li><a href=/docs/groups/dimensionality-reduction-in-nlp-visualizing-sentence-embeddings-with-umap-and-t-sne/>Dimensionality Reduction in NLP: Visualizing Sentence Embeddings with UMAP and t-SNE</a></li><li><a href=/docs/groups/example/>Example</a></li><li><a href=/docs/groups/ai-playing-geoguessr-explained/>Ai Playing Geo Guessr Explained</a></li><li><a href=/docs/groups/contrastive-grad-cam-consistency/>Contrastive Grad Cam Consistency</a></li><li><a href=/docs/groups/dndfs_shap/>Dndfs Shap</a></li><li><a href=/docs/groups/gradcam/>Grad Cam</a></li><li><a href=/docs/groups/integrated-gradients/>Integrated Gradients</a></li><li><a href=/docs/groups/integrated_hessians/ class=active>Integrated Hessians</a></li><li><a href=/docs/groups/kernel-shap/>Kernel Shap</a></li><li><a href=/docs/groups/rag/>Rag</a></li><li><a href=/docs/groups/shap_darya_and_viktoria/>Shap Darya and Viktoria</a></li><li><a href=/docs/groups/sverl_tac_toe/>Sverl Tac Toe</a></li><li><a href=/docs/groups/torchprism/>Torch Prism</a></li><li><a href=/docs/groups/xai_for_transformers/>Xai for Transformers</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Integrated Hessians</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><ul><li><a href=#abstract>Abstract</a></li><li><a href=#1-introduction>1. Introduction</a></li><li><a href=#2-background--related-work>2. Background & Related Work</a><ul><li><a href=#21-integrated-gradients>2.1 Integrated Gradients</a></li><li><a href=#22-shap>2.2 SHAP</a></li><li><a href=#23-lime>2.3 LIME</a></li></ul></li><li><a href=#3-application-domain>3. Application Domain</a></li><li><a href=#4-integrated-hessians-methodology>4. Integrated Hessians: Methodology</a><ul><li><a href=#41-mathematical-formulation>4.1 Mathematical Formulation</a></li><li><a href=#integrated-gradients-first-order><strong>Integrated Gradients (First-order)</strong></a></li><li><a href=#integrated-hessians-second-order><strong>Integrated Hessians (Second-order)</strong></a></li><li><a href=#42-theoretical-advantages>4.2 Theoretical Advantages</a></li></ul></li><li><a href=#5-implementation>5. Implementation</a><ul><li><a href=#51-calculating-integrated-gradients>5.1 Calculating Integrated Gradients</a></li><li><a href=#52-calculating-integrated-hessians>5.2 Calculating Integrated Hessians</a></li></ul></li><li><a href=#6-experiments--results>6. Experiments & Results</a><ul><li><a href=#61-qualitative-insights>6.1 Qualitative Insights</a></li></ul></li><li><a href=#imageintegrated_hessianscharming_disaster_gradientspng>Also, Integrated Gradients show that &ldquo;charming&rdquo; tried to sway prediction to positive side.
<img src=/integrated_hessians/charming_disaster_gradients.png alt=image></a></li><li><a href=#7-discussion>7. Discussion</a></li></ul></li></ul></nav></aside></header><article class=markdown><h2 id=abstract>Abstract
<a class=anchor href=#abstract>#</a></h2><p>We introduce <strong>Integrated Hessians</strong>, the first scalable second-order attribution method for Transformer models, applying it to a RoBERTa-based sentiment classifier on the IMDb dataset. Our approach quantifies pairwise token interactions via a path-integrated Hessian, extending the axiomatic foundation of Integrated Gradients. We provide a reproducible PyTorch implementation, Dockerized environment, and visualizations (heatmaps of token–token interactions) that reveal linguistic phenomena (e.g., negation and contrast) inaccessible to first-order methods. All code is publicly available for community use and extension.</p><hr><h2 id=1-introduction>1. Introduction
<a class=anchor href=#1-introduction>#</a></h2><p>Deep NLP models such as RoBERTa achieve state-of-the-art performance but remain opaque “black boxes,” posing risks of bias, unreliability, and reduced user trust when deployed in real-world settings. Explaining model decisions is essential for <strong>transparency</strong>, <strong>fairness</strong>, and <strong>debugging</strong>. Existing gradient-based XAI methods (e.g., Integrated Gradients) provide <strong>first-order</strong> attributions of individual features but ignore <strong>pairwise interactions</strong> crucial for language understanding (e.g., “not good”) . We propose <strong>Integrated Hessians</strong>, extending Integrated Gradients to capture second-order effects, thereby revealing how token pairs jointly influence model outputs.</p><hr><h2 id=2-background--related-work>2. Background & Related Work
<a class=anchor href=#2-background--related-work>#</a></h2><h3 id=21-integrated-gradients>2.1 Integrated Gradients
<a class=anchor href=#21-integrated-gradients>#</a></h3><p>Integrated Gradients (IG) attributes feature importance by integrating gradients along a straight-line path from a baseline
<link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\( x'\)
</span>to the input <span>\(x\)
</span>:</p><span>\(\Large
IG_i(x) = (x_i - x_i') \int_{0}^{1} \frac{\partial f\left(x' + \alpha (x - x')\right)}{\partial x_i} \,d\alpha\)</span><p>IG satisfies Sensitivity and Implementation Invariance axioms but is limited to <strong>first-order</strong> effects, failing to capture interactions between features (e.g., “not” + “good”) .</p><h3 id=22-shap>2.2 SHAP
<a class=anchor href=#22-shap>#</a></h3><p>SHAP (SHapley Additive exPlanations) unifies six existing attribution methods under a game-theoretic framework, assigning each feature a Shapley value that satisfies local accuracy, consistency, and missingness properties:
<span>\(\Large
f(x) - \mathbb{E}[f(X)] = \sum_i \phi_i\)
</span>; where <span>\(\phi_i\)
</span>are Shapley values per feature . Kernel SHAP approximates these values for any black-box model but remains computationally expensive for large feature sets.</p><h3 id=23-lime>2.3 LIME
<a class=anchor href=#23-lime>#</a></h3><p>LIME (Local Interpretable Model-Agnostic Explanations) fits a simple surrogate model locally around an input by perturbing features and weighting by proximity, producing an interpretable linear approximation:
<span>\(\Large
\xi(x) = \underset{g \in G}{\arg\min} \; L(f, g, \pi_x) + \Omega(g)\)
</span>; where <span>\( L \)
</span>measures fidelity to <span>\( f \)
</span>, <span>\(\pi_x\)
</span>defines locality, and <span>\( \Omega \)
</span>penalizes complexity . LIME reveals local importance but does not capture interactions.</p><hr><h2 id=3-application-domain>3. Application Domain
<a class=anchor href=#3-application-domain>#</a></h2><ul><li><p><strong>Domain</strong>: Mostly(as if most commonly) sentiment analysis, but in original paper it was also used in Drug combination response prediction.</p></li><li><p><strong>Model</strong>: Any Transformers like BERT, RoBERTa, DistilBERT.</p></li></ul><hr><h2 id=4-integrated-hessians-methodology>4. Integrated Hessians: Methodology
<a class=anchor href=#4-integrated-hessians-methodology>#</a></h2><h3 id=41-mathematical-formulation>4.1 Mathematical Formulation
<a class=anchor href=#41-mathematical-formulation>#</a></h3><h3 id=integrated-gradients-first-order><strong>Integrated Gradients (First-order)</strong>
<a class=anchor href=#integrated-gradients-first-order>#</a></h3><p><span>\(\Large \text{IG}_i(x) = (x_i - x_i') \int_{\alpha=0}^{1} \frac{\partial F(x' + \alpha (x - x'))}{\partial x_i} \, d\alpha\)
</span>;</p><span>\(\large \text{where } \tilde{x}_\alpha = \alpha x + (1 - \alpha)x'\)</span><hr><h3 id=integrated-hessians-second-order><strong>Integrated Hessians (Second-order)</strong>
<a class=anchor href=#integrated-hessians-second-order>#</a></h3><p><span>\(\Large
\text{IH}_{i,j}(x) = (x_i - x_i') (x_j - x_j') \int_{\alpha=0}^{1} \int_{\beta=0}^{1} \frac{\partial^2 F(x' + \alpha \beta (x - x'))}{\partial x_i \, \partial x_j} \, d\beta \, d\alpha\)
</span>;</p><span>\(\large\text{where } \tilde{x}_\beta = \beta x + (1 - \beta)x', \quad
\tilde{x}_{\alpha,\beta} = \alpha \tilde{x}_\beta + (1 - \alpha)x'\)</span><h3 id=42-theoretical-advantages>4.2 Theoretical Advantages
<a class=anchor href=#42-theoretical-advantages>#</a></h3><ul><li><p><strong>Captures Pairwise Interactions</strong>: Explicitly quantifies how two tokens jointly influence predictions, uncovering phenomena such as negation and contrast.</p></li><li><p><strong>Axiomatic Foundation</strong>: Inherits completeness and sensitivity axioms from IG, extended to second-order .</p></li><li><p><strong>Architecture-Agnostic</strong>: Applicable to any differentiable model, including large Transformers.</p></li></ul><hr><h2 id=5-implementation>5. Implementation
<a class=anchor href=#5-implementation>#</a></h2><ul><li><p><strong>Frameworks</strong>: PyTorch, HuggingFace Transformers.</p></li><li><p><strong>Code Structure</strong>:</p><pre tabindex=0><code>xai_integrated_hessians/
.
├── README.md
├── examples
│   └── integrated_hessians_demo.ipynb      # Ipynb with demonstration
├── requierements.txt
└── src
    ├── integrated_gradients.py     # contains function that calculates integrated gradients
    ├── integrated_hessians.py      # contains function that calculates hessians
    └── model_utils.py      # contains functions for plotting, proccessing and model loading
</code></pre></li></ul><h3 id=51-calculating-integrated-gradients>5.1 Calculating Integrated Gradients
<a class=anchor href=#51-calculating-integrated-gradients>#</a></h3><ul><li>Embeddings</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>input_embeddings <span style=color:#f92672>=</span> embeddings(input_ids)
</span></span><span style=display:flex><span>baseline_embeddings <span style=color:#f92672>=</span> embeddings(baseline_ids)
</span></span></code></pre></div><p>These are vectors <span>\( x \)
</span>and <span>\( x' \)</span></p><ul><li>Interpolation</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>scaled_inputs <span style=color:#f92672>=</span> [baseline_embeddings <span style=color:#f92672>+</span> (float(i) <span style=color:#f92672>/</span> steps) <span style=color:#f92672>*</span> (input_embeddings <span style=color:#f92672>-</span> baseline_embeddings) <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(steps <span style=color:#f92672>+</span> <span style=color:#ae81ff>1</span>)]
</span></span></code></pre></div><p>Generates a list of inputs from <span>\( x' \)
</span>to <span>\( x \)
</span>(discretization of the integral)</p><ul><li>Derivative</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>outputs <span style=color:#f92672>=</span> model(inputs_embeds<span style=color:#f92672>=</span>scaled_inputs)[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>gradients <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>autograd<span style=color:#f92672>.</span>grad(outputs<span style=color:#f92672>.</span>sum(), scaled_inputs)[<span style=color:#ae81ff>0</span>]
</span></span></code></pre></div><p>Computing for each <span>\( \alpha \)</span></p><ul><li>Average Gradient (Riemann sum)</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>avg_gradients <span style=color:#f92672>=</span> gradients[:<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]<span style=color:#f92672>.</span>mean(dim<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span></code></pre></div><p>Approximates the integral with a Riemann sum:</p><span>\( \Large
\int_0^1 \nabla F(x' + \alpha(x - x')) d\alpha \approx \frac{1}{m} \sum_{i=1}^{m} \nabla F(x_i)\)</span><ul><li>Final Attribution</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>attributions <span style=color:#f92672>=</span> (input_embeddings <span style=color:#f92672>-</span> baseline_embeddings) <span style=color:#f92672>*</span> avg_gradients
</span></span></code></pre></div><p>This is: <span>\(\large\text{IG}(x) = (x - x') \cdot \text{avg\_gradients}\)</span></p><p>The result is a tensor of shape (1, seq_len, embed_dim).</p><h3 id=52-calculating-integrated-hessians>5.2 Calculating Integrated Hessians
<a class=anchor href=#52-calculating-integrated-hessians>#</a></h3><ul><li>Embedding Interpolation</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>emb_alpha <span style=color:#f92672>=</span> baseline_embed <span style=color:#f92672>+</span> alpha <span style=color:#f92672>*</span> (input_embed <span style=color:#f92672>-</span> baseline_embed)
</span></span></code></pre></div><ul><li>First Derivative</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>grads <span style=color:#f92672>=</span> grad(score, emb_alpha, create_graph<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>)[<span style=color:#ae81ff>0</span>]
</span></span></code></pre></div><ul><li>Second Derivative</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>grad_i <span style=color:#f92672>=</span> grads[<span style=color:#ae81ff>0</span>, i]<span style=color:#f92672>.</span>unsqueeze(<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>hess <span style=color:#f92672>=</span> grad(grad_i, emb_alpha, grad_outputs<span style=color:#f92672>=</span>torch<span style=color:#f92672>.</span>ones_like(grad_i))[<span style=color:#ae81ff>0</span>]
</span></span></code></pre></div><span>\( \Large \frac{\partial^2 f}{\partial x_i \partial x} \)</span><ul><li>Path Integral Multiplication</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>delta <span style=color:#f92672>=</span> (input_embed <span style=color:#f92672>-</span> baseline_embed)[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>ih[i] <span style=color:#f92672>+=</span> (hess <span style=color:#f92672>*</span> delta)<span style=color:#f92672>.</span>sum(dim<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)<span style=color:#f92672>.</span>squeeze(<span style=color:#ae81ff>0</span>)
</span></span></code></pre></div><p><span>\(\Large \sum_k \frac{\partial^2 f}{\partial x_i \partial x_k} \cdot (x_k - x_k') \approx \int \frac{\partial^2 f}{\partial x_i \partial x_k} \cdot (x_k - x_k') \, d\alpha
\)
</span>;</p><p>Which is equivalent to approximation of Integrated Hessians:</p><span>\(\Large \text{IH}_{i,j} = \int_{0}^{1} \frac{\partial^2 f\left(x' + \alpha (x - x')\right)}{\partial x_i \partial x_j} \cdot (x_j - x_j') \, d\alpha
\)</span><ul><li>Averaging Over Steps (Riemann sum)</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>ih <span style=color:#f92672>=</span> ih <span style=color:#f92672>*</span> (<span style=color:#ae81ff>1.0</span> <span style=color:#f92672>/</span> steps)
</span></span></code></pre></div><span>\(\Large \text{IH}_{i,j} \approx \frac{1}{K} \sum_{k=1}^{K} \frac{\partial^2 f\left(x_{\alpha_k}\right)}{\partial x_i \partial x_j} \cdot (x_j - x_j') \)</span><ul><li><strong>Public Repo</strong>: [
<a href=https://github.com/FORZpewpew/xai_integrated_hessians>GitHub link</a>]</li></ul><hr><h2 id=6-experiments--results>6. Experiments & Results
<a class=anchor href=#6-experiments--results>#</a></h2><h3 id=61-qualitative-insights>6.1 Qualitative Insights
<a class=anchor href=#61-qualitative-insights>#</a></h3><ul><li><strong>Negation</strong>: “not bad" pair of phrase &ldquo;this movie was not bad&rdquo; shows strong negative interaction (even though it should be positive, but for some reason pre-trained on imdb model decided that the sentiment is negative with 100% confidence)</li></ul><p><img src=/integrated_hessians/this_movie_was_not_bad.png alt=image></p><p>On this image you can see exact values of token interactions (some mostly meaningless tokens like punktuation is excluded), confirming that pair (not, bad) has the highest absolute value while not being on the same diagonal.
<img src=/integrated_hessians/this_movie_was_not_bad_filtered.png alt=image></p><p>Here&rsquo;s the explanation of another model&rsquo;s prediction on the same phrase. This time the sentiment was predicted correctly and pair (not, bad) still had the highest value
<img src=/integrated_hessians/this_movie_was_not_bad_distilbert.png alt=image></p><ul><li><strong>Mixed signals</strong>: In phrase &ldquo;A charming disaster from start to finish” IH highlights interaction between &ldquo;charming&rdquo; & &ldquo;disaster&rdquo;.
<img src=/integrated_hessians/charming_disaster.png alt=image></li></ul><h2 id=imageintegrated_hessianscharming_disaster_gradientspng>Also, Integrated Gradients show that &ldquo;charming&rdquo; tried to sway prediction to positive side.
<img src=/integrated_hessians/charming_disaster_gradients.png alt=image>
<a class=anchor href=#imageintegrated_hessianscharming_disaster_gradientspng>#</a></h2><h2 id=7-discussion>7. Discussion
<a class=anchor href=#7-discussion>#</a></h2><ul><li><p><strong>Strengths</strong>:</p><ul><li><p>Reveals hidden linguistic structure via second-order attributions.</p></li><li><p>Theoretically grounded and architecture-agnostic.</p></li></ul></li><li><p><strong>Limitations</strong>:</p><ul><li><p>Hessian computation is computationally heavier than gradients.</p></li><li><p>Scalability concerns with very long sequences (quadratic in feature count).</p></li></ul></li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/IU-PR/Capstone_project/tree/master//content/docs/Groups/integrated_hessians.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#abstract>Abstract</a></li><li><a href=#1-introduction>1. Introduction</a></li><li><a href=#2-background--related-work>2. Background & Related Work</a><ul><li><a href=#21-integrated-gradients>2.1 Integrated Gradients</a></li><li><a href=#22-shap>2.2 SHAP</a></li><li><a href=#23-lime>2.3 LIME</a></li></ul></li><li><a href=#3-application-domain>3. Application Domain</a></li><li><a href=#4-integrated-hessians-methodology>4. Integrated Hessians: Methodology</a><ul><li><a href=#41-mathematical-formulation>4.1 Mathematical Formulation</a></li><li><a href=#integrated-gradients-first-order><strong>Integrated Gradients (First-order)</strong></a></li><li><a href=#integrated-hessians-second-order><strong>Integrated Hessians (Second-order)</strong></a></li><li><a href=#42-theoretical-advantages>4.2 Theoretical Advantages</a></li></ul></li><li><a href=#5-implementation>5. Implementation</a><ul><li><a href=#51-calculating-integrated-gradients>5.1 Calculating Integrated Gradients</a></li><li><a href=#52-calculating-integrated-hessians>5.2 Calculating Integrated Hessians</a></li></ul></li><li><a href=#6-experiments--results>6. Experiments & Results</a><ul><li><a href=#61-qualitative-insights>6.1 Qualitative Insights</a></li></ul></li><li><a href=#imageintegrated_hessianscharming_disaster_gradientspng>Also, Integrated Gradients show that &ldquo;charming&rdquo; tried to sway prediction to positive side.
<img src=/integrated_hessians/charming_disaster_gradients.png alt=image></a></li><li><a href=#7-discussion>7. Discussion</a></li></ul></li></ul></nav></div></aside></main></body></html>