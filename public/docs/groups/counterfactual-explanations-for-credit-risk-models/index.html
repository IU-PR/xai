<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Counterfactual Explanations for Credit Risk Models: A Case Study # TL;DR # In this case study, we implement and incrementally refine a gradient-based approach for generating counterfactual explanations in credit risk modeling. Beginning with a basic optimization procedure, we identify and resolve multiple real-world issues:
Immutable and semantically constrained features Inter-feature dependencies (e.g., derived ratios) Ordinal variables that require discrete treatment One-hot encoded categorical features that demand joint behavior We develop solutions involving gradient masking, differentiable approximations, manual feature injection, and the Gumbel-Softmax trick to ensure our counterfactuals are not only effective, but valid, interpretable, and realistic."><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/groups/counterfactual-explanations-for-credit-risk-models/"><meta property="og:site_name" content="XAI"><meta property="og:title" content="Counterfactual Explanations for Credit Risk Models: A Case Study"><meta property="og:description" content="Counterfactual Explanations for Credit Risk Models: A Case Study # TL;DR # In this case study, we implement and incrementally refine a gradient-based approach for generating counterfactual explanations in credit risk modeling. Beginning with a basic optimization procedure, we identify and resolve multiple real-world issues:
Immutable and semantically constrained features Inter-feature dependencies (e.g., derived ratios) Ordinal variables that require discrete treatment One-hot encoded categorical features that demand joint behavior We develop solutions involving gradient masking, differentiable approximations, manual feature injection, and the Gumbel-Softmax trick to ensure our counterfactuals are not only effective, but valid, interpretable, and realistic."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>Counterfactual Explanations for Credit Risk Models: A Case Study | XAI</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.e832d4e94212199857473bcf13a450d089c3fcd54ccadedcfac84ed0feff83fb.css integrity="sha256-6DLU6UISGZhXRzvPE6RQ0InD/NVMyt7c+shO0P7/g/s=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/mathtex-script-type.min.js integrity=sha384-jiBVvJ8NGGj5n7kJaiWwWp9AjC+Yh8rhZY3GtAX8yU28azcLgoRo4oukO87g7zDT crossorigin=anonymous></script><script defer src=/flexsearch.min.js></script><script defer src=/en.search.min.ad436edd829ec592525c968cb38b5379be6117b5639c053bb9908a9c0a469c15.js integrity="sha256-rUNu3YKexZJSXJaMs4tTeb5hF7VjnAU7uZCKnApGnBU=" crossorigin=anonymous></script><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/YELLOW_BAR.png alt=Logo><span><b>XAI</b></span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=/docs/groups/cam_and_secam/>CAM and SeCAM</a></li><li><a href=/docs/groups/counterfactual-explanations-for-credit-risk-models/ class=active>Counterfactual Explanations for Credit Risk Models: A Case Study</a></li><li><a href=/docs/groups/diffusion-lens-interpreting-text-encoders-in-text-to-image-pipelines-tuned-using-dreambooth/>Diffusion Lens: Interpreting Text Encoders in Text-to-Image pipelines</a></li><li><a href=/docs/groups/dimensionality-reduction-in-nlp-visualizing-sentence-embeddings-with-umap-and-t-sne/>Dimensionality Reduction in NLP: Visualizing Sentence Embeddings with UMAP and t-SNE</a></li><li><a href=/docs/groups/example/>Example</a></li><li><a href=/docs/groups/ai-playing-geoguessr-explained/>Ai Playing Geo Guessr Explained</a></li><li><a href=/docs/groups/contrastive-grad-cam-consistency/>Contrastive Grad Cam Consistency</a></li><li><a href=/docs/groups/dndfs_shap/>Dndfs Shap</a></li><li><a href=/docs/groups/gradcam/>Grad Cam</a></li><li><a href=/docs/groups/integrated-gradients/>Integrated Gradients</a></li><li><a href=/docs/groups/kernel-shap/>Kernel Shap</a></li><li><a href=/docs/groups/rag/>Rag</a></li><li><a href=/docs/groups/shap_darya_and_viktoria/>Shap Darya and Viktoria</a></li><li><a href=/docs/groups/sverl_tac_toe/>Sverl Tac Toe</a></li><li><a href=/docs/groups/torchprism/>Torch Prism</a></li><li><a href=/docs/groups/xai_for_transformers/>Xai for Transformers</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Counterfactual Explanations for Credit Risk Models: A Case Study</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#counterfactual-explanations-for-credit-risk-models-a-case-study>Counterfactual Explanations for Credit Risk Models: A Case Study</a><ul><li><a href=#tldr>TL;DR</a></li><li><a href=#1-introduction-what-are-counterfactual-explanations>1. Introduction: What Are Counterfactual Explanations?</a></li><li><a href=#2-feature-overview-and-data-constraints>2. Feature Overview and Data Constraints</a></li><li><a href=#3-baseline-counterfactual-optimization>3. Baseline Counterfactual Optimization</a><ul><li><a href=#result-example>Result Example</a></li></ul></li><li><a href=#4-maintaining-derived-feature-consistency>4. Maintaining Derived Feature Consistency</a><ul><li><a href=#problem>Problem</a></li><li><a href=#root-cause>Root Cause</a></li><li><a href=#solution>Solution</a></li><li><a href=#example>Example</a></li></ul></li><li><a href=#5-treating-ordinal-features-correctly>5. Treating Ordinal Features Correctly</a><ul><li><a href=#problem-1>Problem</a></li><li><a href=#solutions>Solutions</a></li><li><a href=#final-output>Final Output</a></li><li><a href=#example-1>Example</a></li></ul></li><li><a href=#6-ensuring-valid-one-hot-encoded-categorical-features>6. Ensuring Valid One-Hot Encoded Categorical Features</a><ul><li><a href=#initial-misstep>Initial Misstep</a></li><li><a href=#desired-behavior>Desired Behavior</a></li><li><a href=#solution-gumbel-softmax-trick>Solution: Gumbel-Softmax Trick</a></li><li><a href=#at-inference>At Inference</a></li><li><a href=#example-2>Example</a></li></ul></li><li><a href=#7-full-optimization-pipeline>7. Full Optimization Pipeline</a><ul><li><a href=#at-each-iteration>At each iteration:</a></li></ul></li><li><a href=#8-summary-of-techniques>8. Summary of Techniques</a></li><li><a href=#9-technical-implementation>9. Technical Implementation</a><ul><li><a href=#repository-structure>Repository Structure</a></li><li><a href=#notebooks>Notebooks</a></li><li><a href=#model-training-visualization>Model Training Visualization</a></li><li><a href=#deployment>Deployment</a></li></ul></li><li><a href=#10-conclusion>10. Conclusion</a></li><li><a href=#references>References</a></li></ul></li></ul></nav></aside></header><article class=markdown><h1 id=counterfactual-explanations-for-credit-risk-models-a-case-study>Counterfactual Explanations for Credit Risk Models: A Case Study
<a class=anchor href=#counterfactual-explanations-for-credit-risk-models-a-case-study>#</a></h1><hr><h2 id=tldr>TL;DR
<a class=anchor href=#tldr>#</a></h2><p>In this case study, we implement and incrementally refine a gradient-based approach for generating <strong>counterfactual explanations</strong> in credit risk modeling. Beginning with a basic optimization procedure, we identify and resolve multiple real-world issues:</p><ul><li>Immutable and semantically constrained features</li><li>Inter-feature dependencies (e.g., derived ratios)</li><li>Ordinal variables that require discrete treatment</li><li>One-hot encoded categorical features that demand joint behavior</li></ul><p>We develop solutions involving <strong>gradient masking</strong>, <strong>differentiable approximations</strong>, <strong>manual feature injection</strong>, and the <strong>Gumbel-Softmax trick</strong> to ensure our counterfactuals are not only effective, but <strong>valid</strong>, <strong>interpretable</strong>, and <strong>realistic</strong>.</p><hr><h2 id=1-introduction-what-are-counterfactual-explanations>1. Introduction: What Are Counterfactual Explanations?
<a class=anchor href=#1-introduction-what-are-counterfactual-explanations>#</a></h2><p>Given a binary classifier
<link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\( f(\mathbf{x}) \)
</span>trained to predict whether a loan applicant will <strong>default</strong>, counterfactual explanations aim to answer:</p><blockquote><p><em>‚ÄúWhat minimal changes to the applicant‚Äôs features would have changed the model‚Äôs decision?‚Äù</em></p></blockquote><p>This is framed as an optimization:</p><span>\[\min_{\mathbf{x}'} \; d(\mathbf{x}, \mathbf{x}') + \lambda \,\mathcal{L}(f(\mathbf{x}'), c)\]</span><p>Where:</p><ul><li><span>\( \mathbf{x} \)
</span>: original input (e.g., borrower profile)</li><li><span>\( \mathbf{x}' \)
</span>: counterfactual</li><li><span>\( d(\cdot) \)
</span>: distance measure (e.g., <span>\( L_2 \)
</span>)</li><li><span>\( \mathcal{L} \)
</span>: classification loss (e.g., binary cross-entropy)</li><li><span>\( c \)
</span>: desired target label (e.g., ‚Äúnon-default‚Äù)</li><li><span>\( \lambda \)
</span>: hyperparameter for trade-off</li></ul><p>This setup seeks small yet decisive modifications leading to a different prediction.</p><hr><h2 id=2-feature-overview-and-data-constraints>2. Feature Overview and Data Constraints
<a class=anchor href=#2-feature-overview-and-data-constraints>#</a></h2><p>Below is a portion of the feature table, which highlights which attributes are editable and which must be constrained:</p><table><thead><tr><th><strong>Feature</strong></th><th><strong>Type</strong></th><th><strong>Editable?</strong></th><th><strong>Notes</strong></th></tr></thead><tbody><tr><td><code>person_age</code></td><td>Numerical</td><td>No</td><td>Immutable personal attribute</td></tr><tr><td><code>person_income</code></td><td>Numerical</td><td>Yes</td><td>Editable under assumptions like increased reported income</td></tr><tr><td><code>loan_amnt</code></td><td>Numerical</td><td>Yes</td><td>Can be adjusted in application</td></tr><tr><td><code>loan_percent_income</code></td><td>Derived</td><td>No (recalculated)</td><td>Must reflect ratio <span>\( \frac{\text{loan}}{\text{income}} \)</span></td></tr><tr><td><code>person_home_ownership</code></td><td>Ordinal</td><td>Yes</td><td>Must remain integer in range <span>\([0,3]\)</span></td></tr><tr><td><code>loan_intent_*</code></td><td>One-hot</td><td>No</td><td>User-declared purpose; not editable</td></tr><tr><td><code>loan_grade_*</code></td><td>One-hot</td><td>No</td><td>Lender-assigned; immutable</td></tr><tr><td><code>cb_person_default_on_file_*</code></td><td>One-hot</td><td>No</td><td>Historical; immutable</td></tr></tbody></table><hr><h2 id=3-baseline-counterfactual-optimization>3. Baseline Counterfactual Optimization
<a class=anchor href=#3-baseline-counterfactual-optimization>#</a></h2><p>Our initial approach used unconstrained gradient descent to find <span>\( \mathbf{x}' \)
</span>:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x_cf <span style=color:#f92672>=</span> x_original<span style=color:#f92672>.</span>clone()<span style=color:#f92672>.</span>requires_grad_(<span style=color:#66d9ef>True</span>)
</span></span><span style=display:flex><span>optimizer <span style=color:#f92672>=</span> torch<span style=color:#f92672>.</span>optim<span style=color:#f92672>.</span>Adam([x_cf])
</span></span></code></pre></div><p>The loss combined:</p><ul><li><strong>Distance</strong>: <span>\( \| \mathbf{x}' - \mathbf{x} \|_2 \)</span></li><li><strong>Prediction</strong>: BCE loss w.r.t. the desired label</li></ul><h3 id=result-example>Result Example
<a class=anchor href=#result-example>#</a></h3><p>For a loan applicant predicted to <strong>default</strong>, the method produced a counterfactual flipping the label to <strong>non-default</strong> by nudging:</p><table><thead><tr><th>Feature</th><th>Original</th><th>Counterfactual</th><th>Œî</th></tr></thead><tbody><tr><td><code>person_income</code></td><td>0.0147</td><td>0.0141</td><td>-0.0006</td></tr><tr><td><code>loan_amnt</code></td><td>0.0956</td><td>0.1076</td><td>+0.0119</td></tr></tbody></table><p>Conceptually effective ‚Äî but many <strong>invalid changes</strong> appeared in derived, ordinal, and categorical features.</p><hr><h2 id=4-maintaining-derived-feature-consistency>4. Maintaining Derived Feature Consistency
<a class=anchor href=#4-maintaining-derived-feature-consistency>#</a></h2><h3 id=problem>Problem
<a class=anchor href=#problem>#</a></h3><p>We expected:</p><span>\[\text{loan\_percent\_income} = \frac{\text{loan\_amnt}}{\text{person\_income}}\]</span><p>But after checking with:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>assert</span> np<span style=color:#f92672>.</span>allclose(data[<span style=color:#e6db74>&#39;loan_amnt&#39;</span>] <span style=color:#f92672>/</span> data[<span style=color:#e6db74>&#39;person_income&#39;</span>], data[<span style=color:#e6db74>&#39;loan_percent_income&#39;</span>])
</span></span></code></pre></div><p>We received an <code>AssertionError</code>.</p><h3 id=root-cause>Root Cause
<a class=anchor href=#root-cause>#</a></h3><p>The discrepancy arose from <strong>independent MinMax scaling</strong>. Since each variable was normalized individually:</p><span>\[x_{\text{scaled}} = \frac{x - x_{\min}}{x_{\max} - x_{\min}}\]</span><p>the relationship:</p><span>\[\frac{\text{scaled\_loan}}{\text{scaled\_income}} \neq \text{scaled\_loan\_percent\_income}\]</span><h3 id=solution>Solution
<a class=anchor href=#solution>#</a></h3><p>To preserve consistency, we:</p><ol><li>Inverse-transform <code>loan_amnt</code> and <code>person_income</code> from <span>\([0,1]\)
</span>to real domain</li><li>Recompute:
<span>\[ r = \frac{\text{loan\_amnt}}{\text{income}}
\]</span></li><li>Re-scale <span>\(r\)
</span>into <span>\([0,1]\)</span></li><li>Inject this back into the <code>loan_percent_income</code> column</li></ol><p>This is done after every optimizer step. The feature remains <strong>frozen</strong> during training, but <strong>dynamically updated</strong> to reflect valid relationships.</p><h3 id=example>Example
<a class=anchor href=#example>#</a></h3><table><thead><tr><th>Feature</th><th>Original</th><th>Counterfactual</th><th>Œî</th></tr></thead><tbody><tr><td><code>loan_percent_income</code></td><td>0.1424</td><td>0.1635</td><td>+0.0211</td></tr></tbody></table><p>Shows a recalculated, updated value consistent with other features.</p><hr><h2 id=5-treating-ordinal-features-correctly>5. Treating Ordinal Features Correctly
<a class=anchor href=#5-treating-ordinal-features-correctly>#</a></h2><h3 id=problem-1>Problem
<a class=anchor href=#problem-1>#</a></h3><p>Features like <code>person_home_ownership</code> are stored as integers (e.g., 0 = RENT, 1 = MORTGAGE). During unconstrained optimization, we observed invalid intermediate values:</p><table><thead><tr><th>Feature</th><th>Original</th><th>Counterfactual</th></tr></thead><tbody><tr><td><code>person_home_ownership</code></td><td>1.0000</td><td>0.5095</td></tr></tbody></table><p>The model was never trained on fractional values ‚Äî leading to unpredictable behavior.</p><h3 id=solutions>Solutions
<a class=anchor href=#solutions>#</a></h3><h4 id=a-snapping>A. Snapping
<a class=anchor href=#a-snapping>#</a></h4><p>After each step:</p><span>\(x_{\text{ordinal}} = \text{round}(x_{\text{ordinal}})\)</span><p>Ensures the feature stays valid, but introduces discontinuities in gradients.</p><h4 id=b-soft-round-penalty>B. Soft-Round Penalty
<a class=anchor href=#b-soft-round-penalty>#</a></h4><p>We added a soft penalty to the loss:</p><span>\(\alpha \sum_{i \in \text{ordinals}} \left| x_i - \text{soft\_round}(x_i) \right|\)</span><p>Where:</p><span>\(\text{soft\_round}(x) = \lfloor x \rfloor + \sigma\left( \beta(x - \lfloor x \rfloor - 0.5) \right)\)</span><ul><li>Smooth approximation to rounding</li><li>Allows gradient descent to work effectively</li></ul><h3 id=final-output>Final Output
<a class=anchor href=#final-output>#</a></h3><p>After optimization, we <strong>snap</strong> to the nearest integer to guarantee validity.</p><h3 id=example-1>Example
<a class=anchor href=#example-1>#</a></h3><table><thead><tr><th>Feature</th><th>Original</th><th>Counterfactual</th><th>Treated</th><th>Untreated</th></tr></thead><tbody><tr><td><code>person_home_ownership</code></td><td>1.0000</td><td>0.0000</td><td>‚úÖ Yes</td><td>0.5095</td></tr></tbody></table><hr><h2 id=6-ensuring-valid-one-hot-encoded-categorical-features>6. Ensuring Valid One-Hot Encoded Categorical Features
<a class=anchor href=#6-ensuring-valid-one-hot-encoded-categorical-features>#</a></h2><h3 id=initial-misstep>Initial Misstep
<a class=anchor href=#initial-misstep>#</a></h3><p>Initially, we treated each <strong>one-hot column independently</strong> ‚Äî for instance:</p><ul><li><code>loan_intent_EDUCATION</code>, <code>loan_intent_MEDICAL</code>, etc., were optimized separately.</li></ul><p>This created invalid one-hot groups like:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>EDUCATION: 0.22
</span></span><span style=display:flex><span>PERSONAL:  0.39
</span></span><span style=display:flex><span>MEDICAL:   0.19
</span></span><span style=display:flex><span>VENTURE:   0.20
</span></span></code></pre></div><p>This is <strong>not a valid categorical state</strong>, since:</p><ul><li>More than one entry ‚â† 1.0</li><li>Their sum ‚â† 1.0</li><li>The model was never trained on such combinations</li></ul><h3 id=desired-behavior>Desired Behavior
<a class=anchor href=#desired-behavior>#</a></h3><p>If one value in a one-hot group is increased (e.g., MEDICAL from 0 ‚Üí 1), all others should go to 0. In other words:</p><span>\(\sum_{j=1}^K x_j = 1, \quad x_j \in \{0, 1\}\)</span><h3 id=solution-gumbel-softmax-trick>Solution: Gumbel-Softmax Trick
<a class=anchor href=#solution-gumbel-softmax-trick>#</a></h3><p>We reformulate the optimization problem using <strong>logits</strong> <span>\( \boldsymbol{\pi} \)
</span>instead of directly optimizing one-hot vectors.</p><p>Each one-hot group is represented as:</p><span>\(\mathbf{y} = \text{softmax}\left( \frac{\log \boldsymbol{\pi} + \mathbf{g}}{\tau} \right)\)</span><p>Where:</p><ul><li><span>\( \mathbf{g} \sim \text{Gumbel}(0, 1) \)
</span>: random noise</li><li><span>\( \tau \)
</span>: temperature parameter</li></ul><p>During training:</p><ul><li><span>\( \mathbf{y} \in [0,1]^K \)</span></li><li><span>\( \sum y_i = 1 \)</span></li><li>Behaves like a softened categorical distribution</li></ul><h3 id=at-inference>At Inference
<a class=anchor href=#at-inference>#</a></h3><p>After optimization:</p><span>\(\mathbf{y}_{\text{final}} = \text{one\_hot}(\arg\max_i \pi_i)\)</span><p>We recover a valid one-hot vector for model input and interpretation.</p><h3 id=example-2>Example
<a class=anchor href=#example-2>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># Before optimization</span>
</span></span><span style=display:flex><span>[<span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>]  <span style=color:#75715e># PERSONAL</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># After optimization (logits + Gumbel)</span>
</span></span><span style=display:flex><span>[<span style=color:#ae81ff>0.25</span>, <span style=color:#ae81ff>0.25</span>, <span style=color:#ae81ff>0.05</span>, <span style=color:#ae81ff>0.40</span>, <span style=color:#ae81ff>0.05</span>, <span style=color:#ae81ff>0.00</span>]  <span style=color:#75715e># Soft</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Final snapped output</span>
</span></span><span style=display:flex><span>[<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>1</span>, <span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>0</span>]  <span style=color:#75715e># MEDICAL</span>
</span></span></code></pre></div><p>The result is a valid category shift, suitable for generating counterfactuals in exploratory settings.</p><hr><h2 id=7-full-optimization-pipeline>7. Full Optimization Pipeline
<a class=anchor href=#7-full-optimization-pipeline>#</a></h2><h3 id=at-each-iteration>At each iteration:
<a class=anchor href=#at-each-iteration>#</a></h3><ol><li><p><strong>Forward pass</strong>:</p><ul><li>Compute model prediction</li><li>Compute distance loss</li><li>Add penalties (e.g., soft-round)</li></ul></li><li><p><strong>Backward pass</strong>:</p><ul><li>Apply gradient mask to immutable features</li></ul></li><li><p><strong>Step</strong>:</p><ul><li>Perform <code>optimizer.step()</code></li></ul></li><li><p><strong>Post-processing</strong>:</p><ul><li>Recompute derived features (e.g., ratios)</li><li>Inject into <code>x_cf</code></li><li>Apply soft-round or snapping to ordinal features</li><li>Apply Gumbel-softmax projection to categorical groups</li></ul></li></ol><hr><h2 id=8-summary-of-techniques>8. Summary of Techniques
<a class=anchor href=#8-summary-of-techniques>#</a></h2><table><thead><tr><th>Feature Type</th><th>Issue</th><th>Solution</th></tr></thead><tbody><tr><td>Derived ratio</td><td>Invalid dependency</td><td>Manual recomputation + injection</td></tr><tr><td>Ordinal (int, 0‚Äì3)</td><td>Fractional output</td><td>Soft-round penalty + final rounding</td></tr><tr><td>One-hot categorical</td><td>Soft invalid combinations</td><td>Gumbel-softmax + snapping</td></tr><tr><td>Frozen features</td><td>Should not change</td><td>Gradient masking</td></tr></tbody></table><hr><h2 id=9-technical-implementation>9. Technical Implementation
<a class=anchor href=#9-technical-implementation>#</a></h2><p>The practical implementation of the concepts discussed in this report can be explored through our open-source repository. The repository contains all the necessary code, configurations, datasets, and interactive notebooks required for replicating our results.</p><p><strong>GitHub Repository:</strong><br><a href=https://github.com/ForYourEyesOnlyyy/Credit-Risk-Analysis-Counterfactual-Explanations>üêà‚Äç‚¨õ Link</a></p><h3 id=repository-structure>Repository Structure
<a class=anchor href=#repository-structure>#</a></h3><p>The repository is structured clearly to facilitate easy navigation and reproducibility of the results:</p><p><img src=/CounterfactualExplanations/repo_screenshot.png alt=Repository></p><h3 id=notebooks>Notebooks
<a class=anchor href=#notebooks>#</a></h3><p>The notebook <code>counterfactual_explanations.ipynb</code> provides an interactive and detailed step-by-step walkthrough of the counterfactual generation process, illustrating clearly how each technique from gradient masking to Gumbel-softmax is applied.</p><p><img src=/CounterfactualExplanations/cf_notebook_1.png alt>
<img src=/CounterfactualExplanations/cf_notebook_2.png alt></p><h3 id=model-training-visualization>Model Training Visualization
<a class=anchor href=#model-training-visualization>#</a></h3><p>During the training phase, we monitored the model&rsquo;s performance metrics using TensorBoard, enabling us to fine-tune the hyperparameters and verify convergence effectively.</p><p><img src=/CounterfactualExplanations/tensorboard_experiments.png alt="TensorBoard Screenshot"></p><h3 id=deployment>Deployment
<a class=anchor href=#deployment>#</a></h3><p>To make our approach interactive and accessible, we have implemented a deployment pipeline using Streamlit. After obtaining the trained model weights and updating configuration files, users can deploy the application locally by running:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>bash deploy.sh
</span></span></code></pre></div><p>This will launch a Streamlit app that enables interactive exploration of counterfactual explanations.</p><p><img src=/CounterfactualExplanations/app_UI1.png alt></p><p><img src=/CounterfactualExplanations/app_UI2.png alt></p><p>The interactive deployment provides users with an intuitive interface to explore how changes in feature values affect predictions, thus demonstrating the practical applicability and interpretability of our method.</p><hr><h2 id=10-conclusion>10. Conclusion
<a class=anchor href=#10-conclusion>#</a></h2><p>This case study demonstrates how to go from a naive counterfactual optimizer to a <strong>realistic, interpretable system</strong> that respects domain logic and data semantics.</p><p>By:</p><ul><li>Freezing immutable features</li><li>Respecting feature relationships</li><li>Maintaining categorical validity</li><li>Applying differentiable approximations</li></ul><p>we move toward <strong>counterfactuals that are both accurate and meaningful</strong> ‚Äî critical for high-stakes domains like finance and credit.</p><hr><h2 id=references>References
<a class=anchor href=#references>#</a></h2><ul><li><p><strong>Counterfactual Explanations</strong>:<br>Wachter, S., Mittelstadt, B., & Russell, C. (2018).<br><em>Counterfactual explanations without opening the black box: Automated decisions and the GDPR</em>.<br><em>Harvard Journal of Law & Technology</em>, 31(2), 841‚Äì887.<br><a href=https://arxiv.org/abs/1711.00399>https://arxiv.org/abs/1711.00399</a></p></li><li><p><strong>Gumbel-Softmax Trick for Differentiable Sampling</strong>:<br>Jang, E., Gu, S., & Poole, B. (2017).<br><em>Categorical reparameterization with Gumbel-Softmax</em>.<br><em>International Conference on Learning Representations (ICLR)</em>.<br><a href=https://arxiv.org/abs/1611.01144>https://arxiv.org/abs/1611.01144</a></p></li><li><p><strong>Soft-Rounding (Differentiable Approximation)</strong>:<br>Agustsson, E., & Theis, L. (2020).<br><em>Universally Quantized Neural Compression</em>.<br><em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 33, 12367‚Äì12376.<br><a href=https://arxiv.org/abs/2006.09952>https://arxiv.org/abs/2006.09952</a></p></li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/IU-PR/Capstone_project/tree/master//content/docs/Groups/Counterfactual-Explanations-for-Credit-Risk-Models.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#counterfactual-explanations-for-credit-risk-models-a-case-study>Counterfactual Explanations for Credit Risk Models: A Case Study</a><ul><li><a href=#tldr>TL;DR</a></li><li><a href=#1-introduction-what-are-counterfactual-explanations>1. Introduction: What Are Counterfactual Explanations?</a></li><li><a href=#2-feature-overview-and-data-constraints>2. Feature Overview and Data Constraints</a></li><li><a href=#3-baseline-counterfactual-optimization>3. Baseline Counterfactual Optimization</a><ul><li><a href=#result-example>Result Example</a></li></ul></li><li><a href=#4-maintaining-derived-feature-consistency>4. Maintaining Derived Feature Consistency</a><ul><li><a href=#problem>Problem</a></li><li><a href=#root-cause>Root Cause</a></li><li><a href=#solution>Solution</a></li><li><a href=#example>Example</a></li></ul></li><li><a href=#5-treating-ordinal-features-correctly>5. Treating Ordinal Features Correctly</a><ul><li><a href=#problem-1>Problem</a></li><li><a href=#solutions>Solutions</a></li><li><a href=#final-output>Final Output</a></li><li><a href=#example-1>Example</a></li></ul></li><li><a href=#6-ensuring-valid-one-hot-encoded-categorical-features>6. Ensuring Valid One-Hot Encoded Categorical Features</a><ul><li><a href=#initial-misstep>Initial Misstep</a></li><li><a href=#desired-behavior>Desired Behavior</a></li><li><a href=#solution-gumbel-softmax-trick>Solution: Gumbel-Softmax Trick</a></li><li><a href=#at-inference>At Inference</a></li><li><a href=#example-2>Example</a></li></ul></li><li><a href=#7-full-optimization-pipeline>7. Full Optimization Pipeline</a><ul><li><a href=#at-each-iteration>At each iteration:</a></li></ul></li><li><a href=#8-summary-of-techniques>8. Summary of Techniques</a></li><li><a href=#9-technical-implementation>9. Technical Implementation</a><ul><li><a href=#repository-structure>Repository Structure</a></li><li><a href=#notebooks>Notebooks</a></li><li><a href=#model-training-visualization>Model Training Visualization</a></li><li><a href=#deployment>Deployment</a></li></ul></li><li><a href=#10-conclusion>10. Conclusion</a></li><li><a href=#references>References</a></li></ul></li></ul></nav></div></aside></main></body></html>