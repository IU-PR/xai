<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Dimensionality Reduction in NLP: Visualizing Sentence Embeddings with UMAP and t-SNE # Students: # Yazan Kbaili ( y.kbaili@innopolis.university) Hamada Salhab ( h.salhab@innopolis.university) Introduction # This report delves into the visualization of sentence embeddings derived from the roberta-base model fine-tuned on the &ldquo;go-emotion&rdquo; dataset using two prominent dimensionality reduction techniques: UMAP (Uniform Manifold Approximation and Projection) and t-SNE (t-distributed Stochastic Neighbor Embedding). The dataset used in the visualization consists of Twitter messages and is called “emotions”."><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/groups/dimensionality-reduction-in-nlp_-visualizing-sentence-embeddings-with-umap-and-t-sne/"><meta property="og:site_name" content="XAI"><meta property="og:title" content="XAI"><meta property="og:description" content="Dimensionality Reduction in NLP: Visualizing Sentence Embeddings with UMAP and t-SNE # Students: # Yazan Kbaili ( y.kbaili@innopolis.university) Hamada Salhab ( h.salhab@innopolis.university) Introduction # This report delves into the visualization of sentence embeddings derived from the roberta-base model fine-tuned on the “go-emotion” dataset using two prominent dimensionality reduction techniques: UMAP (Uniform Manifold Approximation and Projection) and t-SNE (t-distributed Stochastic Neighbor Embedding). The dataset used in the visualization consists of Twitter messages and is called “emotions”."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>Dimensionality Reduction in Nlp Visualizing Sentence Embeddings With Umap and T Sne | XAI</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.e832d4e94212199857473bcf13a450d089c3fcd54ccadedcfac84ed0feff83fb.css integrity="sha256-6DLU6UISGZhXRzvPE6RQ0InD/NVMyt7c+shO0P7/g/s=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/mathtex-script-type.min.js integrity=sha384-jiBVvJ8NGGj5n7kJaiWwWp9AjC+Yh8rhZY3GtAX8yU28azcLgoRo4oukO87g7zDT crossorigin=anonymous></script><script defer src=/flexsearch.min.js></script><script defer src=/en.search.min.17c6a74252549f8056ae9171d1f757f6824be99886d02bbb5111a1538e651723.js integrity="sha256-F8anQlJUn4BWrpFx0fdX9oJL6ZiG0Cu7URGhU45lFyM=" crossorigin=anonymous></script><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/YELLOW_BAR.png alt=Logo><span><b>XAI</b></span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=/docs/groups/example/>Example</a></li><li><a href=/docs/groups/dimensionality-reduction-in-nlp_-visualizing-sentence-embeddings-with-umap-and-t-sne/ class=active>Dimensionality Reduction in Nlp Visualizing Sentence Embeddings With Umap and T Sne</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Dimensionality Reduction in Nlp Visualizing Sentence Embeddings With Umap and T Sne</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#dimensionality-reduction-in-nlp-visualizing-sentence-embeddings-with-umap-and-t-sne>Dimensionality Reduction in NLP: Visualizing Sentence Embeddings with UMAP and t-SNE</a><ul><li><a href=#students>Students:</a></li><li><a href=#introduction>Introduction</a></li><li><a href=#methodology>Methodology</a><ul><li><a href=#umap>UMAP</a></li><li><a href=#t-sne>t-SNE</a></li><li><a href=#umap-vs-t-sne>UMAP vs t-SNE</a></li></ul></li><li><a href=#extraction-of-embeddings>Extraction of Embeddings</a></li><li><a href=#visualization>Visualization</a><ul><li><a href=#t-sne-1>t-SNE</a></li><li><a href=#umap-1>UMAP</a></li></ul></li><li><a href=#insights>Insights</a></li><li><a href=#extra-insights>Extra Insights</a></li><li><a href=#applications>Applications</a></li><li><a href=#colab-notebook>Colab Notebook</a></li></ul></li></ul></nav></aside></header><article class=markdown><h1 id=dimensionality-reduction-in-nlp-visualizing-sentence-embeddings-with-umap-and-t-sne>Dimensionality Reduction in NLP: Visualizing Sentence Embeddings with UMAP and t-SNE
<a class=anchor href=#dimensionality-reduction-in-nlp-visualizing-sentence-embeddings-with-umap-and-t-sne>#</a></h1><h2 id=students>Students:
<a class=anchor href=#students>#</a></h2><ul><li>Yazan Kbaili (
<a href=mailto:y.kbaili@innopolis.university>y.kbaili@innopolis.university</a>)</li><li>Hamada Salhab (
<a href=mailto:h.salhab@innopolis.university>h.salhab@innopolis.university</a>)</li></ul><h2 id=introduction>Introduction
<a class=anchor href=#introduction>#</a></h2><p>This report delves into the visualization of sentence embeddings derived from the <code>roberta-base</code> model fine-tuned on the &ldquo;go-emotion&rdquo; dataset using two prominent dimensionality reduction techniques: UMAP (Uniform Manifold Approximation and Projection) and t-SNE (t-distributed Stochastic Neighbor Embedding). The dataset used in the visualization consists of Twitter messages and is called “emotions”.</p><h2 id=methodology>Methodology
<a class=anchor href=#methodology>#</a></h2><p>The embeddings were generated by the <code>roberta-base</code> model, specifically tailored for multiclass emotion classification and accessible via Hugging Face. This model is particularly adept at interpreting emotional contexts within text, making it highly suitable for our study. We focus on embeddings from Twitter messages categorized into six distinct emotions.</p><h3 id=umap>UMAP
<a class=anchor href=#umap>#</a></h3><p>Operates on a foundation of algebraic topology, creating a high-dimensional graph representation of data before optimizing this layout in a lower-dimensional space. It aims to preserve both local and global structures, offering a comprehensive data understanding. UMAP is generally preferred for its ability to maintain a more global structure compared to t-SNE.</p><h3 id=t-sne>t-SNE
<a class=anchor href=#t-sne>#</a></h3><p>Transforms high-dimensional Euclidean distances between points into conditional probabilities that reflect similarities, excelling in the preservation of local data structures and the identification of clusters. While effective, t-SNE can be computationally demanding, especially with large datasets, and may exaggerate cluster separations without maintaining global data integrity.</p><h3 id=umap-vs-t-sne>UMAP vs t-SNE
<a class=anchor href=#umap-vs-t-sne>#</a></h3><p>The behavior of t-SNE and UMAP differs significantly in terms of initialization and the iterative process used to optimize the low-dimensional representation. Below are some of the key differences:</p><table><thead><tr><th>Feature</th><th>t-SNE</th><th>UMAP</th></tr></thead><tbody><tr><td><strong>Initialization</strong></td><td>Starts with a random initialization of the graph.</td><td>Uses Spectral Embedding for deterministic initialization.</td></tr><tr><td><strong>Iteration Process</strong></td><td>Moves every single point slightly each iteration.</td><td>Can move just one point or a small subset of points each time.</td></tr><tr><td><strong>Scalability</strong></td><td>Less efficient with very large datasets.</td><td>Scales well with large datasets due to its partial update approach.</td></tr></tbody></table><h2 id=extraction-of-embeddings>Extraction of Embeddings
<a class=anchor href=#extraction-of-embeddings>#</a></h2><p>We utilized the embedding from the last token at the last layer of the pre-trained model, which typically encapsulates the sentence&rsquo;s contextual essence.</p><h2 id=visualization>Visualization
<a class=anchor href=#visualization>#</a></h2><p>For an interactive visual representation, we employed Plotly, enabling detailed exploration of the structures within the embeddings.</p><h3 id=t-sne-1>t-SNE
<a class=anchor href=#t-sne-1>#</a></h3><p>1:
<img src=https://hackmd.io/_uploads/Hy0VHwm7R.png alt=image>
2:
<img src=https://hackmd.io/_uploads/H1a6ul7mC.png alt=t-SNE></p><h3 id=umap-1>UMAP
<a class=anchor href=#umap-1>#</a></h3><p>1:
<img src=https://hackmd.io/_uploads/BkaTOg77A.png alt=UMAP1>
2:
<img src=https://hackmd.io/_uploads/rJ66_xm7C.png alt=UMAP2></p><h2 id=insights>Insights
<a class=anchor href=#insights>#</a></h2><p>The visualizations above show that the dimensionality reduction methods used did a reasonably good job with clustering the different labels of data. Note that the model is trained on a different dataset, and has never seen the data we tried it, which explain the limiatation and</p><h2 id=extra-insights>Extra Insights
<a class=anchor href=#extra-insights>#</a></h2><p>Let&rsquo;s take a look at this 2D visualization from UMAP:</p><p><img src=https://hackmd.io/_uploads/ryWD9P7QR.jpg alt=telegram-cloud-photo-size-4-5922267642553550056-y></p><ul><li>We zoomed in to the cluster highlighted in red, and found out that altough it contains sentences that belong to all labels, all of them convey some sort of regret/guilt.</li></ul><p><img src=https://hackmd.io/_uploads/Hk25Xv7XC.png alt=image></p><ul><li><p>We took another look at the cluster highlighted in yellow, and saw that all the sentences there talk about humiliation/disgrace.
<img src=https://hackmd.io/_uploads/r1pTwD7XC.png alt=image></p></li><li><p>As for the cluster highlighted in black, all the sentences conveyed meaning for the purpose of thanking and appreciation.
<img src=https://hackmd.io/_uploads/Hyw39PXQ0.png alt=image></p></li></ul><h2 id=applications>Applications
<a class=anchor href=#applications>#</a></h2><ul><li><strong>Model Debugging and Improvement</strong>: Identifies anomalies or biases in embeddings, facilitating targeted improvements to the model.</li><li><strong>Semantic Analysis</strong>: Assists in understanding how sentences are clustered semantically, which helps in tasks such as sentiment analysis.</li><li><strong>Transfer Learning Insights</strong>: Offers insights into the transferability of learned features, especially in domain-specific applications.</li><li><strong>Multilingual Comparisons</strong>: Evaluates the model’s capability across different languages, which helps identify potential biases or gaps.</li><li><strong>Explainability and Trust</strong>: Increases the transparency of NLP systems, and builds trust among end-users and regulators by making complex models more interpretable.</li></ul><h2 id=colab-notebook>Colab Notebook
<a class=anchor href=#colab-notebook>#</a></h2><p>The source code for this project can be found in this
<a href="https://colab.research.google.com/drive/1rs08XMn38GFz2bcOKXdCJJUKrh8rgNHF#scrollTo=cSbyb4wbtCZn">Colab Notebook</a>.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/IU-PR/Capstone_project/tree/master//content/docs/Groups/Dimensionality%20Reduction%20in%20NLP_%20Visualizing%20Sentence%20Embeddings%20with%20UMAP%20and%20t-SNE.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#dimensionality-reduction-in-nlp-visualizing-sentence-embeddings-with-umap-and-t-sne>Dimensionality Reduction in NLP: Visualizing Sentence Embeddings with UMAP and t-SNE</a><ul><li><a href=#students>Students:</a></li><li><a href=#introduction>Introduction</a></li><li><a href=#methodology>Methodology</a><ul><li><a href=#umap>UMAP</a></li><li><a href=#t-sne>t-SNE</a></li><li><a href=#umap-vs-t-sne>UMAP vs t-SNE</a></li></ul></li><li><a href=#extraction-of-embeddings>Extraction of Embeddings</a></li><li><a href=#visualization>Visualization</a><ul><li><a href=#t-sne-1>t-SNE</a></li><li><a href=#umap-1>UMAP</a></li></ul></li><li><a href=#insights>Insights</a></li><li><a href=#extra-insights>Extra Insights</a></li><li><a href=#applications>Applications</a></li><li><a href=#colab-notebook>Colab Notebook</a></li></ul></li></ul></nav></div></aside></main></body></html>