<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Building Explainable Hiring Models with CatBoost and SHAP # Authors: Amir Nigmatullin ( am.nigmatullin@innopolis.university) and Nurislam Zinnatullin ( n.zinnatullin@innopolis.university)
1. Introduction # Explainable Artificial Intelligence (XAI) has become a fundamental area of AI research, striving to improve the transparency and interpretability of machine learning models. Understanding how AI systems make decisions is crucial for fostering trust, ensuring accountability, and mitigating potential risks.
In this post, we focus on the application of AI in hiring decisions, where biased models can lead to significant legal and ethical challenges, as well as financial losses in the future."><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:title" content><meta property="og:description" content="Building Explainable Hiring Models with CatBoost and SHAP # Authors: Amir Nigmatullin ( am.nigmatullin@innopolis.university) and Nurislam Zinnatullin ( n.zinnatullin@innopolis.university)
1. Introduction # Explainable Artificial Intelligence (XAI) has become a fundamental area of AI research, striving to improve the transparency and interpretability of machine learning models. Understanding how AI systems make decisions is crucial for fostering trust, ensuring accountability, and mitigating potential risks.
In this post, we focus on the application of AI in hiring decisions, where biased models can lead to significant legal and ethical challenges, as well as financial losses in the future."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/docs/groups/explainable-hiring-models-with-catboost-and-shap/"><meta property="article:section" content="docs"><title>Explainable Hiring Models With Cat Boost and Shap | XAI</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.e832d4e94212199857473bcf13a450d089c3fcd54ccadedcfac84ed0feff83fb.css integrity="sha256-6DLU6UISGZhXRzvPE6RQ0InD/NVMyt7c+shO0P7/g/s=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/mathtex-script-type.min.js integrity=sha384-jiBVvJ8NGGj5n7kJaiWwWp9AjC+Yh8rhZY3GtAX8yU28azcLgoRo4oukO87g7zDT crossorigin=anonymous></script><script defer src=/flexsearch.min.js></script><script defer src=/en.search.min.45f09ee060aa482d344c82ae8f28314f0f9d7a6d41590df9a613b88bd43b9446.js integrity="sha256-RfCe4GCqSC00TIKujygxTw+dem1BWQ35phO4i9Q7lEY=" crossorigin=anonymous></script><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/YELLOW_BAR.png alt=Logo><span><b>XAI</b></span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=/docs/groups/cam_and_secam/>CAM and SeCAM</a></li><li><a href=/docs/groups/diffusion-lens-interpreting-text-encoders-in-text-to-image-pipelines-tuned-using-dreambooth/>Diffusion Lens: Interpreting Text Encoders in Text-to-Image pipelines</a></li><li><a href=/docs/groups/dimensionality-reduction-in-nlp-visualizing-sentence-embeddings-with-umap-and-t-sne/>Dimensionality Reduction in NLP: Visualizing Sentence Embeddings with UMAP and t-SNE</a></li><li><a href=/docs/groups/example/>Example</a></li><li><a href=/docs/groups/ai-playing-geoguessr-explained/>Ai Playing Geo Guessr Explained</a></li><li><a href=/docs/groups/contrastive-grad-cam-consistency/>Contrastive Grad Cam Consistency</a></li><li><a href=/docs/groups/dndfs_shap/>Dndfs Shap</a></li><li><a href=/docs/groups/explainable-hiring-models-with-catboost-and-shap/ class=active>Explainable Hiring Models With Cat Boost and Shap</a></li><li><a href=/docs/groups/gradcam/>Grad Cam</a></li><li><a href=/docs/groups/integrated-gradients/>Integrated Gradients</a></li><li><a href=/docs/groups/kernel-shap/>Kernel Shap</a></li><li><a href=/docs/groups/rag/>Rag</a></li><li><a href=/docs/groups/shap_darya_and_viktoria/>Shap Darya and Viktoria</a></li><li><a href=/docs/groups/sverl_tac_toe/>Sverl Tac Toe</a></li><li><a href=/docs/groups/torchprism/>Torch Prism</a></li><li><a href=/docs/groups/xai_for_transformers/>Xai for Transformers</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Explainable Hiring Models With Cat Boost and Shap</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#building-explainable-hiring-models-with-catboost-and-shap><strong>Building Explainable Hiring Models with CatBoost and SHAP</strong></a><ul><li><a href=#1-introduction><strong>1. Introduction</strong></a></li><li><a href=#2-catboost-architecture-and-advantages><strong>2. CatBoost: Architecture and Advantages</strong></a></li><li><a href=#3-understanding-shap-and-shapley-values><strong>3. Understanding SHAP and Shapley Values</strong></a><ul><li><a href=#shapley-values-from-game-theory><strong>Shapley Values from Game Theory</strong></a></li><li><a href=#shap-in-machine-learning><strong>SHAP in Machine Learning</strong></a></li></ul></li><li><a href=#4-efficient-approximation-of-shapley-values><strong>4. Efficient Approximation of Shapley Values</strong></a><ul><li><a href=#kernel-shap-kernelexplainer><strong>Kernel SHAP (KernelExplainer)</strong></a></li><li><a href=#sampling-based-approximation-samplingexplainer><strong>Sampling-Based Approximation (SamplingExplainer)</strong></a></li><li><a href=#sampling-strategy><strong>Sampling strategy</strong></a></li><li><a href=#tradeoffs-and-practical-considerations><strong>Tradeoffs and Practical Considerations</strong></a></li></ul></li><li><a href=#5-our-recruitment-model-case-study><strong>5. Our Recruitment Model: Case Study</strong></a><ul><li><a href=#model-performance-metrics><strong>Model Performance Metrics</strong></a></li><li><a href=#feature-analysis><strong>Feature Analysis</strong></a></li><li><a href=#beeswarm-plot><strong>Beeswarm plot</strong>:</a></li><li><a href=#bar-plot-global><strong>Bar plot (Global)</strong>:</a></li><li><a href=#bar-plot-local><strong>Bar plot (Local)</strong>:</a></li><li><a href=#dependence-plot><strong>Dependence plot</strong>:</a></li><li><a href=#waterfall-plot><strong>Waterfall plot</strong>:</a></li><li><a href=#built-in-feature-importance-by-catboost><strong>Built-in Feature importance by CatBoost</strong></a></li><li><a href=#why-not-use-only-catboost-feature-importance><strong>Why not use only CatBoost feature importance?</strong></a></li></ul></li><li><a href=#6-key-insights-and-recommendations><strong>6. Key Insights and Recommendations</strong></a><ul><li><a href=#insights><strong>Insights</strong></a></li><li><a href=#recommendations-for-business><strong>Recommendations for Business</strong></a></li></ul></li><li><a href=#7-conclusion><strong>7. Conclusion</strong></a></li><li><a href=#8-references><strong>8. References</strong></a></li></ul></li></ul></nav></aside></header><article class=markdown><h1 id=building-explainable-hiring-models-with-catboost-and-shap><strong>Building Explainable Hiring Models with CatBoost and SHAP</strong>
<a class=anchor href=#building-explainable-hiring-models-with-catboost-and-shap>#</a></h1><p><em>Authors: Amir Nigmatullin (
<a href=mailto:am.nigmatullin@innopolis.university>am.nigmatullin@innopolis.university</a>) and Nurislam Zinnatullin (
<a href=mailto:n.zinnatullin@innopolis.university>n.zinnatullin@innopolis.university</a>)</em></p><hr><h2 id=1-introduction><strong>1. Introduction</strong>
<a class=anchor href=#1-introduction>#</a></h2><p>Explainable Artificial Intelligence (XAI) has become a fundamental area of AI research, striving to improve the transparency and interpretability of machine learning models. Understanding how AI systems make decisions is crucial for fostering trust, ensuring accountability, and mitigating potential risks.</p><p>In this post, we focus on the application of AI in hiring decisions, where biased models can lead to significant legal and ethical challenges, as well as financial losses in the future. By leveraging the expertise of HR specialists, we can train a model that aligns with their decision-making processes. To enhance interpretability, we propose using SHAP (Shapley Additive Explanations) to estimate the factors influencing predictions. Specifically, we will explore the use of the CatBoost classifier to ensure accurate and explainable hiring decisions</p><hr><h2 id=2-catboost-architecture-and-advantages><strong>2. CatBoost: Architecture and Advantages</strong>
<a class=anchor href=#2-catboost-architecture-and-advantages>#</a></h2><p>CatBoost, a state-of-the-art algorithm developed by Yandex, is a powerful solution for efficient and accurate machine learning tasks, including classification and regression. With its innovative Ordered Boosting technique, CatBoost enhances predictive performance by leveraging decision trees effectively.</p><p><img src=/Explainable%20Hiring%20Models%20with%20CatBoost%20and%20SHAP/catboost_schema.png alt="CatBoost schema"></p><p>A major advantage of CatBoost is its efficient handling of categorical features. It utilizes a unique approach known as &ldquo;ordered boosting,&rdquo; which enables the model to process categorical data directly. This method enhances training speed and boosts model accuracy by encoding categorical variables while maintaining their inherent order.</p><p><img src=/Explainable%20Hiring%20Models%20with%20CatBoost%20and%20SHAP/catboost_schema_1.png alt="CatBoost schema"></p><p>Our dataset mainly consisted of categorical or near-categorical features (Age, Gender, EducationLevel, ExperienceYears, InterviewScore, SkillScore, PersonalityScore, RecruitmentStrategy), therefore we decided to choose CatBoost as a recruitment system model.</p><hr><h2 id=3-understanding-shap-and-shapley-values><strong>3. Understanding SHAP and Shapley Values</strong>
<a class=anchor href=#3-understanding-shap-and-shapley-values>#</a></h2><p>SHAP is a state-of-the-art method that explains model predictions by quantifying the contribution of each feature. Its foundation lies in Shapley values from cooperative game theory.</p><h3 id=shapley-values-from-game-theory><strong>Shapley Values from Game Theory</strong>
<a class=anchor href=#shapley-values-from-game-theory>#</a></h3><p>Originally proposed by Lloyd Shapley in 1953, Shapley values were designed to fairly distribute a total payoff among players in a cooperative game. In our context:</p><ul><li><strong>Players:</strong> The features of the model.</li><li><strong>Payoff:</strong> The prediction outcome.</li></ul><p>The Shapley value for a feature <em>i</em> is calculated as:</p><link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\[\phi_i = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|! (|N| - |S| - 1)!}{|N|!} \left[ f(S \cup \{i\}) - f(S) \right]\]</span><p>Where:</p><ul><li><em>N</em> is the set of all features.</li><li><em>S</em> is any subset excluding feature <em>i</em>.</li><li><em>f(S)</em> is the model prediction using subset <em>S</em>.</li><li>The factorial terms account for all possible inclusion orders.</li></ul><p><strong>A Simple Example</strong></p><p>Suppose your model uses 3 features: Age, Income, and Education.
To compute the Shapley value for &ldquo;Income&rdquo;, we look at every possible way to add &ldquo;Income&rdquo; into different combinations of the other features and see how much the prediction changes when we do that.</p><p>We repeat this for all combinations and average the effect.
This way, even if &ldquo;Income&rdquo; interacts with &ldquo;Education&rdquo;, its impact is fairly shared.</p><h3 id=shap-in-machine-learning><strong>SHAP in Machine Learning</strong>
<a class=anchor href=#shap-in-machine-learning>#</a></h3><p>SHAP leverages these values to provide explanations for individual predictions as well as overall model behavior. Its key properties include:</p><ul><li><strong>Additivity:</strong><br>Model predictions can be decomposed as a sum of a baseline value <span>\(\phi_0\)
</span>and individual SHAP values <span>\(\phi_i\)
</span>:</li></ul><span>\[f(x) = \phi_0 + \sum_{i=1}^{n} \phi_i\]</span><p>For example, in ensemble models like random forests Additivity property guarantees that for a feature value, you can calculate the Shapley value for each tree individually, average them, and get the Shapley value for the feature value for the random forest.</p><ul><li><p><strong>Fairness:</strong><br>Features with similar contributions receive similar SHAP values.</p></li><li><p><strong>Zero Importance:</strong><br>Features that do not affect the prediction have SHAP values close to zero.</p></li></ul><hr><h2 id=4-efficient-approximation-of-shapley-values><strong>4. Efficient Approximation of Shapley Values</strong>
<a class=anchor href=#4-efficient-approximation-of-shapley-values>#</a></h2><p>Calculating exact Shapley values is computationally expensive due to the need to evaluate all possible subsets of features (exponential in number). To overcome this, approximation methods are used.</p><h3 id=kernel-shap-kernelexplainer><strong>Kernel SHAP (KernelExplainer)</strong>
<a class=anchor href=#kernel-shap-kernelexplainer>#</a></h3><p>Kernel SHAP approximates Shapley values via weighted linear regression on a subset of feature coalitions.</p><p><strong>How It Works:</strong></p><ul><li><strong>Coalition Sampling:</strong><br>Instead of evaluating all (2^M) subsets (<strong><em>M</em></strong> is number of features), it samples a limited number (using a parameter like <code>max_samples</code>) with weights determined by the Shapley kernel:</li></ul><span>\[w(S) = \frac{M-1}{\binom{M}{|S|} |S| (M-|S|)}\]</span><p>This prioritizes small (1-2 features) and large (M-1 to M-2 features) coalitions, which carry the highest information value.</p><ul><li><p><strong>Background Data Integration:</strong><br>For each coalition, creates masked instances by blending the target observation&rsquo;s features with <code>background_data</code> (typically 10-100 samples). Predictions on these hybrid instances approximate <strong><em>f(S ⋃ i)</em></strong> and <strong><em>f(S)</em></strong>.</p></li><li><p><strong>Regression:</strong><br>Uses weighted linear regression to solve for Shapley values that best explain the prediction differences:</p></li></ul><span>\[\min_{\phi} \sum_{S} w(S) [f(S) - (\phi_0 + \sum_{i∈S}\phi_i)]²\]</span><p>This reduces the complexity from <strong><em>O(2^M)</em></strong> to <strong><em>O(S · P + M^3)</em></strong>, where</p><ul><li><strong><em>M</em></strong> is number of features</li><li><strong><em>S</em></strong> is number of sampled coalitions in Kernel SHAP</li><li><strong><em>B</em></strong> is number of background samples in SamplingExplainer</li><li><strong><em>P</em></strong> is cost of a single model prediction (e.g., one forward pass)</li><li><strong>Model evaluations: <em>O(S⋅P)</em></strong> because we sample <strong><em>S</em></strong> coalitions and evaluate the model once per coalition</li><li><strong>Regression fit: <em>O((M+1)^3)</em></strong> to solve the weighted linear system for <strong><em>M</em></strong> feature weights plus the intercept.</li></ul><h4 id=pseudo-code><strong>Pseudo-code</strong>
<a class=anchor href=#pseudo-code>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Input:
</span></span><span style=display:flex><span>  f         := trained model
</span></span><span style=display:flex><span>  x         := target instance (length-M vector)
</span></span><span style=display:flex><span>  D         := background dataset {z₁…z_B}
</span></span><span style=display:flex><span>  S         := number of coalitions to sample
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Output:
</span></span><span style=display:flex><span>  φ₁…φ_M    := SHAP values (no φ₀; can be reconstructed from expected value)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>1.  Sample background indices D′ ⊆ D
</span></span><span style=display:flex><span>2.  Generate S coalitions C₁…C_S where each C_j ⊆ {1…M}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>3.  For each j = 1…S:
</span></span><span style=display:flex><span>4.    Compute kernel weight:
</span></span><span style=display:flex><span>        w_j ← (M−1) / (|C_j| · (M−|C_j|))    ⟶ Shapley kernel
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>5.    For each z ∈ D′:
</span></span><span style=display:flex><span>6.      Create masked sample x^{(j)}_z:
</span></span><span style=display:flex><span>         [x^{(j)}_z]_i = x_i   if i ∈ C_j
</span></span><span style=display:flex><span>                        z_i   otherwise
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>7.    Compute y_j ← (1/B) * Σ_{z∈D′} f(x^{(j)}_z)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>8.  End for
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>9.  Use weighted linear regression:
</span></span><span style=display:flex><span>       Fit φ minimizing Σ_j w_j · (y_j − (Σ_{i∈C_j} φ_i))²
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>10. Return φ₁…φ_M
</span></span></code></pre></div><h3 id=sampling-based-approximation-samplingexplainer><strong>Sampling-Based Approximation (SamplingExplainer)</strong>
<a class=anchor href=#sampling-based-approximation-samplingexplainer>#</a></h3><p>SamplingExplainer uses <em>feature perturbation</em> and <em>averaging over background data</em> to estimate feature contributions more quickly.</p><p><strong>Key Steps:</strong></p><ol><li><p><strong>Baseline Prediction:</strong><br>Compute average predictions using background data.</p></li><li><p><strong>Perturbation Analysis:</strong><br>For each feature in the target instance:</p><ul><li>Replace the feature&rsquo;s value in all background samples with the instance&rsquo;s value</li><li>Compute prediction deltas in log-odds space</li><li>Average deltas across background samples as the feature&rsquo;s contribution</li></ul></li><li><p><strong>Additivity Enforcement:</strong><br>Rescales contributions to ensure:</p><span>\[ \sum\phi_i = f(x) - E[f]
\]</span></li></ol><p>This preserves SHAP&rsquo;s local accuracy guarantee.</p><p>Complexity scales as <strong><em>O(B·M·P)</em></strong> where <strong><em>B</em></strong> is background samples (typically 100-1000) and others you can see in kernel SHAP section.</p><h4 id=pseudo-code-1><strong>Pseudo-code</strong>
<a class=anchor href=#pseudo-code-1>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Input:
</span></span><span style=display:flex><span>  f       := trained model
</span></span><span style=display:flex><span>  x       := target instance (length-M vector)
</span></span><span style=display:flex><span>  D       := background dataset {z₁…z_B}
</span></span><span style=display:flex><span>  L       := background predictions in log-odds space
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Output:
</span></span><span style=display:flex><span>  φ₀…φ_M  := SHAP values (including intercept φ₀)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>1.  Compute log-odds baseline:
</span></span><span style=display:flex><span>      φ₀ ← (1/B) * Σ_{z∈D} logit(f(z))       ⟶ baseline in log-odds space
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>2.  Compute log-odds of f(x):
</span></span><span style=display:flex><span>      logit_f_x ← logit(f(x))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>3.  For each feature i = 1…M:
</span></span><span style=display:flex><span>4.    For each background point z ∈ D:
</span></span><span style=display:flex><span>5.      Create z^{(i)} where
</span></span><span style=display:flex><span>         [z^{(i)}]_i = x_i
</span></span><span style=display:flex><span>         [z^{(i)}]_k = z_k    for k ≠ i
</span></span><span style=display:flex><span>6.      Compute δ_{i,z} ← logit(f(z^{(i)})) − logit(f(z))
</span></span><span style=display:flex><span>7.    End for
</span></span><span style=display:flex><span>8.    φ_i ← (1/B) * Σ_{z∈D} δ_{i,z}
</span></span><span style=display:flex><span>9.  End for
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>10. Enforce additivity:
</span></span><span style=display:flex><span>      total ← Σ_{i=1}^M φ_i
</span></span><span style=display:flex><span>      adjust ← logit_f_x − φ₀ − total
</span></span><span style=display:flex><span>      Distribute `adjust` proportionally or add to φ₁
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>11. Return φ₀…φ_M
</span></span></code></pre></div><h3 id=sampling-strategy><strong>Sampling strategy</strong>
<a class=anchor href=#sampling-strategy>#</a></h3><p>A sampling strategy is simply a way to pick a manageable number of examples instead of checking every possible case. In our SHAP implementation, we used two such strategies:</p><ul><li><p>Kernel SHAP: we randomly choose groups of features—giving priority to very small and very large groups—to see how including them changes the model’s output.</p></li><li><p>SamplingExplainer: we randomly draw past data points and swap in one feature at a time from the instance we’re explaining, measuring how each swap shifts the prediction.</p></li></ul><p>These methods let us approximate each feature’s contribution quickly and accurately without testing all possible feature combinations.</p><h3 id=tradeoffs-and-practical-considerations><strong>Tradeoffs and Practical Considerations</strong>
<a class=anchor href=#tradeoffs-and-practical-considerations>#</a></h3><table><thead><tr><th><strong>Method</strong></th><th><strong>Strength</strong></th><th><strong>Limitation</strong></th></tr></thead><tbody><tr><td><strong>KernelExplainer</strong></td><td>Theoretically robust, model-agnostic</td><td>Slower for high <strong><em>M</em></strong>, sensitive to <strong><em>S</em></strong></td></tr><tr><td><strong>SamplingExplainer</strong></td><td>Very simple, scales linearly in <strong><em>M</em></strong></td><td>Assumes feature independence</td></tr></tbody></table><hr><h2 id=5-our-recruitment-model-case-study><strong>5. Our Recruitment Model: Case Study</strong>
<a class=anchor href=#5-our-recruitment-model-case-study>#</a></h2><p>We applied our approach to build an autonomous hiring system using historical HR data. Our model is based on CatBoost, chosen for its excellence in handling categorical data.</p><h3 id=model-performance-metrics><strong>Model Performance Metrics</strong>
<a class=anchor href=#model-performance-metrics>#</a></h3><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>Accuracy:
</span></span><span style=display:flex><span> 0.9566666666666667 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ROC AUC Score:
</span></span><span style=display:flex><span> 0.941313269493844 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Classification Report:
</span></span><span style=display:flex><span>               precision    recall  f1-score   support
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>           0       0.96      0.98      0.97       215
</span></span><span style=display:flex><span>           1       0.94      0.91      0.92        85
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    accuracy                           0.96       300
</span></span><span style=display:flex><span>   macro avg       0.95      0.94      0.95       300
</span></span><span style=display:flex><span>weighted avg       0.96      0.96      0.96       300
</span></span></code></pre></div><h3 id=feature-analysis><strong>Feature Analysis</strong>
<a class=anchor href=#feature-analysis>#</a></h3><p>While our EDA we found key predictors that were highly correlated with hiring decisions:</p><p><img src=/Explainable%20Hiring%20Models%20with%20CatBoost%20and%20SHAP/correlation_matrix.png alt="Correlation Matrix"></p><p>RecruitmentStrategy, EducationLevel, SkillScore, PersonalityScore, and ExperienceYears</p><h3 id=beeswarm-plot><strong>Beeswarm plot</strong>:
<a class=anchor href=#beeswarm-plot>#</a></h3><p>A SHAP beeswarm plot visualizes the distribution and impact of each feature&rsquo;s SHAP values across all predictions, showing feature importance (vertical order) and effect direction (red/blue for positive/negative influence). It helps identify key drivers of model behavior while revealing nonlinear patterns and outliers in feature contributions.</p><p><img src=/Explainable%20Hiring%20Models%20with%20CatBoost%20and%20SHAP/beeswarm_plot.png alt="Beeswarm Plot"></p><h3 id=bar-plot-global><strong>Bar plot (Global)</strong>:
<a class=anchor href=#bar-plot-global>#</a></h3><p>The SHAP bar plot ranks features by their average impact (mean absolute SHAP values), showing which features most influence the model&rsquo;s predictions across all data.</p><p><img src=/Explainable%20Hiring%20Models%20with%20CatBoost%20and%20SHAP/bar_plot_global.png alt="Bar Plot"></p><h3 id=bar-plot-local><strong>Bar plot (Local)</strong>:
<a class=anchor href=#bar-plot-local>#</a></h3><p>For a single prediction, it displays each feature&rsquo;s exact contribution (positive/negative SHAP value), explaining how they pushed the prediction higher or lower than the baseline. This can help HR specialists understand why a specific candidate was recommended or not.</p><p><img src=/Explainable%20Hiring%20Models%20with%20CatBoost%20and%20SHAP/bar_plot_local.png alt="Bar Plot"></p><h3 id=dependence-plot><strong>Dependence plot</strong>:
<a class=anchor href=#dependence-plot>#</a></h3><p>The dependence plot shows how the impact of feature varies across different values, revealing non-linear relationships not captured by simple correlation</p><p><img src=/Explainable%20Hiring%20Models%20with%20CatBoost%20and%20SHAP/dependence_plot.png alt="Depence Plot"></p><h3 id=waterfall-plot><strong>Waterfall plot</strong>:
<a class=anchor href=#waterfall-plot>#</a></h3><p>The waterfall plot for a specific candidate shows how each feature contributed to their final prediction, providing transparency for individual decisions</p><p><img src=/Explainable%20Hiring%20Models%20with%20CatBoost%20and%20SHAP/waterfall_plot.png alt="Waterfall Plot"></p><h3 id=built-in-feature-importance-by-catboost><strong>Built-in Feature importance by CatBoost</strong>
<a class=anchor href=#built-in-feature-importance-by-catboost>#</a></h3><p>Visual plots from both SHAP approaches and the built-in CatBoost feature importance revealed consistent insights, aligning well with our initial exploratory data analysis.</p><p><img src=/Explainable%20Hiring%20Models%20with%20CatBoost%20and%20SHAP/catboost_features.png alt="CatBoost feature dependency"></p><h3 id=why-not-use-only-catboost-feature-importance><strong>Why not use only CatBoost feature importance?</strong>
<a class=anchor href=#why-not-use-only-catboost-feature-importance>#</a></h3><p>It shows average magnitude of feature impact across the dataset</p><p>SHAP values provide detailed feature contributions for each prediction, showing both magnitude and direction. So it can interpret not only global features importance, but also individual instance interpretation.</p><hr><h2 id=6-key-insights-and-recommendations><strong>6. Key Insights and Recommendations</strong>
<a class=anchor href=#6-key-insights-and-recommendations>#</a></h2><h3 id=insights><strong>Insights</strong>
<a class=anchor href=#insights>#</a></h3><ul><li><p><strong>Strategic Hiring:</strong><br>Aggressive recruitment strategies can significantly influence hiring outcomes.</p></li><li><p><strong>Candidate Evaluation:</strong><br>Educational background and technical skills are strong predictors.</p></li><li><p><strong>Fairness:</strong><br>The model shows minimal bias based on demographic features like age and gender.</p></li><li><p><strong>Transparency:</strong><br>XAI techniques such as SHAP enhance trust by explaining the model’s predictions both globally and locally.</p></li></ul><h3 id=recommendations-for-business><strong>Recommendations for Business</strong>
<a class=anchor href=#recommendations-for-business>#</a></h3><ol><li><strong>Adjust Recruitment Strategies:</strong><br>Align strategies with market conditions and organizational needs.</li><li><strong>Focus on Predictive Factors:</strong><br>Prioritize education and skills in candidate evaluation.</li><li><strong>Continuous Monitoring:</strong><br>Regularly audit the model using SHAP to maintain fairness and accuracy.</li><li><strong>Embrace Interpretability:</strong><br>Incorporate explainability into all high-stakes decision-making systems to ensure transparency and accountability.</li></ol><hr><h2 id=7-conclusion><strong>7. Conclusion</strong>
<a class=anchor href=#7-conclusion>#</a></h2><p>By integrating CatBoost and SHAP, we not only achieve high model performance but also offer deep insights into the decision-making process. This dual focus on accuracy and interpretability supports fair and informed HR practices, making it a powerful approach for modern recruitment systems.</p><h2 id=8-references><strong>8. References</strong>
<a class=anchor href=#8-references>#</a></h2><p><a href=https://www.kaggle.com/datasets/rabieelkharoua/predicting-hiring-decisions-in-recruitment-data>Dataset that we used to train the hiring model</a></p><p><a href=https://papers.nips.cc/paper_files/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html>Original article about SHAP values</a></p><p><a href=https://github.com/shap/shap>Original shap implementation (library)</a></p><p><a href=https://github.com/Zaurall/XAI>Our implementation</a></p><p><a href=https://catboost.ai/docs/en/>CatBoost documentation</a></p></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/IU-PR/Capstone_project/tree/master//content/docs/Groups/Explainable%20Hiring%20Models%20with%20CatBoost%20and%20SHAP.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#building-explainable-hiring-models-with-catboost-and-shap><strong>Building Explainable Hiring Models with CatBoost and SHAP</strong></a><ul><li><a href=#1-introduction><strong>1. Introduction</strong></a></li><li><a href=#2-catboost-architecture-and-advantages><strong>2. CatBoost: Architecture and Advantages</strong></a></li><li><a href=#3-understanding-shap-and-shapley-values><strong>3. Understanding SHAP and Shapley Values</strong></a><ul><li><a href=#shapley-values-from-game-theory><strong>Shapley Values from Game Theory</strong></a></li><li><a href=#shap-in-machine-learning><strong>SHAP in Machine Learning</strong></a></li></ul></li><li><a href=#4-efficient-approximation-of-shapley-values><strong>4. Efficient Approximation of Shapley Values</strong></a><ul><li><a href=#kernel-shap-kernelexplainer><strong>Kernel SHAP (KernelExplainer)</strong></a></li><li><a href=#sampling-based-approximation-samplingexplainer><strong>Sampling-Based Approximation (SamplingExplainer)</strong></a></li><li><a href=#sampling-strategy><strong>Sampling strategy</strong></a></li><li><a href=#tradeoffs-and-practical-considerations><strong>Tradeoffs and Practical Considerations</strong></a></li></ul></li><li><a href=#5-our-recruitment-model-case-study><strong>5. Our Recruitment Model: Case Study</strong></a><ul><li><a href=#model-performance-metrics><strong>Model Performance Metrics</strong></a></li><li><a href=#feature-analysis><strong>Feature Analysis</strong></a></li><li><a href=#beeswarm-plot><strong>Beeswarm plot</strong>:</a></li><li><a href=#bar-plot-global><strong>Bar plot (Global)</strong>:</a></li><li><a href=#bar-plot-local><strong>Bar plot (Local)</strong>:</a></li><li><a href=#dependence-plot><strong>Dependence plot</strong>:</a></li><li><a href=#waterfall-plot><strong>Waterfall plot</strong>:</a></li><li><a href=#built-in-feature-importance-by-catboost><strong>Built-in Feature importance by CatBoost</strong></a></li><li><a href=#why-not-use-only-catboost-feature-importance><strong>Why not use only CatBoost feature importance?</strong></a></li></ul></li><li><a href=#6-key-insights-and-recommendations><strong>6. Key Insights and Recommendations</strong></a><ul><li><a href=#insights><strong>Insights</strong></a></li><li><a href=#recommendations-for-business><strong>Recommendations for Business</strong></a></li></ul></li><li><a href=#7-conclusion><strong>7. Conclusion</strong></a></li><li><a href=#8-references><strong>8. References</strong></a></li></ul></li></ul></nav></div></aside></main></body></html>