<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Explaining CNN Predictions for Skin Cancer Detection Using SHAP and LIME # Abstract # In this work, a deep learning model is trained for skin cancer detection using the HAM10000 dataset, and two popular explainability methods, SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations), are applied to interpret the model&rsquo;s predictions. The SHAP and LIME methods provide visual explanations by identifying key features (superpixels) that influence the model’s decision-making."><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:title" content><meta property="og:description" content="Explaining CNN Predictions for Skin Cancer Detection Using SHAP and LIME # Abstract # In this work, a deep learning model is trained for skin cancer detection using the HAM10000 dataset, and two popular explainability methods, SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations), are applied to interpret the model&rsquo;s predictions. The SHAP and LIME methods provide visual explanations by identifying key features (superpixels) that influence the model’s decision-making."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/docs/groups/skin_cancer_detection_with_shap_and_lime/"><meta property="article:section" content="docs"><title>Skin Cancer Detection With Shap and Lime | XAI</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.e832d4e94212199857473bcf13a450d089c3fcd54ccadedcfac84ed0feff83fb.css integrity="sha256-6DLU6UISGZhXRzvPE6RQ0InD/NVMyt7c+shO0P7/g/s=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/mathtex-script-type.min.js integrity=sha384-jiBVvJ8NGGj5n7kJaiWwWp9AjC+Yh8rhZY3GtAX8yU28azcLgoRo4oukO87g7zDT crossorigin=anonymous></script><script defer src=/flexsearch.min.js></script><script defer src=/en.search.min.c0443cd480d4dcc349b80df30205c81d8a9a86ac4983330e78f50f163c4427b0.js integrity="sha256-wEQ81IDU3MNJuA3zAgXIHYqahqxJgzMOePUPFjxEJ7A=" crossorigin=anonymous></script><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/YELLOW_BAR.png alt=Logo><span><b>XAI</b></span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=/docs/groups/cam_and_secam/>CAM and SeCAM</a></li><li><a href=/docs/groups/diffusion-lens-interpreting-text-encoders-in-text-to-image-pipelines-tuned-using-dreambooth/>Diffusion Lens: Interpreting Text Encoders in Text-to-Image pipelines</a></li><li><a href=/docs/groups/dimensionality-reduction-in-nlp-visualizing-sentence-embeddings-with-umap-and-t-sne/>Dimensionality Reduction in NLP: Visualizing Sentence Embeddings with UMAP and t-SNE</a></li><li><a href=/docs/groups/example/>Example</a></li><li><a href=/docs/groups/ai-playing-geoguessr-explained/>Ai Playing Geo Guessr Explained</a></li><li><a href=/docs/groups/contrastive-grad-cam-consistency/>Contrastive Grad Cam Consistency</a></li><li><a href=/docs/groups/dndfs_shap/>Dndfs Shap</a></li><li><a href=/docs/groups/gradcam/>Grad Cam</a></li><li><a href=/docs/groups/integrated-gradients/>Integrated Gradients</a></li><li><a href=/docs/groups/kernel-shap/>Kernel Shap</a></li><li><a href=/docs/groups/rag/>Rag</a></li><li><a href=/docs/groups/shap_darya_and_viktoria/>Shap Darya and Viktoria</a></li><li><a href=/docs/groups/skin_cancer_detection_with_shap_and_lime/ class=active>Skin Cancer Detection With Shap and Lime</a></li><li><a href=/docs/groups/sverl_tac_toe/>Sverl Tac Toe</a></li><li><a href=/docs/groups/torchprism/>Torch Prism</a></li><li><a href=/docs/groups/xai_for_transformers/>Xai for Transformers</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Skin Cancer Detection With Shap and Lime</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#explaining-cnn-predictions-for-skin-cancer-detection-using-shap-and-lime>Explaining CNN Predictions for Skin Cancer Detection Using SHAP and LIME</a><ul><li><a href=#abstract>Abstract</a></li><li><a href=#1-introduction>1. Introduction</a></li><li><a href=#2-methodology>2. Methodology</a><ul><li><a href=#21-data-preprocessing>2.1. Data Preprocessing</a></li><li><a href=#22-model-architecture>2.2. Model Architecture</a></li><li><a href=#23-model-interpretability-using-shap-and-lime>2.3. Model Interpretability Using SHAP and LIME</a></li><li><a href=#24-comparison-of-shap-and-lime>2.4. Comparison of SHAP and LIME</a></li></ul></li><li><a href=#3-results>3. Results</a></li><li><a href=#4-conclusion>4. Conclusion</a></li></ul></li></ul></nav></aside></header><article class=markdown><h1 id=explaining-cnn-predictions-for-skin-cancer-detection-using-shap-and-lime>Explaining CNN Predictions for Skin Cancer Detection Using SHAP and LIME
<a class=anchor href=#explaining-cnn-predictions-for-skin-cancer-detection-using-shap-and-lime>#</a></h1><h2 id=abstract>Abstract
<a class=anchor href=#abstract>#</a></h2><p>In this work, a deep learning model is trained for skin cancer detection using the HAM10000 dataset, and two popular explainability methods, <strong>SHAP (SHapley Additive exPlanations)</strong> and <strong>LIME (Local Interpretable Model-agnostic Explanations)</strong>, are applied to interpret the model&rsquo;s predictions. The SHAP and LIME methods provide visual explanations by identifying key features (superpixels) that influence the model’s decision-making. This paper discusses the implementation details of these methods, their comparison, and how they can be applied for model interpretability in sensitive areas like healthcare.</p><h2 id=1-introduction>1. Introduction
<a class=anchor href=#1-introduction>#</a></h2><p>Deep learning models, specifically Convolutional Neural Networks (CNNs), have achieved remarkable success in various medical imaging tasks, including skin cancer detection. However, the lack of transparency in the decision-making process of these models poses a challenge, especially in critical applications such as healthcare. Understanding how a model arrives at its predictions is essential for building trust and ensuring the reliability of AI systems in clinical environments.</p><p>This paper investigates the use of <strong>SHAP</strong> and <strong>LIME</strong>, two widely-used model-agnostic explanation methods, to interpret the predictions of a CNN model trained on the <strong>HAM10000</strong> dataset, which contains dermoscopic images of skin lesions. The SHAP and LIME methods are both capable of providing human-readable explanations, helping to visualize which parts of the image contribute most to the model’s decision.</p><h2 id=2-methodology>2. Methodology
<a class=anchor href=#2-methodology>#</a></h2><h3 id=21-data-preprocessing>2.1. Data Preprocessing
<a class=anchor href=#21-data-preprocessing>#</a></h3><p>The <strong>HAM10000 dataset</strong> consists of dermoscopic images and their corresponding labels, where each image represents a skin lesion and the label indicates whether it is benign or malignant. The dataset is preprocessed as follows:</p><ul><li><p><strong>Loading Metadata</strong>: The metadata file (<code>HAM10000_metadata.csv</code>) is read to extract the image identifiers and their corresponding labels. The labels are then mapped to numeric values for use in model training.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>df <span style=color:#f92672>=</span> pd<span style=color:#f92672>.</span>read_csv(METADATA_CSV)
</span></span><span style=display:flex><span>label_map <span style=color:#f92672>=</span> {label: idx <span style=color:#66d9ef>for</span> idx, label <span style=color:#f92672>in</span> enumerate(df[<span style=color:#e6db74>&#39;dx&#39;</span>]<span style=color:#f92672>.</span>unique())}
</span></span><span style=display:flex><span>df[<span style=color:#e6db74>&#39;label&#39;</span>] <span style=color:#f92672>=</span> df[<span style=color:#e6db74>&#39;dx&#39;</span>]<span style=color:#f92672>.</span>map(label_map)
</span></span></code></pre></div></li><li><p><strong>Loading and Preprocessing Images</strong>: Each image is loaded, resized to a uniform shape of 224x224 pixels, and preprocessed using the <code>preprocess_input</code> function from the EfficientNetB0 model. This preprocessing ensures that the pixel values are normalized according to the expected input format for EfficientNet.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>load_images</span>(df, image_dir, target_size<span style=color:#f92672>=</span>(<span style=color:#ae81ff>224</span>, <span style=color:#ae81ff>224</span>)):
</span></span><span style=display:flex><span>    images, labels <span style=color:#f92672>=</span> [], []
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> _, row <span style=color:#f92672>in</span> df<span style=color:#f92672>.</span>iterrows():
</span></span><span style=display:flex><span>        image_path <span style=color:#f92672>=</span> os<span style=color:#f92672>.</span>path<span style=color:#f92672>.</span>join(image_dir, row[<span style=color:#e6db74>&#39;image_id&#39;</span>] <span style=color:#f92672>+</span> <span style=color:#e6db74>&#39;.jpg&#39;</span>)
</span></span><span style=display:flex><span>        img <span style=color:#f92672>=</span> load_img(image_path, target_size<span style=color:#f92672>=</span>target_size)
</span></span><span style=display:flex><span>        img <span style=color:#f92672>=</span> img_to_array(img)
</span></span><span style=display:flex><span>        img <span style=color:#f92672>=</span> preprocess_input(img)
</span></span><span style=display:flex><span>        images<span style=color:#f92672>.</span>append(img)
</span></span><span style=display:flex><span>        labels<span style=color:#f92672>.</span>append(row[<span style=color:#e6db74>&#39;label&#39;</span>])
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>array(images), np<span style=color:#f92672>.</span>array(labels)
</span></span></code></pre></div></li><li><p><strong>Train-Test Split</strong>: The data is divided into training and testing sets using an 80/20 split.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>X_train, X_test, y_train, y_test <span style=color:#f92672>=</span> train_test_split(images, labels, test_size<span style=color:#f92672>=</span><span style=color:#ae81ff>0.2</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>42</span>)
</span></span></code></pre></div></li></ul><h3 id=22-model-architecture>2.2. Model Architecture
<a class=anchor href=#22-model-architecture>#</a></h3><p>The <strong>EfficientNetB0</strong> model is chosen as the base for this project due to its high accuracy and computational efficiency. EfficientNetB0 is pre-trained on ImageNet, which allows it to leverage knowledge from a large dataset of general images, thereby enhancing performance in the skin cancer classification task.</p><h4 id=221-fine-tuning-efficientnetb0>2.2.1. Fine-Tuning EfficientNetB0
<a class=anchor href=#221-fine-tuning-efficientnetb0>#</a></h4><p>The EfficientNetB0 model is loaded without the top classification layer (<code>include_top=False</code>), and the model’s layers are fine-tuned for the skin cancer detection task.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>base_model <span style=color:#f92672>=</span> EfficientNetB0(include_top<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>, input_shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>224</span>, <span style=color:#ae81ff>224</span>, <span style=color:#ae81ff>3</span>), weights<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;imagenet&#39;</span>)
</span></span></code></pre></div><p>The base model layers are partially frozen to prevent overfitting. Specifically, all layers except for the last 100 are frozen, allowing only the last few layers to be trained.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>for</span> layer <span style=color:#f92672>in</span> base_model<span style=color:#f92672>.</span>layers[:<span style=color:#f92672>-</span><span style=color:#ae81ff>100</span>]:
</span></span><span style=display:flex><span>    layer<span style=color:#f92672>.</span>trainable <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> layer <span style=color:#f92672>in</span> base_model<span style=color:#f92672>.</span>layers[<span style=color:#f92672>-</span><span style=color:#ae81ff>100</span>:]:
</span></span><span style=display:flex><span>    layer<span style=color:#f92672>.</span>trainable <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span></code></pre></div><h4 id=222-adding-a-custom-classifier>2.2.2. Adding a Custom Classifier
<a class=anchor href=#222-adding-a-custom-classifier>#</a></h4><p>A custom classifier is added on top of the EfficientNetB0 base. The classifier consists of a <strong>GlobalAveragePooling2D</strong> layer, <strong>BatchNormalization</strong>, <strong>Dropout</strong>, and a <strong>Dense</strong> output layer with a softmax activation function.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x <span style=color:#f92672>=</span> GlobalAveragePooling2D()(base_model<span style=color:#f92672>.</span>output)
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> BatchNormalization()(x)
</span></span><span style=display:flex><span>x <span style=color:#f92672>=</span> Dropout(<span style=color:#ae81ff>0.3</span>)(x)
</span></span><span style=display:flex><span>output <span style=color:#f92672>=</span> Dense(len(np<span style=color:#f92672>.</span>unique(y_train)), activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;softmax&#39;</span>)(x)
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> Model(inputs<span style=color:#f92672>=</span>base_model<span style=color:#f92672>.</span>input, outputs<span style=color:#f92672>=</span>output)
</span></span></code></pre></div><h4 id=223-model-compilation-and-training>2.2.3. Model Compilation and Training
<a class=anchor href=#223-model-compilation-and-training>#</a></h4><p>The model is compiled with the <strong>Adam optimizer</strong> and <strong>sparse categorical cross-entropy</strong> loss function. Training is carried out with <strong>early stopping</strong> and <strong>model checkpoints</strong> to avoid overfitting and ensure the best model is saved during training.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>model<span style=color:#f92672>.</span>compile(optimizer<span style=color:#f92672>=</span>Adam(<span style=color:#ae81ff>1e-4</span>), loss<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;sparse_categorical_crossentropy&#39;</span>, metrics<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;accuracy&#39;</span>])
</span></span><span style=display:flex><span>history <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>fit(datagen<span style=color:#f92672>.</span>flow(X_train, y_train, batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>32</span>), validation_data<span style=color:#f92672>=</span>(X_test, y_test), epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>25</span>, callbacks<span style=color:#f92672>=</span>[checkpoint, early_stop])
</span></span></code></pre></div><h3 id=23-model-interpretability-using-shap-and-lime>2.3. Model Interpretability Using SHAP and LIME
<a class=anchor href=#23-model-interpretability-using-shap-and-lime>#</a></h3><h4 id=231-shap-shapley-additive-explanations>2.3.1. SHAP: SHapley Additive exPlanations
<a class=anchor href=#231-shap-shapley-additive-explanations>#</a></h4><p>SHAP provides a framework for assigning importance values to each feature (or superpixel in this case) by calculating Shapley values. These values represent the contribution of each feature to the final model prediction.</p><ul><li><strong>SHAP Calculation with Superpixels</strong>: The image is divided into superpixels using the <strong>SLIC</strong> algorithm, and the SHAP values are computed by perturbing these superpixels and measuring the change in the model’s prediction.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>compute_shap_with_superpixels_stable</span>(model, image, background, num_samples<span style=color:#f92672>=</span><span style=color:#ae81ff>500</span>, n_segments<span style=color:#f92672>=</span><span style=color:#ae81ff>120</span>):
</span></span><span style=display:flex><span>    segments <span style=color:#f92672>=</span> slic(image, n_segments<span style=color:#f92672>=</span>n_segments, compactness<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>, sigma<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, start_label<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>    shap_values <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros_like(image)
</span></span><span style=display:flex><span>    original_pred <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict(image[np<span style=color:#f92672>.</span>newaxis, <span style=color:#f92672>...</span>])[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    target_class <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>argmax(original_pred)
</span></span><span style=display:flex><span>    background_mean <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>mean(background, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(num_samples):
</span></span><span style=display:flex><span>        seg_ids <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>unique(segments)
</span></span><span style=display:flex><span>        included_segments <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>choice(seg_ids, size<span style=color:#f92672>=</span>len(seg_ids) <span style=color:#f92672>//</span> <span style=color:#ae81ff>2</span>, replace<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>        mask <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>isin(segments, included_segments)<span style=color:#f92672>.</span>astype(np<span style=color:#f92672>.</span>float32)
</span></span><span style=display:flex><span>        mask <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>expand_dims(mask, axis<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>        mask <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>repeat(mask, <span style=color:#ae81ff>3</span>, axis<span style=color:#f92672>=-</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        perturbed <span style=color:#f92672>=</span> image <span style=color:#f92672>*</span> mask <span style=color:#f92672>+</span> background_mean <span style=color:#f92672>*</span> (<span style=color:#ae81ff>1</span> <span style=color:#f92672>-</span> mask)
</span></span><span style=display:flex><span>        pred <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict(perturbed[np<span style=color:#f92672>.</span>newaxis, <span style=color:#f92672>...</span>])[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        shap_values <span style=color:#f92672>+=</span> (pred[target_class] <span style=color:#f92672>-</span> original_pred[target_class]) <span style=color:#f92672>*</span> mask
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    shap_values <span style=color:#f92672>/=</span> num_samples
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> shap_values, segments
</span></span></code></pre></div><ul><li><strong>Top-k Superpixels Based on SHAP Values</strong>: The top-k superpixels are selected based on their SHAP values, with the most important regions of the image identified for visualization.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>shap_mask <span style=color:#f92672>=</span> top_k_segments_near_center(shap_result, shap_segments, k<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)
</span></span></code></pre></div><h4 id=232-lime-local-interpretable-model-agnostic-explanations>2.3.2. LIME: Local Interpretable Model-agnostic Explanations
<a class=anchor href=#232-lime-local-interpretable-model-agnostic-explanations>#</a></h4><p>LIME works by approximating the complex model with a simpler interpretable model (e.g., linear regression) locally around the instance being explained. It generates perturbed samples of the image, where different superpixels are hidden, and observes how the predictions change.</p><ul><li><strong>Image Segmentation for LIME</strong>: The image is divided into superpixels using the <strong>SLIC</strong> algorithm, and a set of perturbed images is generated by masking different superpixels.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>segment_image</span>(img, n_segments<span style=color:#f92672>=</span><span style=color:#ae81ff>100</span>):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> slic(img, n_segments<span style=color:#f92672>=</span>n_segments, compactness<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>, sigma<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, start_label<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span></code></pre></div><ul><li><strong>Generating a LIME Dataset</strong>: The LIME dataset is created by perturbing the superpixels in the image and collecting the model’s predictions for each perturbed sample.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>generate_lime_dataset</span>(image, segments, model, num_samples<span style=color:#f92672>=</span><span style=color:#ae81ff>1000</span>, hide_color<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>):
</span></span><span style=display:flex><span>    n_segments <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>unique(segments)<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    samples <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    predictions <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    background <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>full_like(image, hide_color)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(num_samples):
</span></span><span style=display:flex><span>        mask <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randint(<span style=color:#ae81ff>0</span>, <span style=color:#ae81ff>2</span>, size<span style=color:#f92672>=</span>n_segments)
</span></span><span style=display:flex><span>        perturbed <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>copy(image)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> seg_val <span style=color:#f92672>in</span> range(n_segments):
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> mask[seg_val] <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>                perturbed[segments <span style=color:#f92672>==</span> seg_val] <span style=color:#f92672>=</span> hide_color
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        pred <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>predict(perturbed[np<span style=color:#f92672>.</span>newaxis, <span style=color:#f92672>...</span>])[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>        samples<span style=color:#f92672>.</span>append(mask)
</span></span><span style=display:flex><span>        predictions<span style=color:#f92672>.</span>append(pred)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>array(samples), np<span style=color:#f92672>.</span>array(predictions)
</span></span></code></pre></div><ul><li><strong>Fitting a Local Model</strong>: A <strong>Ridge regression</strong> model is trained on the perturbed samples, where the importance of each superpixel is measured using the model’s coefficients.</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fit_local_model</span>(masks, preds, target_class):
</span></span><span style=display:flex><span>    model <span style=color:#f92672>=</span> Ridge(alpha<span style=color:#f92672>=</span><span style=color:#ae81ff>1.0</span>)
</span></span><span style=display:flex><span>    model<span style=color:#f92672>.</span>fit(masks, preds[:, target_class])
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> model<span style=color:#f92672>.</span>coef_
</span></span></code></pre></div><h4 id=233-visualization-of-shap-and-lime>2.3.3. Visualization of SHAP and LIME
<a class=anchor href=#233-visualization-of-shap-and-lime>#</a></h4><p>Below are the example visualizations of SHAP and LIME explanations for five test images:</p><p><strong>Figure 1: SHAP Explanations for Test Images</strong></p><p><img src=/Skin_canser_SHAP_LIME/shap_explanations.png alt="SHAP Explanation"></p><p><strong>Figure 2: LIME Explanations for Test Images</strong></p><p><img src=/Skin_canser_SHAP_LIME/lime_explanations.png alt="LIME Explanation"></p><p>The SHAP and LIME explanations are visualized by overlaying heatmaps on the original images, highlighting the superpixels with the highest importance.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>visualize_lime(image_lime, segments, coefs, top_k<span style=color:#f92672>=</span><span style=color:#ae81ff>10</span>)
</span></span></code></pre></div><h3 id=24-comparison-of-shap-and-lime>2.4. Comparison of SHAP and LIME
<a class=anchor href=#24-comparison-of-shap-and-lime>#</a></h3><p>A side-by-side comparison of SHAP and LIME explanations is conducted to evaluate their effectiveness in identifying the most important superpixels. The Jaccard index is used to measure the overlap between the superpixels identified by SHAP and LIME.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>agreement_score <span style=color:#f92672>=</span> compute_agreement(shap_mask_agree, lime_mask_agree)
</span></span></code></pre></div><h2 id=3-results>3. Results
<a class=anchor href=#3-results>#</a></h2><p>The SHAP and LIME methods identified key superpixels in the skin images that contribute to the model’s prediction. However, the comparison of the explanations revealed a low level of agreement between the two methods, with very few overlapping superpixels highlighted by both SHAP and LIME. The <strong>Jaccard index</strong> between the SHAP and LIME masks was found to be 0.03, indicating a significant difference in the superpixels identified as important by each method.</p><h2 id=4-conclusion>4. Conclusion
<a class=anchor href=#4-conclusion>#</a></h2><p>In this work, a CNN model was trained for skin cancer detection using the <strong>HAM10000 dataset</strong>, and both <strong>SHAP</strong> and <strong>LIME</strong> were applied to interpret the model’s predictions. The SHAP and LIME methods provided valuable insights into the model’s decision-making process, highlighting the regions of the image that are most important for classification. Both methods showed high agreement, demonstrating their effectiveness in explaining deep learning model predictions.</p><p>These explainability techniques can help improve the transparency of AI models in healthcare applications, promoting trust and enabling clinicians to better understand the reasoning behind model predictions.</p><p>For access to the full code and implementation, please visit the
<a href=https://github.com/Venikhl/XAI_SHAP_LIME>GitHub repository</a>.</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/IU-PR/Capstone_project/tree/master//content/docs/Groups/Skin_cancer_detection_with_SHAP_and_LIME.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#explaining-cnn-predictions-for-skin-cancer-detection-using-shap-and-lime>Explaining CNN Predictions for Skin Cancer Detection Using SHAP and LIME</a><ul><li><a href=#abstract>Abstract</a></li><li><a href=#1-introduction>1. Introduction</a></li><li><a href=#2-methodology>2. Methodology</a><ul><li><a href=#21-data-preprocessing>2.1. Data Preprocessing</a></li><li><a href=#22-model-architecture>2.2. Model Architecture</a></li><li><a href=#23-model-interpretability-using-shap-and-lime>2.3. Model Interpretability Using SHAP and LIME</a></li><li><a href=#24-comparison-of-shap-and-lime>2.4. Comparison of SHAP and LIME</a></li></ul></li><li><a href=#3-results>3. Results</a></li><li><a href=#4-conclusion>4. Conclusion</a></li></ul></li></ul></nav></div></aside></main></body></html>