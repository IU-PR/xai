<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="XAI for Transformers. Explanations through LRP # Introduction # Transformers are becoming more and more common these days. But transformers are based on DNN that makes it harder to explain than other models. However, more and more ordinary users are starting to work with LLMs and to have more questions and doubts for its&rsquo; work and decisions. Thus, there is a need for some explanation of Transformers. The method presented in the article &ldquo;XAI for Transformers: Better Explanations through Conservative Propagation&rdquo; by Ameen Ali et."><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/groups/xai_for_transformers/"><meta property="og:site_name" content="XAI"><meta property="og:title" content="XAI"><meta property="og:description" content="XAI for Transformers. Explanations through LRP # Introduction # Transformers are becoming more and more common these days. But transformers are based on DNN that makes it harder to explain than other models. However, more and more ordinary users are starting to work with LLMs and to have more questions and doubts for its’ work and decisions. Thus, there is a need for some explanation of Transformers. The method presented in the article “XAI for Transformers: Better Explanations through Conservative Propagation” by Ameen Ali et."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>Xai for Transformers | XAI</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.e832d4e94212199857473bcf13a450d089c3fcd54ccadedcfac84ed0feff83fb.css integrity="sha256-6DLU6UISGZhXRzvPE6RQ0InD/NVMyt7c+shO0P7/g/s=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/mathtex-script-type.min.js integrity=sha384-jiBVvJ8NGGj5n7kJaiWwWp9AjC+Yh8rhZY3GtAX8yU28azcLgoRo4oukO87g7zDT crossorigin=anonymous></script><script defer src=/flexsearch.min.js></script><script defer src=/en.search.min.fa9a5612c6d891d8ff3a763f9eb1c2916a388026163bc6bf2e7cd1c2ec1740aa.js integrity="sha256-+ppWEsbYkdj/OnY/nrHCkWo4gCYWO8a/LnzRwuwXQKo=" crossorigin=anonymous></script><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/YELLOW_BAR.png alt=Logo><span><b>XAI</b></span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=/docs/groups/cam_and_secam/>CAM and SeCAM</a></li><li><a href=/docs/groups/diffusion-lens-interpreting-text-encoders-in-text-to-image-pipelines-tuned-using-dreambooth/>Diffusion Lens: Interpreting Text Encoders in Text-to-Image pipelines</a></li><li><a href=/docs/groups/dimensionality-reduction-in-nlp-visualizing-sentence-embeddings-with-umap-and-t-sne/>Dimensionality Reduction in NLP: Visualizing Sentence Embeddings with UMAP and t-SNE</a></li><li><a href=/docs/groups/example/>Example</a></li><li><a href=/docs/groups/grad-cam++/>Grad-CAM++</a></li><li><a href=/docs/groups/ai-playing-geoguessr-explained/>Ai Playing Geo Guessr Explained</a></li><li><a href=/docs/groups/contrastive-grad-cam-consistency/>Contrastive Grad Cam Consistency</a></li><li><a href=/docs/groups/dndfs_shap/>Dndfs Shap</a></li><li><a href=/docs/groups/gradcam/>Grad Cam</a></li><li><a href=/docs/groups/integrated-gradients/>Integrated Gradients</a></li><li><a href=/docs/groups/kernel-shap/>Kernel Shap</a></li><li><a href=/docs/groups/rag/>Rag</a></li><li><a href=/docs/groups/shap_darya_and_viktoria/>Shap Darya and Viktoria</a></li><li><a href=/docs/groups/sverl_tac_toe/>Sverl Tac Toe</a></li><li><a href=/docs/groups/torchprism/>Torch Prism</a></li><li><a href=/docs/groups/xai_for_transformers/ class=active>Xai for Transformers</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Xai for Transformers</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#xai-for-transformers-explanations-through-lrp>XAI for Transformers. Explanations through LRP</a><ul><li><a href=#introduction>Introduction</a></li><li><a href=#lrp-method>LRP method</a></li><li><a href=#better-lrp-rules-for-transformers>Better LRP Rules for Transformers</a></li><li><a href=#results>Results</a></li><li><a href=#references>References</a></li><li><a href=#code>Code</a></li></ul></li></ul></nav></aside></header><article class=markdown><h1 id=xai-for-transformers-explanations-through-lrp>XAI for Transformers. Explanations through LRP
<a class=anchor href=#xai-for-transformers-explanations-through-lrp>#</a></h1><h2 id=introduction>Introduction
<a class=anchor href=#introduction>#</a></h2><p>Transformers are becoming more and more common these days. But transformers are based on DNN that makes it harder to explain than other models. However, more and more ordinary users are starting to work with LLMs and to have more questions and doubts for its&rsquo; work and decisions. Thus, there is a need for some explanation of Transformers. The method presented in the article
<a href=https://proceedings.mlr.press/v162/ali22a/ali22a.pdf>&ldquo;XAI for Transformers: Better Explanations through Conservative Propagation&rdquo;</a> by Ameen Ali et. al. serves this purpose.</p><h2 id=lrp-method>LRP method
<a class=anchor href=#lrp-method>#</a></h2><p>Layer-wise Relevance Propagation method here are compared with Gradient×Input method presented in
<a href=https://openaccess.thecvf.com/content/CVPR2021/papers/Chefer_Transformer_Interpretability_Beyond_Attention_Visualization_CVPR_2021_paper.pdf>earlier article</a> in this field.</p><p align=center><img src=/xai_for_transformers/1.png width=300 alt="LRP outlook"></p><h5><font color=DimGray><center>Img.1. Layer-wise Relevance Propagation principe</center></font></h5><p>The relevence in LRP is computing as
$$R(x_i) = \sum_{j} \frac{\delta y_j}{\delta x_i} \frac{x_i}{y_j} R(y_j)$$</p><p>But in some layers of transformer formulas look little different. For the attention-head layer and for normalization layers rules are look like</p><link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\[R(x_i)=\sum_{j}\frac{x_i p_ij}{\sum_{i'} x_{i'} p_{i'j}}R(y_j) \text{ and } R(x_i)=\sum_{j}\frac{x_i (\delta_{ij} - \frac{1}{N})}{\sum_{i'} x_{i'} (\delta_{i'j} - \frac{1}{N})}R(y_j),\]</span><p>where <span>\(p_{ij}\)
</span>is a gating term value from attention head and for the LayerNorm <span>\((\delta_{ij} - \frac{1}{N})\)
</span>is the other way of writing the &lsquo;centering matrix&rsquo;, <span>\(N\)
</span>is the number of tokens in the input sequence.</p><h2 id=better-lrp-rules-for-transformers>Better LRP Rules for Transformers
<a class=anchor href=#better-lrp-rules-for-transformers>#</a></h2><p>In practice authors observed that these rules do not need to be implemented explicitly. There are trick makes the method straightforward to implement, by adding <code>detach()</code> calls at the appropriate locations in the neural network code and then running standard Gradient×Input.</p><p>So improved rules will be
<span>\[y_i = \sum_i x_i[p_{ij}].detach()\]
</span>for every attention head, and
<span>\[y_i = \frac{x_i - \mathbb{E}[x]}{\sqrt{\varepsilon + Var[x]}}.detach()\]
</span>for every LayerNorm, where <span>\( \mathbb{E}\)
</span>and <span>\(Var[x]\)
</span>is mean and variance.</p><h2 id=results>Results
<a class=anchor href=#results>#</a></h2><p>In the article different methods was tested on various datasets, but for now most inetersing is comparisom between old Gradient×Input (GI) method and new LRP methods with modifications in attention head rule (AH), LayerNorm (LN) or both (AH+LN).</p><p align=center><img src=/xai_for_transformers/2.png width=300 alt="GIvsLRP results"></p><h5><font color=DimGray><center>Img.2. AU-MSE (area under the mean squared error)</center></font></h5><p>The LRP with both modifications shows slightly better results in comparison with Gradient×Input method, but may make a huge difference in the future.</p><p>The results on SST-2 dataset that contains movie reviews and ratings are shown below. Both transformers was learned to classify review as positive or negative, and LRP shows slightly brighter and more concrete relevance values.</p><p align=center><img src=/xai_for_transformers/4.png alt="SST-2 results"></p><h2 id=references>References
<a class=anchor href=#references>#</a></h2><p>[1]
<a href=https://proceedings.mlr.press/v162/ali22a.html>Ameen Ali et. al. “XAI for Transformers: Better Explanations through Conservative Propagation.” ICML, 2022</a></p><p>[2]
<a href=https://openaccess.thecvf.com/content/CVPR2021/papers/Chefer_Transformer_Interpretability_Beyond_Attention_Visualization_CVPR_2021_paper.pdf>Hila Chefer et. al. “Transformer Interpretability Beyond Attention Visualization.” CVPR, 2021</a></p><h2 id=code>Code
<a class=anchor href=#code>#</a></h2><p>All code for Transformer you can find in
<a href=https://github.com/AmeenAli/XAI_Transformers>https://github.com/AmeenAli/XAI_Transformers</a></p></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/IU-PR/Capstone_project/tree/master//content/docs/Groups/xai_for_transformers.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#xai-for-transformers-explanations-through-lrp>XAI for Transformers. Explanations through LRP</a><ul><li><a href=#introduction>Introduction</a></li><li><a href=#lrp-method>LRP method</a></li><li><a href=#better-lrp-rules-for-transformers>Better LRP Rules for Transformers</a></li><li><a href=#results>Results</a></li><li><a href=#references>References</a></li><li><a href=#code>Code</a></li></ul></li></ul></nav></div></aside></main></body></html>