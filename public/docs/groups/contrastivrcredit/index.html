<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="&ldquo;Contrastive Explanations and Fairness in Credit Risk Predictions&rdquo; # authors:
Aleksandra Kuzmich Introduction # In recent years, machine learning has become a part of decision-making pipelines in finance and banking. Models are used for a wide range of tasks, from loan approvals to credit scoring. But as these models grow in complexity, they become harder to understand. For financial institutions, relying blindly on model predictions is not acceptable. Understanding why a particular decision was made ‚Äî and which features influenced is crucial."><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:title" content><meta property="og:description" content="&ldquo;Contrastive Explanations and Fairness in Credit Risk Predictions&rdquo; # authors:
Aleksandra Kuzmich Introduction # In recent years, machine learning has become a part of decision-making pipelines in finance and banking. Models are used for a wide range of tasks, from loan approvals to credit scoring. But as these models grow in complexity, they become harder to understand. For financial institutions, relying blindly on model predictions is not acceptable. Understanding why a particular decision was made ‚Äî and which features influenced is crucial."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/docs/groups/contrastivrcredit/"><meta property="article:section" content="docs"><title>Contrastivr Credit | XAI</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.e832d4e94212199857473bcf13a450d089c3fcd54ccadedcfac84ed0feff83fb.css integrity="sha256-6DLU6UISGZhXRzvPE6RQ0InD/NVMyt7c+shO0P7/g/s=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/mathtex-script-type.min.js integrity=sha384-jiBVvJ8NGGj5n7kJaiWwWp9AjC+Yh8rhZY3GtAX8yU28azcLgoRo4oukO87g7zDT crossorigin=anonymous></script><script defer src=/flexsearch.min.js></script><script defer src=/en.search.min.c9008eb139ac970f1f53a56abe5cdb2f36fa54018801e9ce80aa550667d5f022.js integrity="sha256-yQCOsTmslw8fU6VqvlzbLzb6VAGIAenOgKpVBmfV8CI=" crossorigin=anonymous></script><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/YELLOW_BAR.png alt=Logo><span><b>XAI</b></span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=/docs/groups/cam_and_secam/>CAM and SeCAM</a></li><li><a href=/docs/groups/diffusion-lens-interpreting-text-encoders-in-text-to-image-pipelines-tuned-using-dreambooth/>Diffusion Lens: Interpreting Text Encoders in Text-to-Image pipelines</a></li><li><a href=/docs/groups/dimensionality-reduction-in-nlp-visualizing-sentence-embeddings-with-umap-and-t-sne/>Dimensionality Reduction in NLP: Visualizing Sentence Embeddings with UMAP and t-SNE</a></li><li><a href=/docs/groups/example/>Example</a></li><li><a href=/docs/groups/ai-playing-geoguessr-explained/>Ai Playing Geo Guessr Explained</a></li><li><a href=/docs/groups/contrastive-grad-cam-consistency/>Contrastive Grad Cam Consistency</a></li><li><a href=/docs/groups/contrastivrcredit/ class=active>Contrastivr Credit</a></li><li><a href=/docs/groups/dndfs_shap/>Dndfs Shap</a></li><li><a href=/docs/groups/gradcam/>Grad Cam</a></li><li><a href=/docs/groups/integrated-gradients/>Integrated Gradients</a></li><li><a href=/docs/groups/kernel-shap/>Kernel Shap</a></li><li><a href=/docs/groups/rag/>Rag</a></li><li><a href=/docs/groups/shap_darya_and_viktoria/>Shap Darya and Viktoria</a></li><li><a href=/docs/groups/sverl_tac_toe/>Sverl Tac Toe</a></li><li><a href=/docs/groups/torchprism/>Torch Prism</a></li><li><a href=/docs/groups/xai_for_transformers/>Xai for Transformers</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Contrastivr Credit</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><ul><li><a href=#contrastive-explanations-and-fairness-in-credit-risk-predictions>&ldquo;Contrastive Explanations and Fairness in Credit Risk Predictions&rdquo;</a></li></ul></li><li><a href=#introduction>Introduction</a></li><li><a href=#what-are-contrastive-explanations>What Are Contrastive Explanations?</a></li><li><a href=#dataset>Dataset</a><ul><li><a href=#preprocessing>Preprocessing</a></li></ul></li><li><a href=#model-tabnet>Model: TabNet</a></li><li><a href=#start-of-xai-analysis>Start of XAI analysis</a><ul><li><ul><li><a href=#feature-dictionary>Feature Dictionary</a></li></ul></li><li><a href=#visualizing-contrastive-features>Visualizing Contrastive Features</a><ul><li><a href=#key-observations>Key Observations</a></li><li><a href=#comparing-contrastive-sets>Comparing Contrastive Sets</a></li></ul></li><li><a href=#fairness-through-contrastive-explanations>Fairness Through Contrastive Explanations</a><ul><li><a href=#by-income-group>By Income Group</a></li><li><a href=#by-verification-status>By Verification Status</a></li></ul></li><li><a href=#counterfactual-explanations>Counterfactual Explanations</a><ul><li><a href=#most-frequently-changed-features>Most Frequently Changed Features</a></li></ul></li><li><a href=#extra-shap-jumpscare>Extra! SHAP jumpscare</a><ul><li><a href=#single-prediction>Single Prediction</a></li><li><a href=#fairness-audit>Fairness Audit</a></li><li><a href=#what-obtained>What obtained?</a></li></ul></li><li><a href=#results--future-work>Results & Future Work</a><ul><li><a href=#-whats-next>üöÄ What‚Äôs Next?</a></li></ul></li></ul></li></ul></nav></aside></header><article class=markdown><h2 id=contrastive-explanations-and-fairness-in-credit-risk-predictions>&ldquo;Contrastive Explanations and Fairness in Credit Risk Predictions&rdquo;
<a class=anchor href=#contrastive-explanations-and-fairness-in-credit-risk-predictions>#</a></h2><p>authors:</p><ul><li>Aleksandra Kuzmich</li></ul><h1 id=introduction>Introduction
<a class=anchor href=#introduction>#</a></h1><p>In recent years, machine learning has become a part of decision-making pipelines in finance and banking. Models are used for a wide range of tasks, from loan approvals to credit scoring. But as these models grow in complexity, they become harder to understand. For financial institutions, relying blindly on model predictions is not acceptable. Understanding why a particular decision was made ‚Äî and which features influenced is crucial.</p><p>In this tutorial, we explore how contrastive explanations and counterfactual reasoning can help uncover the decision-making process behind a credit risk prediction model.</p><p>The model we use is <strong>TabNet</strong>, introduced in 2020 in the paper <em>TabNet: Attentive Interpretable Tabular Learning</em> by Arik and Pfister. As of today, TabNet is no longer considered state-of-the-art in terms of raw predictive performance, but it remains a popular and practical choice, especially for interpretable deep learning on tabular data.</p><p>One of TabNet‚Äôs key features is its built-in interpretability. It uses attention-based feature masks that reveal which features were used at each decision step. This provides a baseline level of transparency, but it doesn‚Äôt answer deeper questions like:</p><ul><li><em>Why did the model make this decision instead of the other one?</em></li><li><em>What features would need to change to get a different result?</em></li><li><em>Is the model treating similar applicants consistently across subgroups?</em></li></ul><p>Our goal is to go beyond TabNet‚Äôs built-in explanations. We&rsquo;ll use contrastive logic, greedy counterfactual search, and subgroup analysis to explore not just how the model works, but whether it works fairly and sensibly.</p><h1 id=what-are-contrastive-explanations>What Are Contrastive Explanations?
<a class=anchor href=#what-are-contrastive-explanations>#</a></h1><p>Most explainability methods try to answer: <strong>‚ÄúWhy did the model predict this outcome?‚Äù</strong></p><p>Contrastive explanations flip the question:</p><blockquote><p><strong>‚ÄúWhy this outcome, instead of some other one?‚Äù</strong></p></blockquote><p>This small shift makes a big difference. Humans often think in contrastive terms. Like, really, we don&rsquo;t usually ask why something happened in isolation. Instead, we want to know why it happened <em>instead of</em> what we expected.</p><p>For example: Why was this loan rejected instead of approved?</p><p>Contrastive explanations are useful because they focus only on what matters for a specific decision, ignoring everything that‚Äôs irrelevant. That makes the reasoning both more compact and more actionable.</p><p>Here we also consider identifying the minimal set of features that need to change to flip the model‚Äôs decision.</p><p>For example: If the applicant had a verified income and a slightly lower loan amount, the loan would have been approved.</p><p>This approach helps bridge the gap between black-box predictions and real-world reasoning.</p><h1 id=dataset>Dataset
<a class=anchor href=#dataset>#</a></h1><p>We use real-world credit application data from the
<a href=https://www.kaggle.com/datasets/wordsforthewise/lending-club>Lending Club Loan Data</a> dataset on Kaggle. This dataset contains accepted loan applications submitted between 2007 and 2018Q4 and includes both financial and demographic attributes of borrowers.</p><p><strong>File used</strong>: <code>accepted_2007_to_2018Q4.csv.gz</code></p><p>According to the web, dataset is widely used in both academic and applied research related to:</p><ul><li>Credit scoring and risk modeling</li><li>Fair lending analysis</li><li>Automated underwriting systems</li><li>Explainability and fairness audits in finance</li></ul><p>It includes features such as: Annual income, Loan amount, Debt-to-income (DTI) ratio, Employment length, and more. Where by more I mean 151 column.</p><p>The target variable for our task is whether the loan was <strong>fully paid</strong> or <strong>charged off</strong>, which is viewed as &ldquo;approved&rdquo; or &ldquo;rejected&rdquo;.</p><h2 id=preprocessing>Preprocessing
<a class=anchor href=#preprocessing>#</a></h2><p>For the preprocessing we:</p><ol><li>Filtered relevant loan statuses to create a binary classification task</li><li>Handled missing values</li><li>Encoded categorical variables</li><li>Standardized numerical columns</li><li>Sampled 20k and split the data into 80% training and 20% testing</li></ol><h1 id=model-tabnet>Model: TabNet
<a class=anchor href=#model-tabnet>#</a></h1><p>To model credit risk, we use <strong>TabNet</strong>, a deep learning architecture designed specifically for tabular data. TabNet replaces traditional fully-connected layers with sequential attention-based feature selection, allowing the model to focus on different subsets of features at each decision step.</p><p>We trained the model using the <code>pytorch-tabnet</code> library with the following setup:</p><ul><li><strong>Optimizer</strong>: Adam with a moderately aggressive learning rate</li><li><strong>Training duration</strong>: Up to 50 epochs with early stopping based on validation accuracy</li><li><strong>Batching</strong>: Large batch size with smaller virtual batches to regularize learning</li></ul><p>The model was trained on a binary classification task: predicting whether a loan would be fully paid or charged off. After training, it achieved good performance on the test set, with precision and recall above baseline levels for both classes (above 95%).</p><h1 id=start-of-xai-analysis>Start of XAI analysis
<a class=anchor href=#start-of-xai-analysis>#</a></h1><h3 id=feature-dictionary>Feature Dictionary
<a class=anchor href=#feature-dictionary>#</a></h3><ul><li><code>total_rec_prncp</code>: Total loan principal repaid</li><li><code>last_pymnt_amnt</code>: Amount of the most recent payment</li><li><code>last_fico_range_low</code>: Lower bound of the latest FICO score range</li><li><code>last_pymnt_d</code>: Date of the last payment</li><li><code>last_credit_pull_d</code>: Date of last credit check</li><li><code>issue_d</code>: When the loan was issued</li><li><code>earliest_cr_line</code>: Date of the borrower&rsquo;s first credit line</li><li><code>addr_state</code>: State of residence</li><li><code>emp_length</code>: Length of employment</li><li><code>last_fico_range_high</code>: Upper bound of latest FICO range</li><li><code>percent_bc_gt_75</code>: % of credit cards with high balances</li><li><code>revol_util</code>: % of revolving credit used</li><li><code>mths_since_recent_inq</code>: Months since last credit inquiry</li><li><code>total_bc_limit</code>: Total credit limit across all bankcard accounts</li><li><code>bc_open_to_buy</code>: Remaining credit on open bankcard accounts</li><li><code>application_type</code>: Indicates if the application is individual or joint</li><li><code>debt_settlement_flag</code>: Whether the borrower has settled debt for less than owed</li><li><code>num_tl_90g_dpd_24m</code>: Number of tradelines (loans/credit lines) with 90+ days past due in the past 24 months</li></ul><h2 id=visualizing-contrastive-features>Visualizing Contrastive Features
<a class=anchor href=#visualizing-contrastive-features>#</a></h2><p>After generating contrastive explanations, we aggregate the results to identify which features most often contributed to flipping a decision. This provides insight into the most influential features for each class.</p><p>Below are two horizontal bar plots showing the top contrastive features for:</p><ul><li><strong>Charged-Off</strong> loans (predicted defaults)</li><li><strong>Fully Paid</strong> loans (successful repayments)</li></ul><p><img src=/ContrastiveCredit/contr_co.jpg alt="Top for Charged-Off loans"></p><p><img src=/ContrastiveCredit/contr_f.jpg alt="Top for Fully Paid loans"></p><p>These plots reflect which features most frequently differed between a sample.</p><h3 id=key-observations>Key Observations
<a class=anchor href=#key-observations>#</a></h3><ul><li>Features related to <strong>payment timing</strong> (e.g., <code>last_pymnt_d</code>, <code>last_credit_pull_d</code>) and <strong>credit history</strong> (e.g., <code>earliest_cr_line</code>, <code>issue_d</code>) show up in both classes.</li><li><code>total_rec_prncp</code> (total principal received) is highly contrastive in <strong>Charged-Off</strong> cases, suggesting the model often associates low repayment with risk.</li><li><code>last_pymnt_amnt</code> and <code>total_bc_limit</code> appear only for <strong>Fully Paid</strong> cases, possibly indicating that the model pays more attention to available revolving credit when assessing lower-risk applicants.</li></ul><h3 id=comparing-contrastive-sets>Comparing Contrastive Sets
<a class=anchor href=#comparing-contrastive-sets>#</a></h3><p>To better understand how the model treats different classes, we built a table comparing the features that appear exclusively in one class, shared between both, or are unique to the other.</p><p><img src=/ContrastiveCredit/contr_venn.jpg alt="Venn-style feature comparison"></p><p>Features <code>debt_settlement_flag</code> or <code>num_tl_90g_dpd_24m</code> are only contrastive for Charged-Off loans, aligning with higher-risk borrower profiles.
Conversely, <code>application_type</code>, <code>bc_open_to_buy</code>, and <code>total_bc_limit</code> are unique to Fully Paid cases, potentially reflecting stronger financial standing.</p><h2 id=fairness-through-contrastive-explanations>Fairness Through Contrastive Explanations
<a class=anchor href=#fairness-through-contrastive-explanations>#</a></h2><p>One of the strengths of contrastive explanations is that they can reveal how a model reasons about different subgroups. In this section, we investigate whether TabNet&rsquo;s predictions of loan default &ldquo;Charged-Off&rdquo; depend on different features for different types of applicants.</p><p>We break applicants down into two dimensions:</p><ul><li><strong>Income group</strong>: High vs. Low, based on median annual income.</li><li><strong>Verification status</strong>: Whether the applicant&rsquo;s income was Verified, Partially Verified, or Not Verified.</li></ul><h3 id=by-income-group>By Income Group
<a class=anchor href=#by-income-group>#</a></h3><p>We generated contrastive explanations for Charged-Off predictions from both High Income and Low Income groups. The tables below show which features most frequently contributed to flipping a prediction ‚Äî in other words, which features differentiated Charged-Off applicants from otherwise similar Fully Paid applicants.</p><p><img src=/ContrastiveCredit/income_fair.jpg alt="Top by income group"></p><p>We observe some notable differences:</p><ul><li>Features like <code>total_rec_prncp</code>, <code>last_pymnt_amnt</code>, and <code>last_fico_range_low</code> appear more frequently in High Income explanations.</li><li>Meanwhile, <code>revol_util</code> and <code>percent_bc_gt_75</code> were more common contrastive factors among Low Income applicants.</li></ul><p>These patterns suggest that the model uses different signals to justify risk across income levels even when predicting the same outcome.</p><h3 id=by-verification-status>By Verification Status
<a class=anchor href=#by-verification-status>#</a></h3><p>We repeated this analysis for income verification status. Again, we sampled predictions from each group: Verified, Partially Verified, and Not Verified.</p><p><img src=/ContrastiveCredit/verif_fair.jpg alt="Top by verification status"></p><p>The differences here are even more striking:</p><ul><li>For Verified applicants, contrastive features leaned heavily on repayment history (<code>total_rec_prncp</code>, <code>last_pymnt_d</code>, etc.).</li><li>For Not Verified applicants, features like <code>last_fico_range_high</code> and <code>percent_bc_gt_75</code> were more influential, suggesting the model may be more cautious in the absence of income documentation.</li></ul><p>These findings don‚Äôt imply discrimination but they do reveal asymmetries in how the model makes decisions for different groups.</p><h2 id=counterfactual-explanations>Counterfactual Explanations
<a class=anchor href=#counterfactual-explanations>#</a></h2><p>To understand how sensitive the model is to individual features, we implemented a greedy counterfactual search. The goal is to answer the question:</p><blockquote><p>What‚Äôs the smallest set of changes we can make to flip the model‚Äôs prediction?</p></blockquote><p>We focused on samples predicted as Fully Paid (Class 0), and tried to change them into Charged-Off (Class 1) by incrementally tweaking one feature at a time. The search stops once the prediction flips or no combination works.</p><h3 id=most-frequently-changed-features>Most Frequently Changed Features
<a class=anchor href=#most-frequently-changed-features>#</a></h3><p>The bar chart below shows which features were most often changed across samples:</p><p><img src=/ContrastiveCredit/top_flip.jpg alt=Counterfactual></p><p>Samples requiring more changes are likely closer to the model&rsquo;s decision boundary. Unflippable cases suggest high model confidence in the original prediction.</p><p>These are the features the model relies on most when reconsidering its decision. <code>loan_amnt</code>, in particular, appears highly influential in flipping predictions from low to high risk.</p><p>This kind of analysis is useful when considering actionability, it tells not just what features mattered, but which changes would actually make a difference.</p><h2 id=extra-shap-jumpscare>Extra! SHAP jumpscare
<a class=anchor href=#extra-shap-jumpscare>#</a></h2><p>While contrastive explanations tell us what sets examples apart, SHAP shows which features actually pushed the decision one way or the other.</p><p>We implemented a simplified version of SHAP from scratch.
For each feature, we compute the model‚Äôs output with and without that feature (using a baseline value in its place), then average the marginal contribution across random subsets.</p><h3 id=single-prediction>Single Prediction
<a class=anchor href=#single-prediction>#</a></h3><p>Since the whole ivestigation is time expensive, we applied this method to one sample predicted as Fully Paid. Features with negative SHAP values reduced the predicted risk (pushed toward approval), while positive values pushed the prediction toward Charged-Off.</p><h3 id=fairness-audit>Fairness Audit
<a class=anchor href=#fairness-audit>#</a></h3><p>To investigate fairness, we extended the SHAP approach to compare average feature attributions across demographic subgroups. Income and verification, as in previous example.
For each group, we sampled examples, computed SHAP values, and averaged them to reveal the top influencing features.</p><h3 id=what-obtained>What obtained?
<a class=anchor href=#what-obtained>#</a></h3><p>Since the SHAP part was a bit of an afterthought (and the implementation isn‚Äôt optimized), we didn‚Äôt run a full-blown analysis on the entire dataset. Still, a few sample explanations were enough to show that the model uses different signals depending on income and verification status, which already tells us something interesting.</p><h2 id=results--future-work>Results & Future Work
<a class=anchor href=#results--future-work>#</a></h2><p>Contrastive explanations helped pinpoint which features separate approved from rejected applicants.
Using feature frequency across contrastive matches, we saw clear patterns: the model uses different signals for different classes and subgroups.
Fairness analysis revealed that even when predicting the same outcome, the model looks at different features for high- vs. low-income users or verified vs. unverified ones.</p><h3 id=-whats-next>üöÄ What‚Äôs Next?
<a class=anchor href=#-whats-next>#</a></h3><ul><li>Improve the efficiency of SHAP</li><li>Add more counterfactual metrics, like distance or plausibility scoring, to go beyond basic flips.</li><li>Try other fairness criteria.</li><li>Most of the explanations were done for just one class (Charged-Off). That was mainly for time and clarity. But in a full study, we‚Äôd absolutely want to repeat everything for the opposite class too to make sure the model isn‚Äôt only explainable or fair in one direction.</li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/IU-PR/Capstone_project/tree/master//content/docs/Groups/ContrastivrCredit.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><ul><li><a href=#contrastive-explanations-and-fairness-in-credit-risk-predictions>&ldquo;Contrastive Explanations and Fairness in Credit Risk Predictions&rdquo;</a></li></ul></li><li><a href=#introduction>Introduction</a></li><li><a href=#what-are-contrastive-explanations>What Are Contrastive Explanations?</a></li><li><a href=#dataset>Dataset</a><ul><li><a href=#preprocessing>Preprocessing</a></li></ul></li><li><a href=#model-tabnet>Model: TabNet</a></li><li><a href=#start-of-xai-analysis>Start of XAI analysis</a><ul><li><ul><li><a href=#feature-dictionary>Feature Dictionary</a></li></ul></li><li><a href=#visualizing-contrastive-features>Visualizing Contrastive Features</a><ul><li><a href=#key-observations>Key Observations</a></li><li><a href=#comparing-contrastive-sets>Comparing Contrastive Sets</a></li></ul></li><li><a href=#fairness-through-contrastive-explanations>Fairness Through Contrastive Explanations</a><ul><li><a href=#by-income-group>By Income Group</a></li><li><a href=#by-verification-status>By Verification Status</a></li></ul></li><li><a href=#counterfactual-explanations>Counterfactual Explanations</a><ul><li><a href=#most-frequently-changed-features>Most Frequently Changed Features</a></li></ul></li><li><a href=#extra-shap-jumpscare>Extra! SHAP jumpscare</a><ul><li><a href=#single-prediction>Single Prediction</a></li><li><a href=#fairness-audit>Fairness Audit</a></li><li><a href=#what-obtained>What obtained?</a></li></ul></li><li><a href=#results--future-work>Results & Future Work</a><ul><li><a href=#-whats-next>üöÄ What‚Äôs Next?</a></li></ul></li></ul></li></ul></nav></div></aside></main></body></html>