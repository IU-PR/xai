<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Retrieval-augmented generation # Project by:
Vladislav Urzhumov Danil Timofeev What is RAG? # Retrieval-Augmented Generation is a framework in the explainable artificial intelligence field for improving the quality of LLM-generated responses by grounding the model on external sources of knowledge to supplement the LLM’s internal representation of information. By grounding an LLM on a set of external, verifiable facts, the model has fewer opportunities to pull information baked into its parameters."><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:title" content><meta property="og:description" content="Retrieval-augmented generation # Project by:
Vladislav Urzhumov Danil Timofeev What is RAG? # Retrieval-Augmented Generation is a framework in the explainable artificial intelligence field for improving the quality of LLM-generated responses by grounding the model on external sources of knowledge to supplement the LLM’s internal representation of information. By grounding an LLM on a set of external, verifiable facts, the model has fewer opportunities to pull information baked into its parameters."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/docs/groups/rag/"><meta property="article:section" content="docs"><title>Rag | XAI</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.e832d4e94212199857473bcf13a450d089c3fcd54ccadedcfac84ed0feff83fb.css integrity="sha256-6DLU6UISGZhXRzvPE6RQ0InD/NVMyt7c+shO0P7/g/s=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/mathtex-script-type.min.js integrity=sha384-jiBVvJ8NGGj5n7kJaiWwWp9AjC+Yh8rhZY3GtAX8yU28azcLgoRo4oukO87g7zDT crossorigin=anonymous></script><script defer src=/flexsearch.min.js></script><script defer src=/en.search.min.45f09ee060aa482d344c82ae8f28314f0f9d7a6d41590df9a613b88bd43b9446.js integrity="sha256-RfCe4GCqSC00TIKujygxTw+dem1BWQ35phO4i9Q7lEY=" crossorigin=anonymous></script><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/YELLOW_BAR.png alt=Logo><span><b>XAI</b></span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=/docs/groups/cam_and_secam/>CAM and SeCAM</a></li><li><a href=/docs/groups/diffusion-lens-interpreting-text-encoders-in-text-to-image-pipelines-tuned-using-dreambooth/>Diffusion Lens: Interpreting Text Encoders in Text-to-Image pipelines</a></li><li><a href=/docs/groups/dimensionality-reduction-in-nlp-visualizing-sentence-embeddings-with-umap-and-t-sne/>Dimensionality Reduction in NLP: Visualizing Sentence Embeddings with UMAP and t-SNE</a></li><li><a href=/docs/groups/example/>Example</a></li><li><a href=/docs/groups/ai-playing-geoguessr-explained/>Ai Playing Geo Guessr Explained</a></li><li><a href=/docs/groups/contrastive-grad-cam-consistency/>Contrastive Grad Cam Consistency</a></li><li><a href=/docs/groups/dndfs_shap/>Dndfs Shap</a></li><li><a href=/docs/groups/explainable-hiring-models-with-catboost-and-shap/>Explainable Hiring Models With Cat Boost and Shap</a></li><li><a href=/docs/groups/gradcam/>Grad Cam</a></li><li><a href=/docs/groups/integrated-gradients/>Integrated Gradients</a></li><li><a href=/docs/groups/kernel-shap/>Kernel Shap</a></li><li><a href=/docs/groups/rag/ class=active>Rag</a></li><li><a href=/docs/groups/shap_darya_and_viktoria/>Shap Darya and Viktoria</a></li><li><a href=/docs/groups/sverl_tac_toe/>Sverl Tac Toe</a></li><li><a href=/docs/groups/torchprism/>Torch Prism</a></li><li><a href=/docs/groups/xai_for_transformers/>Xai for Transformers</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Rag</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#retrieval-augmented-generation>Retrieval-augmented generation</a><ul><li><a href=#what-is-rag>What is RAG?</a></li><li><a href=#advantages-of-rag>Advantages of RAG</a></li><li><a href=#technical-steps-of-rag>Technical steps of RAG</a></li><li><a href=#rag-improvements>RAG improvements</a></li><li><a href=#implementation-details>Implementation details</a></li><li><a href=#code>Code</a></li><li><a href=#further-improvements>Further improvements</a><ul><li><a href=#thanks-for-the-attention>Thanks for the attention!</a></li></ul></li></ul></li></ul></nav></aside></header><article class=markdown><h1 id=retrieval-augmented-generation>Retrieval-augmented generation
<a class=anchor href=#retrieval-augmented-generation>#</a></h1><p>Project by:</p><ul><li>Vladislav Urzhumov</li><li>Danil Timofeev</li></ul><h2 id=what-is-rag>What is RAG?
<a class=anchor href=#what-is-rag>#</a></h2><p>Retrieval-Augmented Generation is a framework in the explainable artificial intelligence field for improving the quality of LLM-generated responses by grounding the model on external sources of knowledge to supplement the LLM’s internal representation of information.
By grounding an LLM on a set of external, verifiable facts, the model has fewer opportunities to pull information baked into its parameters. This reduces the chances that an LLM will leak sensitive data, or &ldquo;hallucinate&rdquo; incorrect or misleading information.</p><h2 id=advantages-of-rag>Advantages of RAG
<a class=anchor href=#advantages-of-rag>#</a></h2><ul><li><em>Transparency</em>: generated response is supported by the document chunks model referred to. A short and easy-to-go variant of references.</li><li><em>Better accountability</em>: RAG allows to trace back the information source to understand, whether the mistake in the response was due to incorrect source data or a flaw in the model’s processing</li><li><em>Hallucinations avoided</em>: knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted.</li><li><em>Reasonably limited documents</em>: users can limit the external resources to provide responses based only one the trusted source, enhanced by the pre-trained model’s ability to construct adequate connected sentences.</li><li><em>Ease of implementation</em>: modern frameworks allow embracing of Retrieval-Augmented Generation with ease and small adjustments of code. One can always add improvements on top to achieve better results, however, standard RAG is easy-to-go.</li><li><em>Model-agnostic explainability of LMs</em>: any LLM&rsquo;s responses can be explained via RAG, despite of the model type and number of parameters.</li><li><em>Responses based on trusted documents</em>: there is an ability to construct a library of trusted documents to use while generating results, avoiding distrusted sources.</li></ul><h2 id=technical-steps-of-rag>Technical steps of RAG
<a class=anchor href=#technical-steps-of-rag>#</a></h2><p>Documents, which are provided for LLM to take information from, are firstly properly indexed.
In vanilla (naive) RAG, user query is not pre-processed specifically, thus validated user query plus the indexed documents are sent to the retrieval stage.</p><p>Relevant chunks of information are retrieved, included to prompt and then processed by LLM. Output is provided coupled with the indexed retrieved chunks of information.</p><h2 id=rag-improvements>RAG improvements
<a class=anchor href=#rag-improvements>#</a></h2><p>Retrieval-Augmented Generation, as a popular framework, was highly researched. Thus, numerous improvement techniques are present to enhance the performance of the technique.</p><p>Before the retrieval part, Pre-retrieval is recommended (such as query routing or query expansion) to provide more relevant or sufficient context.</p><p>After the retrieval part, Post-retrieval is used to summarize, fuse or rerank retrieved results.</p><p>Our framework research and implementation embrace the pre-retrieval addition, leaving the post-retrieval for the readers to try out and make an experiment.</p><h2 id=implementation-details>Implementation details
<a class=anchor href=#implementation-details>#</a></h2><ul><li>LLM used is <code>llama-3-8b-8192</code> from Groq. We picked the Groq provider for open-source models because it’s the quickest one out there and llama-3-8b is the current state-of-the-art for small language models.</li><li>Embedding model embraced is <code>bge-base-en-v1.5</code>. The embedding model used is also open-source, it’s currently the SOTA for its current size. We could have used a bigger more capable multilingual model, but we decided to just keep the English support, so this was sufficient.</li><li>RAG framework for our purpose should be well-maintained and easy-to-use. Thus, we have chosen <code>llama-index</code> (vector databases and retrieval);</li><li>As an improvement, our team has used AutoContext to pre-retrieve the necessary information. We have used it to correctly summarize the main idea behind the whole document with specific details in separate chunks.</li></ul><h2 id=code>Code
<a class=anchor href=#code>#</a></h2><p>Best way to check code is interactive environment, such as Google Colab.
Hence, our team provides one for any developer interested in trying by own hand.</p><p>Please, enjoy by following the
<a href="https://colab.research.google.com/drive/1chU3jbysPW3z9j6b8zrg--P9UUXkrfKq?usp=sharing">link</a>.
Don&rsquo;t forget to insert your own Groq API key here (it&rsquo;s free):</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#66d9ef>if</span> <span style=color:#e6db74>&#34;llm&#34;</span> <span style=color:#f92672>not</span> <span style=color:#f92672>in</span> st<span style=color:#f92672>.</span>session_state:
</span></span><span style=display:flex><span>    st<span style=color:#f92672>.</span>session_state<span style=color:#f92672>.</span>llm <span style=color:#f92672>=</span> Groq(
</span></span><span style=display:flex><span>        model<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;llama3-8b-8192&#34;</span>,
</span></span><span style=display:flex><span>        api_key<span style=color:#f92672>=</span><span style=color:#e6db74>&#34;&#34;</span>,
</span></span><span style=display:flex><span>        kwargs<span style=color:#f92672>=</span>{<span style=color:#e6db74>&#34;&#34;</span>},
</span></span><span style=display:flex><span>    )
</span></span><span style=display:flex><span>    Settings<span style=color:#f92672>.</span>llm <span style=color:#f92672>=</span> st<span style=color:#f92672>.</span>session_state<span style=color:#f92672>.</span>llm
</span></span></code></pre></div><h2 id=further-improvements>Further improvements
<a class=anchor href=#further-improvements>#</a></h2><ul><li><em>Better pipelines</em>: Retrieval can be improved by implementing more complex RAG pipelines, adding better data parsing (better chunking, for example) and adding a reranker, for example (a step in post-retrieval);</li><li><em>Multilingual support</em>: users are interested in explained results not only in english, but also in other languages;</li><li><em>OCR support</em>: Optical Character Recognition (OCR) is a technology that falls under the umbrella of machine learning and computer vision. It involves converting images containing written text into machine-readable text data. Being able to process images with infographics or screenshots along with non-copiable image-based pdfs is a big step towards versatility;</li><li><em>Specific rules for scientific paper</em>: Pre-retrieval for scientific paper should be focused on abstracts, summaries and conclusions written by human authors rather than generated ones, that will increase understanding and trustworthiness of documents and retrieval pipeline.</li></ul><h3 id=thanks-for-the-attention>Thanks for the attention!
<a class=anchor href=#thanks-for-the-attention>#</a></h3></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/IU-PR/Capstone_project/tree/master//content/docs/Groups/RAG.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#retrieval-augmented-generation>Retrieval-augmented generation</a><ul><li><a href=#what-is-rag>What is RAG?</a></li><li><a href=#advantages-of-rag>Advantages of RAG</a></li><li><a href=#technical-steps-of-rag>Technical steps of RAG</a></li><li><a href=#rag-improvements>RAG improvements</a></li><li><a href=#implementation-details>Implementation details</a></li><li><a href=#code>Code</a></li><li><a href=#further-improvements>Further improvements</a><ul><li><a href=#thanks-for-the-attention>Thanks for the attention!</a></li></ul></li></ul></li></ul></nav></div></aside></main></body></html>