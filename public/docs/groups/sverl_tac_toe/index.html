<!doctype html><html lang=en-us dir=ltr><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Explaining Reinforcement Learning with Shapley Values | Tutorial # Introduction # https://www.toptal.com/machine-learning/deep-dive-into-reinforcement-learning Explainability and interpretability of AI models is a hot topic in the research community in recent times. With the growth of new technologies and methods in the neural networks field, the endeavour to understand the ‚Äúblack box‚Äù models is rising in popularity.
In this tutorial, you will learn 2 methods for explaining the reinforcement learning models:
Applying Shapley values to policies Using SVERL-P method Before we start, this tutorial is based on the paper ‚ÄúExplaining Reinforcement Learning with Shapley Values‚Äù by Beechey et."><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/groups/sverl_tac_toe/"><meta property="og:site_name" content="XAI"><meta property="og:title" content="XAI"><meta property="og:description" content="Explaining Reinforcement Learning with Shapley Values | Tutorial # Introduction # https://www.toptal.com/machine-learning/deep-dive-into-reinforcement-learning Explainability and interpretability of AI models is a hot topic in the research community in recent times. With the growth of new technologies and methods in the neural networks field, the endeavour to understand the ‚Äúblack box‚Äù models is rising in popularity.
In this tutorial, you will learn 2 methods for explaining the reinforcement learning models:
Applying Shapley values to policies Using SVERL-P method Before we start, this tutorial is based on the paper ‚ÄúExplaining Reinforcement Learning with Shapley Values‚Äù by Beechey et."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>Sverl Tac Toe | XAI</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.e832d4e94212199857473bcf13a450d089c3fcd54ccadedcfac84ed0feff83fb.css integrity="sha256-6DLU6UISGZhXRzvPE6RQ0InD/NVMyt7c+shO0P7/g/s=" crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.4/dist/contrib/mathtex-script-type.min.js integrity=sha384-jiBVvJ8NGGj5n7kJaiWwWp9AjC+Yh8rhZY3GtAX8yU28azcLgoRo4oukO87g7zDT crossorigin=anonymous></script><script defer src=/flexsearch.min.js></script><script defer src=/en.search.min.4a627215bc1fa1b6d8f93178d3e8c3d3622cfa283905d61db15b6c2b8cc8e0cc.js integrity="sha256-SmJyFbwfobbY+TF40+jD02Is+ig5BdYdsVtsK4zI4Mw=" crossorigin=anonymous></script><script defer src=/sw.min.6f6f90fcb8eb1c49ec389838e6b801d0de19430b8e516902f8d75c3c8bd98739.js integrity="sha256-b2+Q/LjrHEnsOJg45rgB0N4ZQwuOUWkC+NdcPIvZhzk=" crossorigin=anonymous></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/YELLOW_BAR.png alt=Logo><span><b>XAI</b></span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><a href=/docs/groups/cam_and_secam/>CAM and SeCAM</a></li><li><a href=/docs/groups/diffusion-lens-interpreting-text-encoders-in-text-to-image-pipelines-tuned-using-dreambooth/>Diffusion Lens: Interpreting Text Encoders in Text-to-Image pipelines</a></li><li><a href=/docs/groups/dimensionality-reduction-in-nlp-visualizing-sentence-embeddings-with-umap-and-t-sne/>Dimensionality Reduction in NLP: Visualizing Sentence Embeddings with UMAP and t-SNE</a></li><li><a href=/docs/groups/example/>Example</a></li><li><a href=/docs/groups/grad-cam++/>Grad-CAM++</a></li><li><a href=/docs/groups/ai-playing-geoguessr-explained/>Ai Playing Geo Guessr Explained</a></li><li><a href=/docs/groups/contrastive-grad-cam-consistency/>Contrastive Grad Cam Consistency</a></li><li><a href=/docs/groups/dndfs_shap/>Dndfs Shap</a></li><li><a href=/docs/groups/gradcam/>Grad Cam</a></li><li><a href=/docs/groups/integrated-gradients/>Integrated Gradients</a></li><li><a href=/docs/groups/kernel-shap/>Kernel Shap</a></li><li><a href=/docs/groups/rag/>Rag</a></li><li><a href=/docs/groups/shap_darya_and_viktoria/>Shap Darya and Viktoria</a></li><li><a href=/docs/groups/sverl_tac_toe/ class=active>Sverl Tac Toe</a></li><li><a href=/docs/groups/torchprism/>Torch Prism</a></li><li><a href=/docs/groups/xai_for_transformers/>Xai for Transformers</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>Sverl Tac Toe</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#explaining-reinforcement-learning-with-shapley-values--tutorial>Explaining Reinforcement Learning with Shapley Values | Tutorial</a><ul><li><a href=#introduction>Introduction</a></li><li><a href=#explainable-artificial-intelligence>Explainable Artificial Intelligence</a></li><li><a href=#shapley-values>Shapley values</a></li><li><a href=#environment>Environment</a></li><li><a href=#shapley-values-applied-to-policy>Shapley values applied to policy</a></li><li><a href=#sverl-p>SVERL-P</a></li><li><a href=#demo>Demo</a></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#references>References</a></li></ul></li></ul></nav></aside></header><article class=markdown><h1 id=explaining-reinforcement-learning-with-shapley-values--tutorial>Explaining Reinforcement Learning with Shapley Values | Tutorial
<a class=anchor href=#explaining-reinforcement-learning-with-shapley-values--tutorial>#</a></h1><h2 id=introduction>Introduction
<a class=anchor href=#introduction>#</a></h2><div align=center><img src=/Sverl_Tac_Toe/rl.png><p><a href=https://www.toptal.com/machine-learning/deep-dive-into-reinforcement-learning>https://www.toptal.com/machine-learning/deep-dive-into-reinforcement-learning</a></p></div><p>Explainability and interpretability of AI models is a hot topic in the research community in recent times. With the growth of new technologies and methods in the neural networks field, the endeavour to understand the ‚Äúblack box‚Äù models is rising in popularity.</p><p>In this tutorial, you will learn 2 methods for explaining the reinforcement learning models:</p><ul><li>Applying Shapley values to policies</li><li>Using SVERL-P method</li></ul><p>Before we start, this tutorial is based on the paper ‚ÄúExplaining Reinforcement Learning with Shapley Values‚Äù by Beechey et. al. [1], where the researchers present 2 approaches on how to explain reinforcement learning models with Shapley values.</p><p>Also, we assume that you are familiar with machine learning, reinforcement learning, but not familiar with explainable artificial intelligence.</p><p>A quick reminder: reinforcement learning is a type of unsupervised learning technique, in which you train the agent in the environment, which can vary from a
<a href="https://www.youtube.com/watch?v=kopoLzvh5jY">hide-and-seek game</a> to a
<a href=https://github.com/facebookresearch/nocturne>traffic simulation</a>.</p><div align=center><img src=/Sverl_Tac_Toe/hide_and_seek.gif><p>OpenAI reinforcement learning research paper visualisation [2].</p></div><p>For the implementation, we will be using a ‚ö°blazingly fast‚ö° and statically-typed programming language called ü¶Ä Rust ü¶Ä. We will provide a few code snippets, but our main goal is to provide you with the idea on implementation, so you can try to implement these methods in your favourite programming language.</p><h2 id=explainable-artificial-intelligence>Explainable Artificial Intelligence
<a class=anchor href=#explainable-artificial-intelligence>#</a></h2><div align=center><img src=/Sverl_Tac_Toe/xai_growth.png><p>Yearly growth trends. Papers before 2013 were omitted for readability [3].</p></div><p><strong>Explainable Artificial Intelligence (XAI)</strong> is a field of study that seeks to make AI systems more understandable and interpretable to humans [4].</p><p><strong>Interpretability</strong> is the ability of a human to understand the reasoning behind a decision made by an AI system [4].</p><p><strong>Fair AI</strong>: AI systems can perpetuate and amplify existing societal biases if they are not carefully designed. Fair AI techniques can help to ensure that AI systems make decisions that are fair and unbiased for all people [4].</p><p>There are many techniques, methods, and algorithms invented in recent years to explain AI and design it to be fair. These techniques involve: LIME (Locally Interpretable Model-Agnostic Explanations), SHAP (Shapley Additive exPlanations), DeepDream, SVERL-P, and many more.</p><h2 id=shapley-values>Shapley values
<a class=anchor href=#shapley-values>#</a></h2><div align=center><img src=/Sverl_Tac_Toe/group_project.png></div><p>Imagine you and your friend are making a group project in a university course. At the end of the course, you get B and your friend gets A. You think this is unfair, but how to prove it? Well, to fairly assess your performance, you can calculate your contribution to the group project, i.e. to a coalition of two players using Shapley values. For example, if your friend would do the project alone, they would complete 45% of the project. However, together you complete 100% of the project.</p><div align=center><img src=/Sverl_Tac_Toe/shapley_formula.png><p>Shapley value formula [5].</p></div><p>Shapley values are a concept from cooperative game theory that provides a way to fairly distribute the marginal gains among players in a coalition, ensuring that each player gets as much or more than they would have if they acted independently.</p><p>In the above formula, your friend‚Äôs val(S) = 75, and the value of both of you is val(S union {i}) = 100. Now, your marginal contribution to the coalition of two players (you and your friend) is 100 - 45 = 55. Now, you cannot say that your whole contribution is 55, you need to calculate the average of this number to fairly assess your performance. That is when weight comes into play. You can think of it as a way to normalise marginal contribution over all possible coalitions, even when it is an empty coalition, i.e. noone does the project.</p><p>After all the calculations are done, you get a fair assessment of your performance, and prove to your teaching instructor that you deserve a better grade.</p><p>One more thing about Shapley values is that they satisfy the following 4 properties [6]:</p><div align=center><img src=/Sverl_Tac_Toe/shapley_properties.png><p>4 properties of Shapley values [5].</p></div><ul><li><strong>Efficiency</strong> - the sum of the Shapley values of all agents equals the value of the grand coalition, so that all the gain is distributed among the agents.</li><li><strong>Symmetry</strong> - two players are considered interchangeable if they make the same contribution to all coalitions.</li><li><strong>Null player</strong> - if the player makes 0 contribution to all coalitions, then they have zero shapley value.</li><li><strong>Linearity (Additivity)</strong> - shapley value of coalition is equal to shapley values of individuals in this coalition.</li></ul><h2 id=environment>Environment
<a class=anchor href=#environment>#</a></h2><div align=center><img src=/Sverl_Tac_Toe/tic_tac_toe.png></div><p>For this project, we have coded the Tic-Tac-Toe game in the Rust programming language using Geng game engine [7].</p><p>In the centre of the screen there is a grid of cells, where each cell contains either: nothing, cross, or circle. Players make moves in-turns, and one of them wins if they either make a consequent horizontal, vertical, or diagonal line of the same shape, corresponding to them (circle or cross).</p><p>Exact implementation of this environment is out of scope for this tutorial. However, you can find the full source code on our GitHub [8].</p><p>In this simple environment we will introduce 2 models/policies and try to interpret their actions. For that reason, we first need to describe the environment in terms of Markov Decision Process:</p><ul><li><strong>State</strong> is the grid, consisting of 9 cells.</li><li><strong>Action</strong> is an input from the user to place a shape. In our case, we have 9 actions, one for each cell. Environment handles the turns automatically, so that is why you do not need to worry about which shape to put.</li><li><strong>Reward</strong> is either 0 or 1 (player either wins a game or looses)</li><li><strong>Policies</strong>: random and minimax.<ul><li><strong>Random</strong> policy outputs a random legal move, i.e. it cannot place a shape on the occupied cell or make turns when the game is finished.</li><li><strong>Minimax</strong> calculates all possible game outcomes for both players, and compares them. As output it produces the action, which leads to a maximum available value.</li></ul></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-rust data-lang=rust><span style=display:flex><span><span style=color:#66d9ef>pub</span> <span style=color:#66d9ef>enum</span> <span style=color:#a6e22e>Tile</span> {
</span></span><span style=display:flex><span>    Empty,
</span></span><span style=display:flex><span>    X,
</span></span><span style=display:flex><span>    O,
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>pub</span> <span style=color:#66d9ef>struct</span> <span style=color:#a6e22e>Grid</span><span style=color:#f92672>&lt;</span>T <span style=color:#f92672>=</span> Tile<span style=color:#f92672>&gt;</span> {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>pub</span> cells: [[T; <span style=color:#ae81ff>3</span>]; <span style=color:#ae81ff>3</span>],
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>pub</span> <span style=color:#66d9ef>type</span> <span style=color:#a6e22e>Policy</span> <span style=color:#f92672>=</span> Box<span style=color:#f92672>&lt;</span><span style=color:#66d9ef>dyn</span> FnMut(<span style=color:#f92672>&amp;</span>Grid) -&gt; <span style=color:#a6e22e>Grid</span><span style=color:#f92672>&lt;</span><span style=color:#66d9ef>f64</span><span style=color:#f92672>&gt;&gt;</span>; <span style=color:#75715e>// policies return the probabilty distribution over possible action
</span></span></span><span style=display:flex><span><span style=color:#75715e></span><span style=color:#66d9ef>pub</span> <span style=color:#66d9ef>type</span> <span style=color:#a6e22e>Action</span> <span style=color:#f92672>=</span> vec2<span style=color:#f92672>&lt;</span>Coord<span style=color:#f92672>&gt;</span>;
</span></span></code></pre></div><h2 id=shapley-values-applied-to-policy>Shapley values applied to policy
<a class=anchor href=#shapley-values-applied-to-policy>#</a></h2><p>Before applying the method, we need to define terms and try to understand them.</p><ul><li><strong>Policy</strong> takes a state and returns the probability distribution over possible actions.</li><li><strong>Observation</strong> is the state with some of the features (cells) hidden.</li></ul><p>The first method to interpret the reinforcement learning model is to apply shapley values directly to the policy,</p><p>The <strong>value function</strong> used to evaluate a policy, as defined in the paper, is the expected probability of each action over the distribution of possible states (given current partial observation).</p><link rel=stylesheet href=/katex/katex.min.css><script defer src=/katex/katex.min.js></script><script defer src=/katex/auto-render.min.js onload=renderMathInElement(document.body)></script><span>\(v^\pi(C) = \pi_c(a|s) = \sum_{s' \in S} p^\pi(s'|s_C) \pi(a|s') \)</span><p>Where C is the observation (with the features from the coalition), <span>\(p^\pi(s‚Äô|s_C)\)
</span>is the probability of seeing state <span>\(s‚Äô\)
</span>given the observation <span>\(s_C\)
</span>.</p><p>So, to calculate the Shapley value for each feature for every action, we first need to get all partial observations:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-rust data-lang=rust><span style=display:flex><span><span style=color:#66d9ef>impl</span> Grid {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>pub</span> <span style=color:#66d9ef>fn</span> <span style=color:#a6e22e>all_subsets</span>(<span style=color:#f92672>&amp;</span>self) -&gt; Vec<span style=color:#f92672>&lt;</span>Observation<span style=color:#f92672>&gt;</span> {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> positions: Vec<span style=color:#f92672>&lt;</span>_<span style=color:#f92672>&gt;</span> <span style=color:#f92672>=</span> self.positions().collect();
</span></span><span style=display:flex><span>        powerset(<span style=color:#f92672>&amp;</span>positions)
</span></span><span style=display:flex><span>            .into_iter()
</span></span><span style=display:flex><span>            .skip(<span style=color:#ae81ff>1</span>) <span style=color:#75715e>// Skip the set itself
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>            .map(<span style=color:#f92672>|</span>positions<span style=color:#f92672>|</span> Observation {
</span></span><span style=display:flex><span>                positions,
</span></span><span style=display:flex><span>                grid: <span style=color:#a6e22e>self</span>.clone(),
</span></span><span style=display:flex><span>            })
</span></span><span style=display:flex><span>            .collect()
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Then, for every observation we calculate the value function for it and for the observation without the <code>feature</code> we&rsquo;re calculating for:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-rust data-lang=rust><span style=display:flex><span><span style=color:#66d9ef>let</span> observations <span style=color:#f92672>=</span> grid.all_subsets();
</span></span><span style=display:flex><span><span style=color:#66d9ef>let</span> n <span style=color:#f92672>=</span> grid.positions().count(); <span style=color:#75715e>// number of all features
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>observations
</span></span><span style=display:flex><span>    .iter()
</span></span><span style=display:flex><span>    .cloned()
</span></span><span style=display:flex><span>    .map(<span style=color:#f92672>|</span>observation<span style=color:#f92672>|</span> {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> base_value <span style=color:#f92672>=</span> value(feature, <span style=color:#f92672>&amp;</span>observation);
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> s <span style=color:#f92672>=</span> observation.positions.len();
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> scale <span style=color:#f92672>=</span> factorial(s) <span style=color:#66d9ef>as</span> <span style=color:#66d9ef>f64</span> <span style=color:#f92672>*</span> factorial(n <span style=color:#f92672>-</span> s <span style=color:#f92672>-</span> <span style=color:#ae81ff>1</span>) <span style=color:#66d9ef>as</span> <span style=color:#66d9ef>f64</span> <span style=color:#f92672>/</span> factorial(n) <span style=color:#66d9ef>as</span> <span style=color:#66d9ef>f64</span>;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> featureless <span style=color:#f92672>=</span> observation.subtract(feature);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>mut</span> value <span style=color:#f92672>=</span> base_value <span style=color:#f92672>-</span> value(feature, <span style=color:#f92672>&amp;</span>featureless);
</span></span><span style=display:flex><span>        value <span style=color:#f92672>*=</span> scale;
</span></span><span style=display:flex><span>        value
</span></span><span style=display:flex><span>    })
</span></span><span style=display:flex><span>    .fold(Grid::zero(), Grid::add)
</span></span></code></pre></div><p>The value function itself is the expected value for every action, and is easy to implement.
But we also need to calculate possible states given the observation.
We can do that by iterating over all hidden cells in the grid and checking all 3 possible states for that cell.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-rust data-lang=rust><span style=display:flex><span><span style=color:#66d9ef>impl</span> Observation {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>pub</span> <span style=color:#66d9ef>fn</span> <span style=color:#a6e22e>value</span>(<span style=color:#f92672>&amp;</span>self, policy: <span style=color:#66d9ef>&amp;</span><span style=color:#a6e22e>mut</span> Policy) -&gt; <span style=color:#a6e22e>Grid</span><span style=color:#f92672>&lt;</span><span style=color:#66d9ef>f64</span><span style=color:#f92672>&gt;</span> {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> states <span style=color:#f92672>=</span> self.possible_states();
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> scale <span style=color:#f92672>=</span> (states.len() <span style=color:#66d9ef>as</span> <span style=color:#66d9ef>f64</span>).recip();
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>mut</span> result <span style=color:#f92672>=</span> states
</span></span><span style=display:flex><span>            .into_iter()
</span></span><span style=display:flex><span>            .map(<span style=color:#f92672>|</span>state<span style=color:#f92672>|</span> policy(<span style=color:#f92672>&amp;</span>state))
</span></span><span style=display:flex><span>            .fold(Grid::zero(), Grid::add);
</span></span><span style=display:flex><span>        result <span style=color:#f92672>*=</span> scale;
</span></span><span style=display:flex><span>        result
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>pub</span> <span style=color:#66d9ef>fn</span> <span style=color:#a6e22e>possible_states</span>(<span style=color:#f92672>&amp;</span>self) -&gt; Vec<span style=color:#f92672>&lt;</span>Grid<span style=color:#f92672>&gt;</span> {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> hidden: Vec<span style=color:#f92672>&lt;</span>_<span style=color:#f92672>&gt;</span> <span style=color:#f92672>=</span> self
</span></span><span style=display:flex><span>            .grid
</span></span><span style=display:flex><span>            .positions()
</span></span><span style=display:flex><span>            .filter(<span style=color:#f92672>|</span>pos<span style=color:#f92672>|</span> <span style=color:#f92672>!</span>self.positions.contains(pos))
</span></span><span style=display:flex><span>            .collect();
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        (<span style=color:#ae81ff>0</span><span style=color:#f92672>..</span><span style=color:#ae81ff>3</span><span style=color:#66d9ef>usize</span>.pow(hidden.len() <span style=color:#66d9ef>as</span> <span style=color:#66d9ef>u32</span>))
</span></span><span style=display:flex><span>            .map(<span style=color:#f92672>|</span>i<span style=color:#f92672>|</span> {
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>mut</span> grid <span style=color:#f92672>=</span> self.grid.clone();
</span></span><span style=display:flex><span>                <span style=color:#66d9ef>for</span> (pos, cell) <span style=color:#66d9ef>in</span> hidden.iter().enumerate().map(<span style=color:#f92672>|</span>(t, <span style=color:#f92672>&amp;</span>pos)<span style=color:#f92672>|</span> {
</span></span><span style=display:flex><span>                    <span style=color:#66d9ef>let</span> cell <span style=color:#f92672>=</span> <span style=color:#66d9ef>match</span> (i <span style=color:#f92672>/</span> <span style=color:#ae81ff>3_</span><span style=color:#66d9ef>usize</span>.pow(t <span style=color:#66d9ef>as</span> <span style=color:#66d9ef>u32</span>)) <span style=color:#f92672>%</span> <span style=color:#ae81ff>3</span> {
</span></span><span style=display:flex><span>                        <span style=color:#ae81ff>0</span> <span style=color:#f92672>=&gt;</span> Tile::Empty,
</span></span><span style=display:flex><span>                        <span style=color:#ae81ff>1</span> <span style=color:#f92672>=&gt;</span> Tile::X,
</span></span><span style=display:flex><span>                        <span style=color:#ae81ff>2</span> <span style=color:#f92672>=&gt;</span> Tile::O,
</span></span><span style=display:flex><span>                        _ <span style=color:#f92672>=&gt;</span> unreachable!(),
</span></span><span style=display:flex><span>                    };
</span></span><span style=display:flex><span>                    (pos, cell)
</span></span><span style=display:flex><span>                }) {
</span></span><span style=display:flex><span>                    grid.set(pos, cell);
</span></span><span style=display:flex><span>                }
</span></span><span style=display:flex><span>                grid
</span></span><span style=display:flex><span>            })
</span></span><span style=display:flex><span>            .collect()
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h2 id=sverl-p>SVERL-P
<a class=anchor href=#sverl-p>#</a></h2><p>Instead of applying shapley values directly to policy, we can use a better approach, which is called Shapley Values for Explaining Reinforcement Learning Performance or in short SVERL-P.</p><p>SVERL-P is divided into 2 methods: local and global ones.</p><p>Local SVERL-P is essentially a prediction of the reward given uncertainty of the current observation.</p><span>\(v^{local}(C) = E_{\hat\pi}[\sum_{t=0}^\infty \gamma^t r_{t+1} | s_0=s]\)</span><p>Where</p><span>\(\hat\pi(a_t|s_t) = \begin{cases} \pi_C(a_t|s_t)\ if\ s_t=s, \\ \pi(a_t|s_t)\ otherwise \end{cases}\)</span><p><span>\(r\)
</span>is the reward (winner gets the reward of 1), <span>\(\gamma\)
</span>is the discounting factor.</p><p>Global extends the uncertainty to all future states, rather than just the starting one.</p><span>\(v^{global}(C) = E_{\pi_C}[\sum_{t=0}^{\infty} \gamma^t r_{t+1} | s_0=s]\)
</span><span>\(–§_i(v^{global}) = E_{p^\pi(s)}[\phi_i(v^{global}, s)]\)</span><p>Where <span>\(\phi_i\)
</span>is the Shapley value.</p><p>The difference with the previous (regular Shapley) value function is the future prediction. So, after calculating the initial <span>\(\pi_C\)
</span>the same way (<code>Observation::value</code>), we recursively go through all the states in the future and calculate the expected reward, according to the policy action distribution.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-rust data-lang=rust><span style=display:flex><span><span style=color:#66d9ef>impl</span> Grid {
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>fn</span> <span style=color:#a6e22e>predict</span>(<span style=color:#f92672>&amp;</span>self, gamma: <span style=color:#66d9ef>f64</span>, policy: <span style=color:#66d9ef>&amp;</span><span style=color:#a6e22e>mut</span> Policy) -&gt; <span style=color:#66d9ef>f64</span> {
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> Some(player) <span style=color:#f92672>=</span> self.current_player() <span style=color:#66d9ef>else</span> {
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>0.0</span>; <span style=color:#75715e>// The game has ended
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        };
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>mut</span> result <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.0</span>; <span style=color:#75715e>// Expected reward
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        <span style=color:#66d9ef>let</span> weights <span style=color:#f92672>=</span> policy(self); <span style=color:#75715e>// action probability distribution
</span></span></span><span style=display:flex><span><span style=color:#75715e></span>        <span style=color:#66d9ef>for</span> pos <span style=color:#66d9ef>in</span> self.empty_positions() {
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>let</span> prob <span style=color:#f92672>=</span> weights.get(pos).unwrap();
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>let</span> <span style=color:#66d9ef>mut</span> grid <span style=color:#f92672>=</span> self.clone();
</span></span><span style=display:flex><span>            grid.set(pos, player.into());
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>let</span> immediate_reward <span style=color:#f92672>=</span> grid.reward(player);
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>let</span> future_reward <span style=color:#f92672>=</span> gamma <span style=color:#f92672>*</span> grid.predict(gamma, policy);
</span></span><span style=display:flex><span>            result <span style=color:#f92672>+=</span> prob <span style=color:#f92672>*</span> (immediate_reward <span style=color:#f92672>+</span> future_reward);
</span></span><span style=display:flex><span>        }
</span></span><span style=display:flex><span>        result
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h2 id=demo>Demo
<a class=anchor href=#demo>#</a></h2><p>You can try out environment from this tutorial in the web for yourself:
<a href=https://1adis1.github.io/xai-sverl/>https://1adis1.github.io/xai-sverl/</a></p><div align=center><img src=/Sverl_Tac_Toe/shapley.gif></div><h2 id=conclusion>Conclusion
<a class=anchor href=#conclusion>#</a></h2><p>As a result, we have implemented Shapley values and SVERL-P algorithm to Tic-Tac-Toe. While Shapley values are used for interpreting specific actions of the policy, it may be difficult to understand their impact on the game. And SVERL-P is designed to solve that problem by showing directly the contribution of each feature to the outcome of the game.</p><h2 id=references>References
<a class=anchor href=#references>#</a></h2><ul><li>[1]
<a href=https://proceedings.mlr.press/v202/beechey23a/beechey23a.pdf>Beechey et. al. ‚ÄúExplaining Reinforcement Learning with Shapley Values‚Äù</a></li><li>[2]
<a href="https://www.youtube.com/watch?v=kopoLzvh5jY">OpenAI Hide-and-Seek simulation</a></li><li>[3]
<a href=https://arxiv.org/pdf/2301.05433>Trends in Explainable AI by Alon Jacovi</a></li><li>[4] Rustam Lukmanov, Explainable and Fair AI, Spring 24, Lecture 1</li><li>[5]
<a href="https://www.youtube.com/watch?v=UJeu29wq7d0">The mathematics behind Shapley Values</a></li><li>[6]
<a href=https://en.wikipedia.org/wiki/Shapley_value#Properties>Shapley values</a></li><li>[7]
<a href=https://github.com/geng-engine/geng>Geng game engine</a></li><li>[8]
<a href=https://github.com/1ADIS1/xai-sverl>Our implementation with environment in Rust</a></li></ul></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/IU-PR/Capstone_project/tree/master//content/docs/Groups/Sverl_Tac_Toe.md target=_blank rel=noopener><img src=/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#explaining-reinforcement-learning-with-shapley-values--tutorial>Explaining Reinforcement Learning with Shapley Values | Tutorial</a><ul><li><a href=#introduction>Introduction</a></li><li><a href=#explainable-artificial-intelligence>Explainable Artificial Intelligence</a></li><li><a href=#shapley-values>Shapley values</a></li><li><a href=#environment>Environment</a></li><li><a href=#shapley-values-applied-to-policy>Shapley values applied to policy</a></li><li><a href=#sverl-p>SVERL-P</a></li><li><a href=#demo>Demo</a></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#references>References</a></li></ul></li></ul></nav></div></aside></main></body></html>