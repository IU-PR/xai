[{"id":0,"href":"/docs/groups/cam_and_secam/","title":"CAM and SeCAM","section":"Docs","content":" CAM and SeCAM: Explainable AI for Understanding Image Classification Models # Tutorial by Yaroslav Sokolov and Iskander Ishkineev\nTo see the implementation, visit our colab project.\nIntroduction # Explainable Artificial Intelligence (XAI) has emerged as a crucial aspect of AI research, aiming to enhance the transparency and interpretability of AI models. Understanding the decision-making process of AI systems is essential for ensuring trust, accountability, and safety in their applications. In this tutorial, we focus on Class Activation Mapping (CAM) and Segmentation Class Activation Mapping (SeCAM). Specifically, we consider their application in explaining the decisions of the ResNet50 model, a pivotal architecture within the domain of deep CNNs that has significantly impacted image classification tasks. CAM and SeCAM in particular aim to provide fast and intuitive explanations by identifying image regions most influential to the model\u0026rsquo;s prediction.\nSection 1: Overview of XAI Methods # Explainable AI (XAI) refers to methods and techniques in the application of artificial intelligence technology such that the results of the solution can be understood by human experts. It contrasts with the concept of the \u0026ldquo;black box\u0026rdquo; in machine learning where even their designers cannot explain why the AI arrived at a specific decision. XAI is becoming increasingly important as AI systems are used in more critical applications such as diagnostic healthcare, autonomous driving, and more.\nSection 2: ResNet50 Architecture and Importance of Understanding # The ResNet50 model is a pivotal architecture within the domain of deep convolutional neural networks (CNNs) that has significantly impacted image classification tasks, notably achieving remarkable success in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). Generally, ResNet50 has the following architecture:\nThe ResNet50 model, while renowned for its high accuracy in image classification tasks, exemplifies the \u0026ldquo;black box\u0026rdquo; nature inherent to many advanced deep learning models. This characteristic poses a significant challenge for AI researchers, practitioners, and end-users who seek to understand the model\u0026rsquo;s predictive behaviour. The intricate architecture of ResNet50, characterised by its deep layers and residual blocks, complicates the interpretation of how input features influence the final classification outcomes\nSection 3: Class Activation Mapping (CAM) # General Definition # Class Activation Mapping (CAM) is a technique used to identify the discriminative regions in an image that contribute to the class prediction made by a Convolutional Neural Network (CNN). CAM is particularly useful for understanding and interpreting the decisions of CNN models.\nHow CAM Works: # a) Steps Involved in CAM: # Feature Extraction:\nExtract the feature maps from the last convolutional layer of the CNN. Global Average Pooling (GAP):\nApply Global Average Pooling (GAP) to the feature maps to get a vector of size equal to the number of feature maps. Fully Connected Layer:\nThe GAP output is fed into a fully connected layer to get the final class scores. Class Activation Mapping:\nFor a given class, compute the weighted sum of the feature maps using the weights from the fully connected layer. b) Equations # Feature Maps:\nLet \\(f_k(x, y)\\) represent the activation of unit \\(k\\) in the feature map at spatial location \\((x, y)\\) . Global Average Pooling:\nThe GAP for feature map \\(k\\) is computed as: \\[F_k = \\frac{1}{Z} \\sum_{x} \\sum_{y} f_k(x, y)\\] where \\(Z\\) is the number of pixels in the feature map. Class Score:\nThe class score \\(S_c\\) for class \\(c\\) is computed as: \\[S_c = \\sum_{k} w_{k}^{c} F_k \\] where \\(w_{k}^{c}\\) is the weight corresponding to class \\(c\\) for feature map \\(k\\) . Class Activation Map:\nThe CAM for class \\(c\\) is computed as: \\[M_c(x, y) = \\sum_{k} w_{k}^{c} f_k(x, y)\\] This gives the importance of each spatial element \\((x, y)\\) in the feature maps for class \\(c\\) . c) Implementation with Code # Step 1: Preprocess the Input Image # Firstly, we need to read and preprocess the input image to make it compatible with the ResNet50 model. The preprocessing steps include resizing the image to 224x224 pixels, normalizing it, and converting it to a PyTorch tensor.\ndef preprocess_image(image_path): \u0026#34;\u0026#34;\u0026#34; Preprocess the input image. :param image_path: Path to the input image :return: Preprocessed image tensor, original image, image dimensions \u0026#34;\u0026#34;\u0026#34; image = cv2.imread(image_path) original_image = image.copy() image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) height, width, _ = image.shape preprocess = transforms.Compose([ transforms.ToPILImage(), transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ]) image_tensor = preprocess(image) image_tensor = image_tensor.unsqueeze(0) return image_tensor, original_image, height, width image = cv2.imread(image_path): Reads the image from the specified path using OpenCV. original_image = image.copy(): Creates a copy of the original image to preserve it for later use. image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB): Converts the image from BGR to RGB format as OpenCV reads images in BGR format by default. height, width, _ = image.shape: Retrieves the dimensions (height and width) of the image. preprocess = transforms.Compose([...]): Defines a series of preprocessing steps: transforms.ToPILImage(): Converts the image to PIL format. transforms.Resize((224, 224)): Resizes the image to 224x224 pixels. transforms.ToTensor(): Converts the image to a PyTorch tensor. transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]): Normalizes the image tensor using the specified mean and standard deviation. image_tensor = preprocess(image): Applies the preprocessing steps to the image. image_tensor = image_tensor.unsqueeze(0): Adds a batch dimension to the image tensor, making it compatible for input to the CNN. Step 2: Load Model and Extract Features # Secondly, we need to load a pre-trained ResNet50 model and extract the feature maps from the last convolutional layer.\ndef load_model_and_extract_features(): \u0026#34;\u0026#34;\u0026#34; Load the pretrained ResNet50 model and extract features. :return: Model, features blob list, softmax weights \u0026#34;\u0026#34;\u0026#34; model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT).eval() features_blobs = [] def hook_feature(module, input, output): features_blobs.append(output.data.cpu().numpy()) # Hook the feature extractor to get the convolutional features from \u0026#39;layer4\u0026#39; model._modules.get(\u0026#39;layer4\u0026#39;).register_forward_hook(hook_feature) # Get the softmax weights params = list(model.parameters()) softmax_weights = np.squeeze(params[-2].data.numpy()) return model, features_blobs, softmax_weights model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT).eval(): Loads a pre-trained ResNet50 model and sets it to evaluation mode. features_blobs = []: Initializes an empty list to store feature maps from the hooked layer. def hook_feature(module, input, output): Defines a hook function that captures the output of the specified layer. features_blobs.append(output.data.cpu().numpy()): Appends the output feature maps to the features_blobs list, converting them to NumPy arrays and moving them to the CPU. model._modules.get('layer4').register_forward_hook(hook_feature): Registers the hook on the last convolutional layer (layer4) of the model to capture its output. params = list(model.parameters()): Retrieves the parameters of the model. softmax_weights = np.squeeze(params[-2].data.numpy()): Extracts and squeezes the softmax weights from the fully connected layer of the model, converting them to a NumPy array. Step 3: Get Top K Predictions # Thirdly, we need to perform a forward pass through the model, compute the softmax probabilities, and retrieve the top K class indices with the highest probabilities.\ndef get_topk_predictions(model, image_tensor, topk_predictions): \u0026#34;\u0026#34;\u0026#34; Get the top k predictions for the input image. :param model: Pretrained model :param image_tensor: Preprocessed image tensor :param topk_predictions: Number of top predictions to get :return: Top k class indices \u0026#34;\u0026#34;\u0026#34; outputs = model(image_tensor) probabilities = F.softmax(outputs, dim=1).data.squeeze() class_indices = topk(probabilities, topk_predictions)[1].int() return class_indices outputs = model(image_tensor): Performs a forward pass through the model using the preprocessed image tensor. probabilities = F.softmax(outputs, dim=1).data.squeeze(): Computes the softmax probabilities for the outputs, normalizing them across the class dimension, and removes extra dimensions. class_indices = topk(probabilities, topk_predictions)[1].int(): Retrieves the indices of the top K classes with the highest probabilities using the topk function. Step 4: Generate Class Activation Maps (CAM) # After all the above steps, we can generate the Class Activation Maps (CAM) for the top K predictions.\ndef generate_CAM(feature_maps, softmax_weights, class_indices): \u0026#34;\u0026#34;\u0026#34; Generate Class Activation Maps (CAM). :param feature_maps: Convolutional feature maps :param softmax_weights: Weights of the fully connected layer :param class_indices: Class indices :return: List of CAMs for the specified class indices \u0026#34;\u0026#34;\u0026#34; batch_size, num_channels, height, width = feature_maps.shape output_cams = [] for class_idx in class_indices: cam = softmax_weights[class_idx].dot(feature_maps.reshape((num_channels, height * width))) cam = cam.reshape(height, width) cam = cam - np.min(cam) # Normalize CAM to be non-negative cam = cam / np.max(cam) # Scale CAM to be in range [0, 1] cam_img = np.uint8(255 * cam) # Convert to uint8 format output_cams.append(cam_img) return output_cams batch_size, num_channels, height, width = feature_maps.shape: Retrieves the shape of the feature maps. output_cams = []: Initializes an empty list to store the CAMs. for class_idx in class_indices: Iterates over each class index. cam = softmax_weights[class_idx].dot(feature_maps.reshape((num_channels, height * width))): Computes the CAM by taking the weighted sum of the feature maps. cam = cam.reshape(height, width): Reshapes the CAM to the original feature map size. cam = cam - np.min(cam): Normalizes the CAM to be non-negative. cam = cam / np.max(cam): Scales the CAM to be in the range [0, 1]. cam_img = np.uint8(255 * cam): Converts the CAM to uint8 format. output_cams.append(cam_img): Adds the CAM to the list of output CAMs. This function computes the CAM for each class index by taking the weighted sum of the feature maps using the softmax weights. It normalizes and converts the CAM to an 8-bit image.\nStep 5: Display and Save the CAM # After generating the CAMs, we can overlay them on the original image and display the results.\ndef display_and_save_CAM(CAMs, width, height, original_image, class_indices, class_labels, save_name, plot=True): \u0026#34;\u0026#34;\u0026#34; Display and save the CAM images. :param CAMs: List of CAMs :param width: Width of the original image :param height: Height of the original image :param original_image: Original input image :param class_indices: Class indices :param class_labels: List of all class names :param save_name: Name to save the output image :param plot: Whether to display the image \u0026#34;\u0026#34;\u0026#34; matplotlib.rcParams[\u0026#39;figure.figsize\u0026#39;] = 15, 12 for i, cam in enumerate(CAMs): heatmap = cv2.applyColorMap(cv2.resize(cam, (width, height)), cv2.COLORMAP_JET) result = heatmap * 0.3 + original_image * 0.5 # Put class label text on the result cv2.putText(result, class_labels[class_indices[i]], (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2) cv2.imwrite(f\u0026#34;outputs/CAM_{save_name}_{i}.jpg\u0026#34;, result) # Display the result if plot: image = plt.imread(f\u0026#34;outputs/CAM_{save_name}_{i}.jpg\u0026#34;) plt.imshow(image) plt.axis(\u0026#39;off\u0026#39;) plt.show() matplotlib.rcParams['figure.figsize'] = 15, 12: Sets the figure size for matplotlib plots. for i, cam in enumerate(CAMs): Iterates over each CAM. heatmap = cv2.applyColorMap(cv2.resize(cam, (width, height)), cv2.COLORMAP_JET): Creates a heatmap by resizing the CAM and applying a colormap. result = heatmap * 0.3 + original_image * 0.5: Overlays the heatmap on the original image. cv2.putText(result, class_labels[class_indices[i]], (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2): Adds the class label to the result image. cv2.imwrite(f\u0026quot;outputs/CAM_{save_name}_{i}.jpg\u0026quot;, result): Saves the result image. image = plt.imread(f\u0026quot;outputs/CAM_{save_name}_{i}.jpg\u0026quot;): Reads the saved image. plt.imshow(image): Displays the image. plt.axis('off'): Hides the axis. plt.show(): Shows the image. This function overlays the CAM on the original image, adds the class label, and saves the result. It also displays the final image.\nStep 6: Full Pipeline for Generating and Saving CAM # Finally, we can combine all the above steps into a single function to generate and save CAMs for the top K predictions of an input image.\ndef generate_and_save_CAM(image_path, topk_predictions): \u0026#34;\u0026#34;\u0026#34; Generate and save CAM for the specified image and topk predictions. :param image_path: Path to the input image :param topk_predictions: Number of top predictions to generate CAM for \u0026#34;\u0026#34;\u0026#34; # Load the class labels class_labels = load_class_labels(\u0026#39;LOC_synset_mapping.txt\u0026#39;) # Read and preprocess the image image_tensor, original_image, height, width = preprocess_image(image_path) # Load the model and extract features model, features_blobs, softmax_weights = load_model_and_extract_features() # Get the top k predictions class_indices = get_topk_predictions(model, image_tensor, topk_predictions) # Generate CAM for the top predictions CAMs = generate_CAM(features_blobs[0], softmax_weights, class_indices) # Display and save the CAM results save_name = image_path.split(\u0026#39;/\u0026#39;)[-1].split(\u0026#39;.\u0026#39;)[0] display_and_save_CAM(CAMs, width, height, original_image, class_indices, class_labels, save_name) class_labels = load_class_labels('LOC_synset_mapping.txt'): Loads the class labels from a specified file. image_tensor, original_image, height, width = preprocess_image(image_path): Reads and preprocesses the input image, returning the preprocessed image tensor, original image, and image dimensions. model, features_blobs, softmax_weights = load_model_and_extract_features(): Loads the model and extracts the feature maps and softmax weights. class_indices, _ = get_topk_predictions(model, image_tensor, topk_predictions): Gets the top K predictions for the input image. CAMs = generate_CAM(features_blobs[0], softmax_weights, class_indices): Generates the CAMs for the top predictions. save_name = image_path.split('/')[-1].split('.')[0]: Extracts the base name of the image file for saving the results. display_and_save_CAM(CAMs, width, height, original_image, class_indices, class_labels, save_name): Displays and saves the CAM results. This function combines all the steps to generate and save CAMs for the top K predictions of an input image.\nd) Examples of Work: # Here you can see the results of applying CAM to an image of a dogs. The CAM highlights the regions of the image that are most influential in the model\u0026rsquo;s prediction of the class \u0026ldquo;Yorkshire Terrier\u0026rdquo;:\nThere is another example of applying CAM to an image of a bird:\nNow let\u0026rsquo;s see the difference of CAMs for different classes of images.\nSection 4: Segmentation - Class Activation Mapping (CAM) # General Definition # Segmentation-based Class Activation Mapping (SeCAM) is an advanced technique proposed by Quoc Hung Cao, Truong Thanh Hung Nguyen, Vo Thanh Khang Nguyen, and Xuan Phong Nguyen in their paper Segmentation-based Class Activation Mapping. This method combines the principles of Class Activation Mapping (CAM) with image segmentation to provide more interpretable and precise discriminative regions in an image. SeCAM helps in understanding and interpreting the decisions of Convolutional Neural Network (CNN) models by highlighting regions of the image that are most influential in the model\u0026rsquo;s prediction, segmented into meaningful parts.\nHow SeCAM Works # a) Steps Involved in SeCAM: # Feature Extraction:\nExtract the feature maps from the last convolutional layer of the CNN. Global Average Pooling (GAP):\nApply Global Average Pooling (GAP) to the feature maps to get a vector of size equal to the number of feature maps. Fully Connected Layer:\nThe GAP output is fed into a fully connected layer to get the final class scores. Class Activation Mapping (CAM):\nFor a given class, compute the weighted sum of the feature maps using the weights from the fully connected layer. Superpixel Segmentation:\nSegment the input image into superpixels using the SLIC (Simple Linear Iterative Clustering) algorithm. Segmentation-based CAM (SeCAM):\nCombine the CAM values with the segmented superpixels to compute the SeCAM values for each region. b) SLIC (Simple Linear Iterative Clustering) Algorithm # SLIC is a superpixel segmentation algorithm that clusters pixels in an image into superpixels. Superpixels are contiguous groups of pixels with similar colors or gray levels. The SLIC algorithm adapts k-means clustering to efficiently generate superpixels with uniform size and compactness.\nSteps of the SLIC Algorithm # Initialization:\nGrid Sampling: The image is divided into a grid of \\( N/K \\) equally spaced initial cluster centers, where \\(N\\) is the number of pixels, and \\(K\\) is the desired number of superpixels. Perturbation: Each cluster center is moved to the lowest gradient position within a 3x3 neighborhood to avoid placing centers at edges. Assignment:\nFor each pixel, find the nearest cluster center based on a distance measure that includes color and spatial proximity. Distance \\(D\\) is computed as a weighted sum of color distance and spatial distance. Update:\nUpdate each cluster center to the mean of the pixels assigned to it. Recompute the cluster centers. Enforce Connectivity:\nEnsure that each superpixel is a single connected component. Repeat:\nRepeat the assignment and update steps until convergence. Distance Measure # The distance \\(D\\) between a pixel \\(i\\) and a cluster center \\(k\\) is defined as:\n\\[D = \\sqrt{d_{lab}^2 \u0026#43; \\left(\\frac{m}{S}\\right)^2 d*{xy}^2}\\] where:\n\\(d_{lab}\\) : Euclidean distance in the CIELAB color space. \\(d_{xy}\\) : Euclidean distance in the pixel coordinate space. \\(S\\) : Grid interval, approximately \\(\\sqrt{N/K}\\) . \\(m\\) : Compactness parameter, controlling the trade-off between color similarity and spatial proximity. Numerical Example # Assume we have a small 5x5 grayscale image:\n\\[\\begin{bmatrix} 10 \u0026amp; 10 \u0026amp; 10 \u0026amp; 20 \u0026amp; 20 \\\\ 10 \u0026amp; 10 \u0026amp; 10 \u0026amp; 20 \u0026amp; 20 \\\\ 10 \u0026amp; 10 \u0026amp; 10 \u0026amp; 20 \u0026amp; 20 \\\\ 30 \u0026amp; 30 \u0026amp; 30 \u0026amp; 40 \u0026amp; 40 \\\\ 30 \u0026amp; 30 \u0026amp; 30 \u0026amp; 40 \u0026amp; 40 \\\\ \\end{bmatrix}\\] Let\u0026rsquo;s apply SLIC to generate 4 superpixels.\nInitialization:\nNumber of pixels \\(N = 25\\) Desired superpixels \\(K = 4\\) Grid interval \\(S \\approx \\sqrt{25 / 4} = 2.5\\) Place initial cluster centers (perturbed for the lowest gradient): Cluster Centers = \\(\\{ (1, 1), (1, 4), (4, 1), (4, 4) \\}\\) Assignment:\nFor each pixel, compute the distance \\(D\\) to each cluster center. Example for pixel at (2,2): Color distance \\(d_{lab} = |10 - 10| = 0\\) Spatial distance \\(d_{xy} = \\sqrt{(2-1)^2 \u0026#43; (2-1)^2} = \\sqrt{2}\\) Assume \\(m = 10\\) , \\(S = 2.5\\) : \\[ D = \\sqrt{0^2 \u0026#43; \\left(\\frac{10}{2.5}\\right)^2 \\cdot 2} = \\sqrt{0 \u0026#43; 16 \\cdot 2} = \\sqrt{32} = 5.66 \\] Update:\nUpdate cluster centers based on the mean of the assigned pixels. Recompute centers. Enforce Connectivity:\nEnsure all superpixels are connected components. Repeat:\nIterate until convergence. Output Matrix with Segments:\nAfter applying SLIC, we get the segmented image with superpixels. The output matrix might look like this:\n\\[\\begin{bmatrix} 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 2 \u0026amp; 2 \\\\ 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 2 \u0026amp; 2 \\\\ 1 \u0026amp; 1 \u0026amp; 1 \u0026amp; 2 \u0026amp; 2 \\\\ 3 \u0026amp; 3 \u0026amp; 3 \u0026amp; 4 \u0026amp; 4 \\\\ 3 \u0026amp; 3 \u0026amp; 3 \u0026amp; 4 \u0026amp; 4 \\\\ \\end{bmatrix}\\] c) SeCAM Equations # Feature Maps:\nLet \\(f_k(x, y)\\) represent the activation of unit \\(k\\) in the feature map at spatial location \\((x, y)\\) . Class Activation Map (CAM):\nThe CAM for class \\(c\\) is computed as: \\[ M_c(x, y) = \\sum_{k} w_{k}^{c} f_k(x, y) \\] This gives the importance of each spatial element \\((x, y)\\) in the feature maps for class \\(c\\) . SeCAM:\nFor each superpixel, the SeCAM value is computed by averaging the CAM values within the superpixel: \\[ S_c(s) = \\frac{1}{|s|} \\sum_{(x, y) \\in s} M_c(x, y) \\] where \\(|s|\\) is the number of pixels in superpixel \\(s\\) . d) Implementation with Code # Step 1-3: Same as for CAM # Step 4: Generate SeCAM # After all preparation steps, we can generate the Segmentation-based Class Activation Maps (SeCAM) for the top K predictions.\ndef generate_seCAM(feature_maps, softmax_weights, class_indices, segments): \u0026#34;\u0026#34;\u0026#34; Generate Segmentation-based Class Activation Maps (SeCAM). :param feature_maps: Convolutional feature maps :param softmax_weights: Weights of the fully connected layer :param class_indices: Class indices :param segments: Segmented image regions :return: List of SeCAMs for the specified class indices \u0026#34;\u0026#34;\u0026#34; batch_size, num_channels, height, width = feature_maps.shape output_seCAMs = [] for class_idx in class_indices: cam = softmax_weights[class_idx].dot(feature_maps.reshape((num_channels, height * width))) cam = cam.reshape(height, width) cam = np.maximum(cam, 0) cam = cam / np.max(cam) # Resize segments to match cam size segments_resized = cv2.resize(segments.astype(np.float32), (width, height), interpolation=cv2.INTER_NEAREST) segments_resized = segments_resized.astype(int) seCAM = np.zeros_like(cam) for seg_val in np.unique(segments_resized): mask = (segments_resized == seg_val) seCAM[mask] = cam[mask].mean() output_seCAMs.append(seCAM) return output_seCAMs batch_size, num_channels, height, width = feature_maps.shape: Retrieves the shape of the feature maps. output_seCAMs = []: Initializes an empty list to store the SeCAMs. for class_idx in class_indices: Iterates over each class index. cam = softmax_weights[class_idx].dot(feature_maps.reshape((num_channels, height * width))): Computes the CAM by taking the weighted sum of the feature maps. cam = cam.reshape(height, width): Reshapes the CAM to the original feature map size. cam = np.maximum(cam, 0): Ensures all CAM values are non-negative. cam = cam / np.max(cam): Normalizes the CAM values to be in the range [0, 1]. segments_resized = cv2.resize(segments.astype(np.float32), (width, height), interpolation=cv2.INTER_NEAREST): Resizes the segments to match the CAM size. segments_resized = segments_resized.astype(int): Converts the resized segments to integers. seCAM = np.zeros_like(cam): Initializes an array of zeros with the same shape as the CAM. for seg_val in np.unique(segments_resized): Iterates over each unique segment value. mask = (segments_resized == seg_val): Creates a mask for the current segment. seCAM[mask] = cam[mask].mean(): Assigns the mean CAM value of the current segment to the SeCAM array. output_seCAMs.append(seCAM): Adds the SeCAM to the list of output SeCAMs. This function generates Segmentation-based CAMs (SeCAMs) by combining CAM values with superpixel segments, resulting in more interpretable and precise discriminative regions.\nStep 5: Display and Save SeCAM # After generating the SeCAMs, we can overlay them on the original image, mask insignificant regions, and display the results.\ndef display_and_save_seCAM(SeCAMs, width, height, original_image, save_name, secam_threshold, plot=True): \u0026#34;\u0026#34;\u0026#34; Display and save the SeCAM images. :param SeCAMs: List of SeCAMs :param width: Width of the original image :param height: Height of the original image :param original_image: Original input image :param save_name: Name to save the output image :param secam_threshold: Threshold to mask significant regions :param plot: Whether to display the image \u0026#34;\u0026#34;\u0026#34; matplotlib.rcParams[\u0026#39;figure.figsize\u0026#39;] = 15, 12 for i, seCAM in enumerate(SeCAMs): seCAM = np.uint8(255 * seCAM) seCAM_resized = cv2.resize(seCAM, (width, height)) # Create a mask of the significant regions mask = seCAM_resized \u0026gt; (secam_threshold * seCAM_resized.max()) # Create a black background image black_bg = np.zeros_like(original_image) # Apply the mask to the original image result = original_image.copy() result[~mask] = black_bg[~mask] # Save the result cv2.imwrite(f\u0026#34;outputs/SeCAM_{save_name}_{i}.jpg\u0026#34;, result) # Display the result if plot: image = plt.imread(f\u0026#34;outputs/SeCAM_{save_name}_{i}.jpg\u0026#34;) plt.imshow(image) plt.axis(\u0026#39;off\u0026#39;) plt.show() matplotlib.rcParams['figure.figsize'] = 15, 12: Sets the figure size for matplotlib plots. for i, seCAM in enumerate(SeCAMs): Iterates over each SeCAM. seCAM = np.uint8(255 * seCAM): Scales the SeCAM values to the range [0, 255] and converts them to uint8 format. seCAM_resized = cv2.resize(seCAM, (width, height)): Resizes the SeCAM to match the original image size. mask = seCAM_resized \u0026gt; (secam_threshold * seCAM_resized.max()): Creates a mask for significant regions in the SeCAM. black_bg = np.zeros_like(original_image): Creates a black background image. result = original_image.copy(): Copies the original image. result[~mask] = black_bg[~mask]: Applies the mask to the original image, keeping only the significant regions. cv2.imwrite(f\u0026quot;outputs/SeCAM_{save_name}_{i}.jpg\u0026quot;, result): Saves the result image. image = plt.imread(f\u0026quot;outputs/SeCAM_{save_name}_{i}.jpg\u0026quot;): Reads the saved image. plt.imshow(image): Displays the image. plt.axis('off'): Hides the axis. plt.show(): Shows the image. This function overlays the SeCAM on the original image, masks insignificant regions, adds the class label, and saves the result. It also displays the final image.\nStep 3: Full Pipeline for Generating and Saving SeCAM # Finally, we can combine all the above steps into a single function to generate and save SeCAMs for the top K predictions of an input image.\ndef generate_and_save_seCAM(image_path, topk_predictions, num_segments=50, compactness=10, secam_threshold=0.8): \u0026#34;\u0026#34;\u0026#34; Generate and save SeCAM for the specified image and topk predictions. :param image_path: Path to the input image :param topk_predictions: Number of top predictions to generate SeCAM for :param num_segments: Number of segments for SLIC :param compactness: Compactness parameter for SLIC :param secam_threshold: Threshold to mask significant regions in SeCAM \u0026#34;\u0026#34;\u0026#34; # Load the class labels class_labels = load_class_labels(\u0026#39;LOC_synset_mapping.txt\u0026#39;) # Read and preprocess the image image_tensor, original_image, height, width = preprocess_image(image_path) # Load the model and extract features model, features_blobs, softmax_weights = load_model_and_extract_features() # Get the top k predictions class_indices = get_topk_predictions(model, image_tensor, topk_predictions) # Generate superpixels using SLIC segments = slic(original_image, n_segments=num_segments, compactness=compactness, start_label=1) # Generate SeCAM for the top predictions SeCAMs = generate_seCAM(features_blobs[0], softmax_weights, class_indices, segments) # Display and save the SeCAM results save_name = image_path.split(\u0026#39;/\u0026#39;)[-1].split(\u0026#39;.\u0026#39;)[0] + f\u0026#34;_seg_{num_segments}_ts_{secam_threshold}\u0026#34; display_and_save_seCAM(SeCAMs, width, height, original_image, save_name, secam_threshold) class_labels = load_class_labels('LOC_synset_mapping.txt'): Loads the class labels from a specified file. image_tensor, original_image, height, width = preprocess_image(image_path): Reads and preprocesses the input image, returning the preprocessed image tensor, original image, and image dimensions. model, features_blobs, softmax_weights = load_model_and_extract_features(): Loads the model and extracts the feature maps and softmax weights. class_indices, _ = get_topk_predictions(model, image_tensor, topk_predictions): Gets the top K predictions for the input image. segments = slic(original_image, n_segments=num_segments, compactness=compactness, start_label=1): Segments the input image into superpixels using the SLIC algorithm. SeCAMs = generate_seCAM(features_blobs[0], softmax_weights, class_indices, segments): Generates the SeCAMs for the top predictions. save_name = image_path.split('/')[-1].split('.')[0]: Extracts the base name of the image file for saving the results. display_and_save_seCAM(SeCAMs, width, height, original_image, class_indices, class_labels, save_name, secam_threshold): Displays and saves the SeCAM results. e) Examples of Work: # Here you can see the results of applying SeCAM to an image of a dogs using different number of segments and threshold values:\nThere is another example of applying SeCAM to an image of a tiger:\nConclusion # In this tutorial, we have explored the concepts of Class Activation Mapping (CAM) and Segmentation-based Class Activation Mapping (SeCAM) as Explainable AI (XAI) methods for understanding image classification models. We have seen how CAM highlights discriminative regions in an image that contribute to the model\u0026rsquo;s prediction and how SeCAM combines CAM with image segmentation to provide more interpretable and precise results. By visualizing the CAM and SeCAM results, we can gain insights into the decision-making process of Convolutional Neural Networks and understand the basis of their predictions. These XAI methods play a crucial role in making AI models more transparent, interpretable, and trustworthy in critical applications where human understanding is essential.\n"},{"id":1,"href":"/docs/groups/diffusion-lens-interpreting-text-encoders-in-text-to-image-pipelines-tuned-using-dreambooth/","title":"Diffusion Lens: Interpreting Text Encoders in Text-to-Image pipelines","section":"Docs","content":" Diffusion Lens: Interpreting Text Encoders in Text-to-Image pipelines # Authors: Ivan Golov, Roman Makeev\nTo see the implementation, visit our github project.\nIntroduction # In this work, we introduce an interpretable, end-to-end framework that enhances Stable Diffusion v1.5 model fine‑tuned via the DreamBooth method [1] to generate high‑fidelity, subject‑driven images from as few reference examples.\nWhile DreamBooth effectively personalizes generation by associating a unique rare token with the target concept, the internal process through which textual prompts are transformed into visual representations remains opaque. To bridge this gap, we integrate Diffusion Lens [2], a visualization technique that decodes the text encoder’s intermediate hidden states into images, producing a layer‑by‑layer sequence that illuminates how semantic concepts emerge and refine over the course of encoding.\nBy uniting DreamBooth’s subject‑specific fine‑tuning with Diffusion Lens’s interpretability during both training and inference, our framework not only delivers compelling, stylized outputs but also offers transparent, quantitative insights into the hierarchical construction of visual content from natural language descriptions.\nBackground # Section 1: DreamBooth Fine-Tuning # DreamBooth [1] adapts a pre-trained diffusion model to a new subject by fine-tuning on a few of images (typically 3–5) paired with a unique rare-token identifier. During training, the loss optimizes reconstruction under noise:\n\\[\\[ L = \\mathbb{E}_{x,c,\\epsilon,t}\\big[ w_t \\|\\hat x_{\\theta}(\\alpha_t x \u0026#43; \\sigma_t \\epsilon, c) - x\\|_2^2 \\big], \\]\\] where:\n$x$ is a ground-truth image of the subject. $c$ is the prompt embedding including the rare token. $\\epsilon \\sim \\mathcal{N}(0,I)$, and $\\alpha_t,\\sigma_t,w_t$ control the noise schedule and weighting. This process binds the new subject’s appearance and style to the rare token, enabling subject-driven generation without degrading the model’s generalization over the original class.\nSection 2: Diffusion Lens Interpretability # Diffusion Lens [2] visualizes how the text encoder builds up semantic concepts by decoding intermediate hidden states into images. For each encoder layer $\\ell$ (of $L$ layers), we apply the layer normalization then run the diffusion decoder:\n[ I_\\ell = \\mathrm{DiffusionDecode}\\big(\\mathrm{LN}(h_\\ell)\\big), ]\nwhere $h_\\ell$ is the hidden representation at layer $\\ell$. This yields a layer-by-layer image sequence revealing when and how different concepts (e.g., color, object shape, relations) emerge and refine.\nMethodology # We structured our investigation around two complementary analyses during both fine-tuning and inference:\nIntermediate Diffusion Outputs.\nWe instrument the denoising pipeline to snapshot the latent image at key timesteps. This reveals how noise is incrementally removed and how subject details (e.g., specific textures or lighting) become more pronounced.\nVisualizing Text Encoder Layers.\nBy hooking into the text encoder’s forward pass, we extract hidden states $h_\\ell$ at several checkpoints. Feeding these through the frozen diffusion decoder yields images that represent the semantic content captured at each layer.\nImplementation Highlights # Our codebase is organized as follows:\nGAI_course_project/ ├── dreambooth_finetune.py # Fine-tuning script using HuggingFace Diffusers ├── diffusion_lens.py # Utilities to extract hidden states and render them ├── inference.py # Combined pipeline for generation and visualization └── notebooks/ └── analysis.ipynb # Experimentation and plotting Below is a snippet demonstrating how we capture and decode hidden states:\nfrom diffusers import StableDiffusionPipeline from diffusion_lens import DiffusionLens # Load fine-tuned pipeline pipe = StableDiffusionPipeline.from_pretrained(\u0026#34;./fine_tuned_model\u0026#34;) pipe.enable_attention_slicing() # Initialize Diffusion Lens lens = DiffusionLens(pipe) # Prompt and extract layer images layer_images = lens.visualize_encoder_layers( prompt=\u0026#34;A stylized portrait of \u0026lt;rare_token\u0026gt;\u0026#34;, layers=[2, 5, 10, 16] ) # Save or display results for idx, img in enumerate(layer_images): img.save(f\u0026#34;lens_layer_{idx}.png\u0026#34;) Experiments and Analysis # We evaluated our approach on several subjects (e.g., unique objects, artistic portraits) using prompts that vary in complexity:\nSimple: “\u0026lt;rare_token\u0026gt;” Medium: “A colorful illustration of \u0026lt;rare_token\u0026gt; in a forest.” Complex: “\u0026lt;rare_token\u0026gt; performing a dynamic action under dramatic lighting.” Below are representative outputs demonstrating both DreamBooth personalization and the progressive concept refinement captured by Diffusion Lens.\nQualitatively, we observe:\nEarly layers capture coarse attributes (color palettes, rough shapes). Middle layers introduce structural details (object outlines, relations). Final layers refine fine-grained textures and lighting. This aligns with hypotheses about hierarchical feature construction in transformer-based encoders.\nConclusion # By combining DreamBooth fine-tuning with Diffusion Lens interpretability, we achieve not only high-fidelity, subject-driven image synthesis but also transparent insights into the model’s inner semantic processing. Our visualizations confirm that concepts emerge and sharpen progressively across encoder layers, and that rare-token personalization integrates seamlessly into this hierarchy.\nFuture Work. We plan to explore automated metrics for concept emergence timing and extend our lens to multimodal fusion layers in larger architectures.\nReferences # [1] N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein, and K. Aberman, Dreambooth: Fine tuning text- to-image diffusion models for subject-driven generation, 2023. arXiv: 2208.12242 [cs.CV]. [Online]. Available: https://arxiv.org/abs/2208.12242.\n[2] M. Toker, H. Orgad, M. Ventura, D. Arad, and Y. Belinkov, “Diffusion lens: Interpreting text encoders in text-to-image pipelines,” Association for Computational Linguistics, 2024, pp. 9713–9728. doi: 10.18653/v1/2024.acl-long.524. [Online]. Available: http://dx.doi.org/10.18653/v1/2024.acl-long.524.\n"},{"id":2,"href":"/docs/groups/dimensionality-reduction-in-nlp-visualizing-sentence-embeddings-with-umap-and-t-sne/","title":"Dimensionality Reduction in NLP: Visualizing Sentence Embeddings with UMAP and t-SNE","section":"Docs","content":" Dimensionality Reduction in NLP: Visualizing Sentence Embeddings with UMAP and t-SNE # Students: # Yazan Kbaili ( y.kbaili@innopolis.university) Hamada Salhab ( h.salhab@innopolis.university) Introduction # This report delves into the visualization of sentence embeddings derived from the roberta-base model fine-tuned on the \u0026ldquo;go-emotion\u0026rdquo; dataset using two prominent dimensionality reduction techniques: UMAP (Uniform Manifold Approximation and Projection) and t-SNE (t-distributed Stochastic Neighbor Embedding). The dataset used in the visualization consists of Twitter messages and is called “emotions”.\nMethodology # The embeddings were generated by the roberta-base model, specifically tailored for multiclass emotion classification and accessible via Hugging Face. This model is particularly adept at interpreting emotional contexts within text, making it highly suitable for our study. We focus on embeddings from Twitter messages categorized into six distinct emotions.\nUMAP # Operates on a foundation of algebraic topology, creating a high-dimensional graph representation of data before optimizing this layout in a lower-dimensional space. It aims to preserve both local and global structures, offering a comprehensive data understanding. UMAP is generally preferred for its ability to maintain a more global structure compared to t-SNE.\nt-SNE # Transforms high-dimensional Euclidean distances between points into conditional probabilities that reflect similarities, excelling in the preservation of local data structures and the identification of clusters. While effective, t-SNE can be computationally demanding, especially with large datasets, and may exaggerate cluster separations without maintaining global data integrity.\nUMAP vs t-SNE # The behavior of t-SNE and UMAP differs significantly in terms of initialization and the iterative process used to optimize the low-dimensional representation. Below are some of the key differences:\nFeature t-SNE UMAP Initialization Starts with a random initialization of the graph. Uses Spectral Embedding for deterministic initialization. Iteration Process Moves every single point slightly each iteration. Can move just one point or a small subset of points each time. Scalability Less efficient with very large datasets. Scales well with large datasets due to its partial update approach. Extraction of Embeddings # We utilized the embedding from the last token at the last layer of the pre-trained model, which typically encapsulates the sentence\u0026rsquo;s contextual essence.\nVisualization # For an interactive visual representation, we employed Plotly, enabling detailed exploration of the structures within the embeddings.\nt-SNE # 1: 2: UMAP # 1: 2: Insights # The visualizations above show that the dimensionality reduction methods used did a reasonably good job with clustering the different labels of data. Note that the model is trained on a different dataset, and has never seen the data we tried it, which explain the limiatation and\nExtra Insights # Let\u0026rsquo;s take a look at this 2D visualization from UMAP:\nWe zoomed in to the cluster highlighted in red, and found out that altough it contains sentences that belong to all labels, all of them convey some sort of regret/guilt. We took another look at the cluster highlighted in yellow, and saw that all the sentences there talk about humiliation/disgrace. As for the cluster highlighted in black, all the sentences conveyed meaning for the purpose of thanking and appreciation. Applications # Model Debugging and Improvement: Identifies anomalies or biases in embeddings, facilitating targeted improvements to the model. Semantic Analysis: Assists in understanding how sentences are clustered semantically, which helps in tasks such as sentiment analysis. Transfer Learning Insights: Offers insights into the transferability of learned features, especially in domain-specific applications. Multilingual Comparisons: Evaluates the model’s capability across different languages, which helps identify potential biases or gaps. Explainability and Trust: Increases the transparency of NLP systems, and builds trust among end-users and regulators by making complex models more interpretable. Colab Notebook # The source code for this project can be found in this Colab Notebook.\nPresentation Slides # The presentation slides for this project can be found on Google Slides.\n"},{"id":3,"href":"/docs/groups/example/","title":"Example","section":"Docs","content":" Presentation # About us # Examplary post here\n"},{"id":4,"href":"/docs/groups/ai-playing-geoguessr-explained/","title":"Ai Playing Geo Guessr Explained","section":"Docs","content":" AI playing GeoGuessr explained # Author: Pavel Roganin\nPrerequisites to read # None\nIntroduction # Everyone has played GeoGuessr at least once in their life. This is how the game looks like:\nIf you do not remember this game, I will briefly explain the rules of the game.\nThis is a simple browser game that selects a random location from around the world and shows the player interactive Google Street View panoramas of that location. The player can move through the streets and look around. The player’s task is to determine where he is and point the expected location on the world map. The closer the location guessed by the player is to the real one, the more points he receives.\nThis game can be played alone or with other players. You can limit the game area to a specific continent or country, and you can play a number of different game modes.\nAt the end of the game, GeoGuessr shows your score, your guesses, and how far they are from the actual locations. The game is actually quite challenging. My results usually look like this:\nNot even once I guessed a right country, let alone being close to the real location.\nBut there are players whose result looks like this:\nYou know what is much more interesting than the score? This is a screenshot from the YouTube video: geoguessr pro guesses in 0.1 seconds …\nNeural Networks # In this video, the person doesn\u0026rsquo;t even look around but can guess the country from a picture shown for just a fraction of a second.\nClearly, he doesn\u0026rsquo;t have time to look into details and think. This means his brain is so well-trained that he doesn’t need to consciously think. He just makes a guess based on his enormous experience in this game.\nThis is very similar to how Neural Networks (NN) work. NNs are actually inspired by the structure and function of biological neural networks. Essentially, it is a network of neurons that are connected and send information to each another.\nNeural networks also learn from experience, just like the human brain. Due to this, neural network is a very powerful and widely used model in Machine Learning.\nHowever, it\u0026rsquo;s worth noting a difference between how RAINBOLT (the player from the video) learned to play at this level and how NN learns. We will come to this in a moment.\nImportant:\nBecause RAINBOLT proved that you do not need to look around in the game to achieve great results, we will simplify the game to just one picture upon which we are supposed to guess a country. This way it will be much easier to use a neural network.\nHigh level view on structure of a neural network for pictures\nNN might have the following inner dialogue when playing and learning the simplified game:\nAt the very beginning (zero experience):\n“They showed me picture and said that I have to guess a country… How am I supposed to guess, I know nothing! But I have to say something… Okay, let it be Slovakia!”\nCorrect answer: Finland.\n“How am I supposed to know?”\nLater (already having some experience):\n“I see something (hieroglyphics) that I saw before, and the correct answer was China… So, I say China this time.”\nCorrect answer: Japan.\n“How? There must be something else that distinguishes China and Japan… How about the streets? Last time they were wide, and this time they are very narrow… Okay, next time if I see hieroglyphics I’ll look at the streets. If they are wide, I\u0026rsquo;ll guess China; if narrow, then Japan.”\nChina\nJapan\nNow, how is human learning different? Humans have more sources of information available. RAINBOLT could asked his friend who also plays GeoGuessr for some insights (for example, what to play attention to when seeing hieroglyphics). That way, he would already know some facts without needing to play many games to figure them out through trial and error.\nDoing the same for a neural network would enormously complicate its design, but it is in fact not needed. Neural networks have an advantage: processing speed. Humans cannot compete with computer in processing speed. An average human finishes one game in 100 seconds, while neural networks can play 100 games per second. This is enough for NN to learn just from experience without extra help.\nTraining a Neural Network # As mentioned earlier, a neural network will learn just by looking at the pictures. Therefore, we need to gather pictures from different countries featured in the game.\nFortunately, there is a collection of these pictures (dataset) available on Kaggle.\nWe will use DenseNet121, a Dense Convolutional Neural Network (Dense CNN), for this task.\nWe\u0026rsquo;ll train and use the model on Kaggle because it provides 29 GB of RAM, whereas Google Colab offers only 13 GB, which is insufficient for efficient model training.\nTraining the model on Google Colab with batch size = 2 (amount of images the model trains on simultaneously)\nAdditionally, Google Colab offers only one GPU, while Kaggle provides two GPUs, which boosts training performance by 1.5x.\nComparison of 1 GPU vs 2 GPUs\nUtilizations of resources and training performance for one GPU on Kaggle (batch size = 32):\nUtilizations of resources and training performance for two GPUs on Kaggle (batch size = 32):\nThe code used for training was originally written by ANIMESH SINGH BASNET.\nI have made adjustments to the model architecture and achieved 52% accuracy, compared to the original 43%.\nThe model was successfully trained for 30 epochs over 3 hours using two GPUs on Kaggle.\nExplaining the model # We are going to use LIME method for explanations. LIME method provides insights into the decision-making process of complex models by approximating them with simpler, more interpretable models.\nHere is an example of how the explanations look:\nOur model works on images with a resolution of 224x224. This resolution is convenient for the model because it contains enough information for pattern recognition while greatly reducing computation times.\nThe first picture shows the original image, its actual class (the country depicted), and the class predicted by our model. The second picture highlights what the model focused on when making its prediction.\nAs you can see, the grass made the model think that the country is Argentina. You might wonder, how does that make sense? This is the same type of question you could ask RAINBOLT: how does he guess the right country by looking at a random picture in the middle of nowhere?\nLet\u0026rsquo;s look at another explanation:\nThis time, the model focused on the pavement. Yet this was sufficient for the model to guess correctly.\nLet\u0026rsquo;s look at a couple more explanations:\nHowever, the model is not perfect and sometimes makes incorrect guesses. This might be due to cases like the one in image 5 above. In this case, the model took into consideration the map, which never changes in any of the pictures and doesn’t give any useful information. This suggests the model needs more training to recognize that focusing on the map is not helpful.\nConclusion # Neural networks utilize a very powerful mechanism designed by nature to be perfect, making them powerful tools for solving a wide range of tasks. Despite their complexity, there are methods that allow us to peak into neural networks’ mind. One of them is LIME.\nTo demonstrate the method, we formulated a problem of guessing a country by looking at an image inspired by the game GeoGuessr. We explored the human capabilities in solving this problem, considered the advantages and disadvantages humans have over neural networks. Next, we trained a neural network on Kaggle and used LIME to understand model’s decision making and faults in its decision making.\n"},{"id":5,"href":"/docs/groups/contrastive-grad-cam-consistency/","title":"Contrastive Grad Cam Consistency","section":"Docs","content":" Consistent Explanations by Contrastive Learning # Introduction: Unveiling the Black Box of Deep Learning # Demystifying Decisions with Post-hoc Explanations # Post-hoc explanation methods are techniques used to interpret and explain the decisions made by the model after they have been trained\nThese methods, such as CAM, Grad-CAM, and FullGrad, typically generate heatmaps highlighting the image regions that were most influential for the model\u0026rsquo;s prediction. High values correspond to the regions that took important role in the network\u0026rsquo;s decision.\nThe Challenge of Spatial Transformations # However, many interpretation methods falter when faced with spatial transformations of images. Shifting, zooming, rotating, or shearing an image can significantly alter the explanation heatmap, even though the image content remains essentially the same. This inconsistency raises concerns about the reliability and robustness of these explanation methods.\nFine-Grained Classification: A Case for Explainability # Authors of the paper pay special attention to the fine-grained image classification task, where the goal is to distinguish between subtle differences within a broader category. For instance, we might want to classify different breeds of dogs or species of birds. In such tasks, understanding the model\u0026rsquo;s reasoning becomes crucial for building trust and ensuring fairness.\nSuggested solution: Enhancing Grad-CAM for Spatial Consistency # Authors propose an approach to improve Grad-CAM, making its explanations more stable across spatial transformations. Inspired by contrastive self-supervised learning, they introduce a novel loss function that leverages unlabeled data during training.\nKey ideas: Guiding Principles for Explainability # Adopt ideas from contrastive self-supervised learning and design a loss function that will allow to train on unlabeled data.\nThe loss function encourages two key properties:\nConsistency: The Grad-CAM heatmap of an image should be similar to the heatmap of its augmented versions (e.g., zoomed, rotated). Distinctiveness: The Grad-CAM heatmap of an image should be different from the heatmaps of other, random images. This approach ensures that the explanations focus on the inherent content of the image rather than its spatial arrangement.\nMetrics # To assess the quality of the explanations, authors utilize classification accuracy, Content Heatmaps and Contrastive Grad-CAM Consistency Loss.\nClassification Accuracy # Authors utilize classification accuracy during training process. We will use it as our main metrics along with CGC Loss during evaluation since we do not have annotated samples to calculate Content Heatmap scores.\nContent Heatmap # Content Heatmaps are annotated by humans. They indicate the regions of importance within an image. By comparing the model-generated heatmaps with the Content Heatmaps, it is possible to evaluate the accuracy and faithfulness of the explanations.\nCGC Loss # Contrastive Grad-CAM Consistency Loss is also used by authors to identify that proposed method generalizes to unseen data as well.\nInsights and Benefits # The proposed method\u0026rsquo;s authors state that it demonstrates several advantages:\nImproved Accuracy: It leads to better performance in fine-grained classification tasks. Unlabeled Data Utilization: It can leverage the abundance of unlabeled data for training. Enhanced Consistency: It generates explanations that are more robust to spatial transformations. Regularization Effect: It acts as a regularizer, leading to better generalization and performance even with limited labeled data. Method # Background on Base Explanation Methods # CAM # Definition # Class Activation Mapping, or CAM for short, is a special technique used in computer vision and machine learning. It helps us understand how a Convolutional Neural Network (CNN) makes decisions when classifying images. Basically, CAM lets us visualize which parts of an image are most important for the network\u0026rsquo;s prediction.\nHow does CAM work? # Modify the CNN: We take a trained CNN and remove the fully connected layers at the end. Instead, we add a Global Average Pooling (GAP) layer after the last convolutional layer. Global Average Pooling: GAP takes each feature map from the last convolutional layer and calculates the average of all values within that map. This results in a single representative value for each feature map. Prediction with a single layer: These averaged values are then fed into a single fully connected layer with as many outputs as there are classes in our problem. This layer learns to predict the image class based on the feature map averages. Weights and Importance: The weights in this final layer tell us how important each feature map is for each class. Creating the heatmap: We multiply these weights with the corresponding feature maps from the last convolutional layer. This creates a weighted sum for each class, highlighting the regions in the feature maps that were most influential for that class. Visualization: These weighted sums are then visualized as heatmaps, showing which parts of the image contributed most to the predicted class. This heatmap provides a visual explanation of the CNN\u0026rsquo;s decision-making process. Grad-CAM # Grad-CAM, which stands for Gradient-weighted Class Activation Mapping, is another technique similar to CAM that helps us visualize what a CNN is \u0026ldquo;looking at\u0026rdquo; when making a prediction.\nOne key advantage of Grad-CAM is that it doesn\u0026rsquo;t require modifying the original CNN architecture, unlike CAM which needs the Global Average Pooling layer. This makes Grad-CAM applicable to a wider range of CNN models.\nHow does Grad-CAM work? # Forward Pass and Prediction: We start by feeding an image into the CNN and obtaining the class prediction. Gradient Calculation: We then calculate the gradient of the score for the predicted class with respect to the feature maps of the last convolutional layer. This tells us how much each feature map activation influences the final prediction score. ReLU and Global Average Pooling: We apply a ReLU function to the gradients to focus on the features that have a positive influence on the class score. Then, for each feature map, we take the average of these positive gradients across all the spatial locations in that map. This gives us a single value representing the importance of each feature map for the predicted class. Weighted Combination: We then use these averaged gradients as weights and combine them with the corresponding feature maps from the last convolutional layer. This creates a weighted sum that highlights the important regions in the feature maps for the predicted class. Visualization as Heatmap: Finally, we visualize this weighted sum as a heatmap, superimposed on the original image. This heatmap shows which parts of the image were most influential in the CNN\u0026rsquo;s decision-making process. Contrastive Grad-CAM Consistency Loss # We want the transformed interpretation of a query image to be close to the interpretation of the transformed query while being far from interpretations of other random images\nTo understand the main formula, let\u0026rsquo;s first define the key elements involved:\n\\(g(\\cdot)\\) : Grad-CAM operator that produces interpretation heatmap \\(\\{X_j\\}^n_{j=1}\\) : Set of \\(n\\) random images \\(\\Tau_j(\\cdot)\\) : Independent random spacial transformation. This transformation could involve scaling, cropping, and/or flipping the image. \\(\\{g_j(T_j(j_j))\\}^n_{j=1}\\) : Grad-CAM heatmaps of the augmented images \\(x_i\\ where\\ i\\in 1..n\\) : Query image \\(\\Tau_i(g_i(x_i))\\) : We apply the same transformation we had applied to \\(x_i\\) to the Grad-CAM heatmap instead of the image We want \\(\\Tau_i(g_i(x_i))\\) to be close to \\(g_i(\\Tau_i(x_i))\\) and far from \\(\\{g_j(\\Tau_j(x_j))\\}_{j\\neq i}\\) Hence, we define the following loss function:\n\\[L_i = -\\log{\\frac{\\exp(\\text{sim}(\\Tau_i(g_i(x_i))), g_i(\\Tau_i(x_i))/\\tau)}{\\sum^n_{j=1}\\exp(\\text{sim}(\\Tau_i(g_i(x_i))), g_j(\\Tau_j(x_j)))}}\\] where \\(\\tau\\) is the temperature hyperparameter and \\(\\text{sim}(a, b)\\) is a similarity function. In the experiments cosine similarity was used.\nSince we want to optimize training we will assume that each image is the query once in one mini-batch. Thus we define our contrastive Grad-CAM consistency loss as: \\[L_{CGC} = \\sum_i{L_i}\\] Our final loss will be a combination of the cross entropy loss with defined consistency loss: \\[L = L_{CE} \u0026#43; \\lambda L_{CGC}\\] where \\(\\lambda\\) is a hyperparameter that controls trade-off between usual supervised way of training and self-supervised method that allows to train our model on unlabeled data using pseudo labels (image labels).\nHere you can see visual description of the approach described above: Our Implementation # Google Colab Notebook # This section describes our implementation of the CGC method using PyTorch. Here\u0026rsquo;s a concise overview:\nData Loading and Transformation:\nWe use the Imagenette dataset for demonstration purposes (CUB dataset is also available in the code). A custom ContrastiveTransforms class handles data augmentation, including random resized cropping and horizontal flips. Data loaders are set up for both labeled and unlabeled data. Model Definition:\nThe CGC_Model class utilizes a ResNet-18 backbone. A forward hook is applied to the last convolutional layer (layer4) to extract feature maps for Grad-CAM computation. The forward pass implements the CGC logic, including Grad-CAM calculation and augmentation. Loss Functions and Optimization:\nWe employ cross-entropy loss for supervised learning. The NCESoftmaxLoss (info-NCE) encourages consistency and distinctiveness in Grad-CAM heatmaps. SGD optimizer with momentum and weight decay is used. Training Loop:\nThe train function iterates through epochs and mini-batches, performing both supervised and contrastive learning. It tracks cross-entropy loss, contrastive loss, and top-1/top-5 accuracies. Grad-CAM Visualization:\nAfter training, the model is loaded and used to generate Grad-CAM heatmaps for sample images. A display_gradcam function visualizes the original images, Grad-CAM masks, and the superimposed results. Model Saving:\nThe trained model\u0026rsquo;s state is saved for future use. Resources # Original CGC Paper Original CGC Implementation CAM Explanation Grad-CAM Explanation "},{"id":6,"href":"/docs/groups/dndfs_shap/","title":"Dndfs Shap","section":"Docs","content":" Deep Neural Decision Forests (DNDFs) with SHAP Values # Introduction # Deep Neural Decision Forests (DNDFs) combine the interpretability and robustness of decision trees with the power of neural networks to capture complex patterns in data. This integration allows DNDFs to perform well on various tasks, especially in high-dimensional spaces where traditional methods may struggle.\nThe method is different from random forests in that it uses a principled, joint, and global optimization of split and leaf node parameters and from conventional deep networks because a decision forest provides the final predictions.\nFormulas Involved # The final probability of an observation belonging to a class is the aggregated probability of that observation belonging to a class in each leaf node. The aggregation is done using a weighted sum, where the probability of the observation reaching the corresponding leaf is taken as weight. From the paper, the actual formula is as below:\nProbability of an observation x belonging to class y Learning Process # The training of the model is done in two stages. Starting from a randomly initiated set of class probabilities for each node, iteratively update 𝜋 and µ for a predefined number of epochs.\nDataset # Data Description: There are three types of input features:\nObjective: factual information Examination: results of medical examination Subjective: information given by the patient Features:\nAge | int (days) Height | int (cm) Weight | float (kg) Gender | categorical code Systolic blood pressure | int Diastolic blood pressure | int Cholesterol | 1: normal, 2: above normal, 3: well above normal Glucose | 1, 2, 3 Smoking | binary Alcohol intake | binary Physical activity | binary Presence or absence of cardiovascular disease | binary Dataset can be found here.\nModel Description # We based our implementation on the DeepNeuralForest model available on GitHub, which is inspired by a paper on neural decision forests. We adapted the model and the training loop from this source. Specifically, we modified one of the layers in the model to better suit our dataset and added a custom class to handle our specific dataset.\nThe paper can be accessed here.\nModel Components # Feature Extraction Layer: This layer is built using fully connected neural networks with ReLU activations and dropout for regularization. Decision Forest: This component consists of multiple decision trees, each trained on a random subset of the features. Model structure for tabular data Implementation # Dataset Class # class CardioDataset(Dataset): \u0026#34;\u0026#34;\u0026#34;Custom dataset class for handling the cardiovascular disease dataset.\u0026#34;\u0026#34;\u0026#34; def __init__(self, df): \u0026#34;\u0026#34;\u0026#34; Args: df (pd.DataFrame): DataFrame containing the dataset. \u0026#34;\u0026#34;\u0026#34; self.data = df self.X = self.data.iloc[:, 1:-1].values.astype(np.float32) # Features self.y = self.data.iloc[:, -1].values.astype(np.int64) # Labels # Normalize the features self.X = (self.X - self.X.mean(axis=0)) / (self.X.std(axis=0) + 1e-6) def __len__(self): \u0026#34;\u0026#34;\u0026#34;Returns the total number of samples in the dataset.\u0026#34;\u0026#34;\u0026#34; return len(self.data) def __getitem__(self, idx): \u0026#34;\u0026#34;\u0026#34; Retrieves the feature and label for a given index. Args: idx (int): Index of the sample to retrieve. Returns: tuple: (feature, label) \u0026#34;\u0026#34;\u0026#34; return self.X[idx], self.y[idx] def prepare_db(csv_file): \u0026#34;\u0026#34;\u0026#34; Prepares the training, validation, and test datasets. Args: csv_file (str): Path to the CSV file containing the dataset. Returns: dict: Dictionary containing the training, validation, and test datasets. \u0026#34;\u0026#34;\u0026#34; df = pd.read_csv(csv_file, sep=\u0026#39;;\u0026#39;) train_df, test_df = train_test_split(df, test_size=0.2, random_state=0) train_df, val_df = train_test_split(train_df, test_size=0.25, random_state=0) # 0.25 * 0.8 = 0.2 train_dataset = CardioDataset(train_df) val_dataset = CardioDataset(val_df) test_dataset = CardioDataset(test_df) return {\u0026#39;train\u0026#39;: train_dataset, \u0026#39;val\u0026#39;: val_dataset, \u0026#39;test\u0026#39;: test_dataset, \u0026#39;test_df\u0026#39;: test_df} Feature Layer # class CardioFeatureLayer(nn.Sequential): \u0026#34;\u0026#34;\u0026#34;Feature extraction layer using fully connected neural networks with dropout.\u0026#34;\u0026#34;\u0026#34; def __init__(self, dropout_rate, shallow=False): \u0026#34;\u0026#34;\u0026#34; Args: dropout_rate (float): Dropout rate for regularization. shallow (bool, optional): Whether to use a shallow network. Defaults to False. \u0026#34;\u0026#34;\u0026#34; super(CardioFeatureLayer, self).__init__() self.add_module(\u0026#39;linear1\u0026#39;, nn.Linear(11, 1024)) self.add_module(\u0026#39;relu1\u0026#39;, nn.ReLU()) self.add_module(\u0026#39;dropout1\u0026#39;, nn.Dropout(dropout_rate)) self.add_module(\u0026#39;linear2\u0026#39;, nn.Linear(1024, 1024)) self.add_module(\u0026#39;relu2\u0026#39;, nn.ReLU()) self.add_module(\u0026#39;dropout2\u0026#39;, nn.Dropout(dropout_rate)) def get_out_feature_size(self): \u0026#34;\u0026#34;\u0026#34;Returns the output feature size of the layer.\u0026#34;\u0026#34;\u0026#34; return 1024 Tree and Forest Classes # class Tree(nn.Module): \u0026#34;\u0026#34;\u0026#34;Tree class for building a single decision tree.\u0026#34;\u0026#34;\u0026#34; def __init__(self, depth, n_in_feature, used_feature_rate, n_class, jointly_training=True): \u0026#34;\u0026#34;\u0026#34; Args: depth (int): Depth of the tree. n_in_feature (int): Number of input features. used_feature_rate (float): Fraction of features to use. n_class (int): Number of classes. jointly_training (bool, optional): Whether to use joint training. Defaults to True. \u0026#34;\u0026#34;\u0026#34; super(Tree, self).__init__() self.depth = depth self.n_leaf = 2 ** depth self.n_class = n_class self.jointly_training = jointly_training n_used_feature = int(n_in_feature * used_feature_rate) onehot = np.eye(n_in_feature) using_idx = np.random.choice(np.arange(n_in_feature), n_used_feature, replace=False) self.feature_mask = onehot[using_idx].T self.feature_mask = Parameter(torch.from_numpy(self.feature_mask).type(torch.FloatTensor), requires_grad=False) if jointly_training: self.pi = np.random.rand(self.n_leaf, n_class) self.pi = Parameter(torch.from_numpy(self.pi).type(torch.FloatTensor), requires_grad=True) else: self.pi = np.ones((self.n_leaf, n_class)) / n_class self.pi = Parameter(torch.from_numpy(self.pi).type(torch.FloatTensor), requires_grad=False) self.decision = nn.Sequential(OrderedDict([ (\u0026#39;linear1\u0026#39;, nn.Linear(n_used_feature, self.n_leaf)), (\u0026#39;sigmoid\u0026#39;, nn.Sigmoid()), ])) def forward(self, x): \u0026#34;\u0026#34;\u0026#34; Forward pass for the tree. Args: x (torch.Tensor): Input tensor. Returns: torch.Tensor: Output tensor after passing through the tree. \u0026#34;\u0026#34;\u0026#34; if x.is_cuda and not self.feature_mask.is_cuda: self.feature_mask = self.feature_mask.cuda() feats = torch.mm(x, self.feature_mask) decision = self.decision(feats) decision = torch.unsqueeze(decision, dim=2) decision_comp = 1 - decision decision = torch.cat((decision, decision_comp), dim=2) batch_size = x.size()[0] _mu = Variable(x.data.new(batch_size, 1, 1).fill_(1.)) begin_idx = 1 end_idx = 2 for n_layer in range(0, self.depth): _mu = _mu.view(batch_size, -1, 1).repeat(1, 1, 2) _decision = decision[:, begin_idx:end_idx, :] _mu = _mu * _decision begin_idx = end_idx end_idx = begin_idx + 2 ** (n_layer + 1) mu = _mu.view(batch_size, self.n_leaf) return mu def get_pi(self): \u0026#34;\u0026#34;\u0026#34;Returns the class probabilities for the leaf nodes.\u0026#34;\u0026#34;\u0026#34; if self.jointly_training: return F.softmax(self.pi, dim=-1) else: return self.pi def cal_prob(self, mu, pi): \u0026#34;\u0026#34;\u0026#34;Calculates the probability of the input belonging to each class.\u0026#34;\u0026#34;\u0026#34; p = torch.mm(mu, pi) return p def update_pi(self, new_pi): \u0026#34;\u0026#34;\u0026#34;Updates the class probabilities for the leaf nodes.\u0026#34;\u0026#34;\u0026#34; self.pi.data = new_pi class Forest(nn.Module): \u0026#34;\u0026#34;\u0026#34;Forest class for building a decision forest.\u0026#34;\u0026#34;\u0026#34; def __init__(self, n_tree, tree_depth, n_in_feature, tree_feature_rate, n_class, jointly_training): \u0026#34;\u0026#34;\u0026#34; Args: n_tree (int): Number of trees in the forest. tree_depth (int): Depth of each tree. n_in_feature (int): Number of input features. tree_feature_rate (float): Fraction of features to use for each tree. n_class (int): Number of classes. jointly_training (bool): Whether to use joint training. \u0026#34;\u0026#34;\u0026#34; super(Forest, self).__init__() self.trees = nn.ModuleList() self.n_tree = n_tree for _ in range(n_tree): tree = Tree(tree_depth, n_in_feature, tree_feature_rate, n_class, jointly_training) self.trees.append(tree) def forward(self, x): \u0026#34;\u0026#34;\u0026#34; Forward pass for the forest. Args: x (torch.Tensor): Input tensor. Returns: torch.Tensor: Output tensor after passing through the forest. \u0026#34;\u0026#34;\u0026#34; probs = [] for tree in self.trees: mu = tree(x) p = tree.cal_prob(mu, tree.get_pi()) probs.append(p.unsqueeze(2)) probs = torch.cat(probs, dim=2) prob = torch.sum(probs, dim=2) / self.n_tree return prob Neural Decision Forest Class # class NeuralDecisionForest(nn.Module): \u0026#34;\u0026#34;\u0026#34;Neural Decision Forest class combining the feature extraction layer and the forest.\u0026#34;\u0026#34;\u0026#34; def __init__(self, feature_layer, forest): \u0026#34;\u0026#34;\u0026#34; Args: feature_layer (nn.Module): Feature extraction layer. forest (nn.Module): Decision forest. \u0026#34;\u0026#34;\u0026#34; super(NeuralDecisionForest, self).__init__() self.feature_layer = feature_layer self.forest = forest def forward(self, x): \u0026#34;\u0026#34;\u0026#34; Forward pass for the neural decision forest. Args: x (torch.Tensor): Input tensor. Returns: torch.Tensor: Output tensor after passing through the neural decision forest. \u0026#34;\u0026#34;\u0026#34; out = self.feature_layer(x) out = out.view(x.size()[0], -1) out = self.forest(out) return out Training and Evaluation Functions # def prepare_model(opt): \u0026#34;\u0026#34;\u0026#34; Prepares the neural decision forest model. Args: opt (dict): Dictionary containing model options. Returns: nn.Module: Neural decision forest model. \u0026#34;\u0026#34;\u0026#34; feat_layer = CardioFeatureLayer(opt[\u0026#39;feat_dropout\u0026#39;]) forest = Forest(n_tree=opt[\u0026#39;n_tree\u0026#39;], tree_depth=opt[\u0026#39;tree_depth\u0026#39;], n_in_feature=feat_layer.get_out_feature_size(), tree_feature_rate=opt[\u0026#39;tree_feature_rate\u0026#39;], n_class=opt[\u0026#39;n_class\u0026#39;], jointly_training=opt[\u0026#39;jointly_training\u0026#39;]) model = NeuralDecisionForest(feat_layer, forest) if opt[\u0026#39;cuda\u0026#39;]: model = model.cuda() else: model = model.cpu() return model def prepare_optim(model, opt): \u0026#34;\u0026#34;\u0026#34; Prepares the optimizer for training. Args: model (nn.Module): Neural decision forest model. opt (dict): Dictionary containing optimization options. Returns: torch.optim.Optimizer: Optimizer. \u0026#34;\u0026#34;\u0026#34; params = [p for p in model.parameters() if p.requires_grad] return torch.optim.Adam(params, lr=opt[\u0026#39;lr\u0026#39;], weight_decay=1e-5) def train(model, optim, db, opt): \u0026#34;\u0026#34;\u0026#34; Trains the neural decision forest model. Args: model (nn.Module): Neural decision forest model. optim (torch.optim.Optimizer): Optimizer. db (dict): Dictionary containing the datasets. opt (dict): Dictionary containing training options. \u0026#34;\u0026#34;\u0026#34; best_val_loss = float(\u0026#39;inf\u0026#39;) for epoch in range(1, opt[\u0026#39;epochs\u0026#39;] + 1): if not opt[\u0026#39;jointly_training\u0026#39;]: print(\u0026#34;Epoch %d : Two Stage Learning - Update PI\u0026#34; % (epoch)) cls_onehot = torch.eye(opt[\u0026#39;n_class\u0026#39;]) feat_batches = [] target_batches = [] train_loader = DataLoader(db[\u0026#39;train\u0026#39;], batch_size=opt[\u0026#39;batch_size\u0026#39;], shuffle=True) with torch.no_grad(): for batch_idx, (data, target) in tqdm(enumerate(train_loader), total=len(train_loader), desc=\u0026#34;Updating PI\u0026#34;): if opt[\u0026#39;cuda\u0026#39;]: data, target, cls_onehot = data.cuda(), target.cuda(), cls_onehot.cuda() data = Variable(data) feats = model.feature_layer(data) feats = feats.view(feats.size()[0], -1) feat_batches.append(feats) target_batches.append(cls_onehot[target]) for tree in model.forest.trees: mu_batches = [] for feats in feat_batches: mu = tree(feats) mu_batches.append(mu) for _ in range(20): new_pi = torch.zeros((tree.n_leaf, tree.n_class)) if opt[\u0026#39;cuda\u0026#39;]: new_pi = new_pi.cuda() for mu, target in zip(mu_batches, target_batches): pi = tree.get_pi() prob = tree.cal_prob(mu, pi) pi = pi.data prob = prob.data mu = mu.data _target = target.unsqueeze(1) _pi = pi.unsqueeze(0) _mu = mu.unsqueeze(2) _prob = torch.clamp(prob.unsqueeze(1), min=1e-6, max=1.) _new_pi = torch.mul(torch.mul(_target, _pi), _mu) / _prob new_pi += torch.sum(_new_pi, dim=0) new_pi = F.softmax(Variable(new_pi), dim=1).data tree.update_pi(new_pi) model.train() train_loader = DataLoader(db[\u0026#39;train\u0026#39;], batch_size=opt[\u0026#39;batch_size\u0026#39;], shuffle=True) running_loss = 0.0 correct = 0 total = 0 with tqdm(total=len(train_loader), desc=f\u0026#39;Epoch {epoch}\u0026#39;) as pbar: for batch_idx, (data, target) in enumerate(train_loader): if opt[\u0026#39;cuda\u0026#39;]: data, target = data.cuda(), target.cuda() data, target = Variable(data), Variable(target) optim.zero_grad() output = model(data) loss = F.nll_loss(torch.log(output + 1e-6), target) # Add small epsilon to prevent NaNs loss.backward() optim.step() running_loss += loss.item() pred = output.data.max(1, keepdim=True)[1] correct += pred.eq(target.data.view_as(pred)).cpu().sum().item() total += target.size(0) pbar.set_postfix({\u0026#39;loss\u0026#39;: running_loss / (batch_idx + 1), \u0026#39;accuracy\u0026#39;: correct / total}) pbar.update(1) val_loss, val_accuracy = evaluate(model, db[\u0026#39;val\u0026#39;], opt, desc=\u0026#34;Validating\u0026#34;) print(f\u0026#39;\\nValidation set: Average loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}\\n\u0026#39;) # Log metrics to wandb wandb.log({ \u0026#39;epoch\u0026#39;: epoch, \u0026#39;train_loss\u0026#39;: running_loss / len(train_loader), \u0026#39;train_accuracy\u0026#39;: 100. * correct / total, \u0026#39;val_loss\u0026#39;: val_loss, \u0026#39;val_accuracy\u0026#39;: val_accuracy }) # Save the best model if val_loss \u0026lt; best_val_loss: best_val_loss = val_loss torch.save(model.state_dict(), \u0026#39;best_model_7.pth\u0026#39;) # Load the best model and make predictions on the test set model.load_state_dict(torch.load(\u0026#39;best_model_7.pth\u0026#39;)) test_loss, test_accuracy = evaluate(model, db[\u0026#39;test\u0026#39;], opt, desc=\u0026#34;Testing\u0026#34;) print(f\u0026#39;\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {test_accuracy:.4f}\\n\u0026#39;) # Log test metrics to wandb wandb.log({ \u0026#39;test_loss\u0026#39;: test_loss, \u0026#39;test_accuracy\u0026#39;: test_accuracy }) # Predict on the test set and print results predictions = predict(model, db[\u0026#39;test\u0026#39;], opt) print(f\u0026#39;Test set predictions:\\n {predictions}\u0026#39;) # SHAP explanations shap_explainer(model, db[\u0026#39;test_df\u0026#39;], opt) Evaluation and Prediction Functions # def evaluate(model, dataset, opt, desc=\u0026#34;Evaluating\u0026#34;): \u0026#34;\u0026#34;\u0026#34; Evaluates the model on the given dataset. Args: model (nn.Module): Neural decision forest model. dataset (Dataset): Dataset to evaluate on. opt (dict): Dictionary containing evaluation options. desc (str): Description for the progress bar. Returns: tuple: (average loss, accuracy) \u0026#34;\u0026#34;\u0026#34; model.eval() loader = DataLoader(dataset, batch_size=opt[\u0026#39;batch_size\u0026#39;], shuffle=False) test_loss = 0 correct = 0 with tqdm(total=len(loader), desc=desc) as pbar: with torch.no_grad(): for data, target in loader: if opt[\u0026#39;cuda\u0026#39;]: data, target = data.cuda(), target.cuda() data, target = Variable(data), Variable(target) output = model(data) test_loss += F.nll_loss(torch.log(output + 1e-6), target, reduction=\u0026#39;sum\u0026#39;).item() # Add small epsilon to prevent NaNs pred = output.data.max(1, keepdim=True)[1] correct += pred.eq(target.data.view_as(pred)).cpu().sum().item() pbar.set_postfix({\u0026#39;val_loss\u0026#39;: test_loss / len(loader.dataset), \u0026#39;val_accuracy\u0026#39;: correct / len(loader.dataset)}) pbar.update(1) test_loss /= len(loader.dataset) accuracy = correct / len(loader.dataset) return test_loss, accuracy def predict(model, dataset, opt): \u0026#34;\u0026#34;\u0026#34; Predicts the class labels for the given dataset. Args: model (nn.Module): Neural decision forest model. dataset (Dataset): Dataset to predict on. opt (dict): Dictionary containing prediction options. Returns: list: List of predicted class labels. \u0026#34;\u0026#34;\u0026#34; model.eval() loader = DataLoader(dataset, batch_size=opt[\u0026#39;batch_size\u0026#39;], shuffle=False) predictions = [] with torch.no_grad(): for data, _ in loader: if opt[\u0026#39;cuda\u0026#39;]: data = data.cuda() data = Variable(data) output = model(data) pred = output.data.max(1, keepdim=True)[1] predictions.extend(pred.cpu().numpy().flatten()) return predictions SHAP Explanations # def shap_explainer(model, dataset, opt): \u0026#34;\u0026#34;\u0026#34; Generates SHAP explanations for the model predictions. Args: model (nn.Module): Neural decision forest model. dataset (dict): Dictionary containing the test dataset. opt (dict): Dictionary containing SHAP options. \u0026#34;\u0026#34;\u0026#34; def model_predict(data): return np.array(predict(model, data, opt)) X_test = dataset[\u0026#34;test\u0026#34;].X # Use the dataset\u0026#39;s features for SHAP values X_sample = X_test[np.random.randint(0, len(X_test), 100)] explainer = shap.KernelExplainer(model_predict, X_sample) shap_values = explainer.shap_values(X_sample, nsamples=100) # Plot the summary plot shap.summary_plot(shap_values, X_sample, plot_type=\u0026#34;bar\u0026#34;, feature_names=[\u0026#39;age\u0026#39;, \u0026#39;gender\u0026#39;, \u0026#39;height\u0026#39;, \u0026#39;weight\u0026#39;, \u0026#39;ap_hi\u0026#39;, \u0026#39;ap_lo\u0026#39;, \u0026#39;cholesterol\u0026#39;, \u0026#39;gluc\u0026#39;, \u0026#39;smoke\u0026#39;, \u0026#39;alco\u0026#39;, \u0026#39;active\u0026#39;]) shap.summary_plot(shap_values, features=X_sample, class_inds=[1], max_display=10, feature_names=[\u0026#39;age\u0026#39;, \u0026#39;gender\u0026#39;, \u0026#39;height\u0026#39;, \u0026#39;weight\u0026#39;, \u0026#39;ap_hi\u0026#39;, \u0026#39;ap_lo\u0026#39;, \u0026#39;cholesterol\u0026#39;, \u0026#39;gluc\u0026#39;, \u0026#39;smoke\u0026#39;, \u0026#39;alco\u0026#39;, \u0026#39;active\u0026#39;]) Main Function # def main(): \u0026#34;\u0026#34;\u0026#34; Main function to initialize wandb, prepare data, model, optimizer, and start training. \u0026#34;\u0026#34;\u0026#34; wandb.init(project=\u0026#34;cardio_prediction\u0026#34;, config={ \u0026#39;batch_size\u0026#39;: 128, \u0026#39;feat_dropout\u0026#39;: 0.3, \u0026#39;n_tree\u0026#39;: 5, \u0026#39;tree_depth\u0026#39;: 3, \u0026#39;n_class\u0026#39;: 2, \u0026#39;tree_feature_rate\u0026#39;: 0.5, \u0026#39;lr\u0026#39;: 0.001, \u0026#39;gpuid\u0026#39;: -1, \u0026#39;jointly_training\u0026#39;: False, \u0026#39;epochs\u0026#39;: 20, \u0026#39;report_every\u0026#39;: 10 }) config = wandb.config # Manually define arguments opt = { \u0026#39;batch_size\u0026#39;: config.batch_size, \u0026#39;feat_dropout\u0026#39;: config.feat_dropout, \u0026#39;n_tree\u0026#39;: config.n_tree, \u0026#39;tree_depth\u0026#39;: config.tree_depth, \u0026#39;n_class\u0026#39;: config.n_class, \u0026#39;tree_feature_rate\u0026#39;: config.tree_feature_rate, \u0026#39;lr\u0026#39;: config.lr, \u0026#39;gpuid\u0026#39;: config.gpuid, \u0026#39;jointly_training\u0026#39;: config.jointly_training, \u0026#39;epochs\u0026#39;: config.epochs, \u0026#39;report_every\u0026#39;: config.report_every, \u0026#39;cuda\u0026#39;: torch.cuda.is_available() } if opt[\u0026#39;gpuid\u0026#39;] \u0026gt;= 0: torch.cuda.set_device(opt[\u0026#39;gpuid\u0026#39;]) else: print(\u0026#34;WARNING: RUN WITHOUT GPU\u0026#34;) db = prepare_db(\u0026#39;cardio_train.csv\u0026#39;) model = prepare_model(opt) optim = prepare_optim(model, opt) train(model, optim, db, opt) if __name__ == \u0026#39;__main__\u0026#39;: main() Results # Model Performance # The model demonstrated a steady improvement in training accuracy, reaching approximately 74% by the 20th epoch. Validation accuracy showed a similar trend, indicating that the model was effectively learning from the training data without overfitting.\nTraining Accuracy # Training Accuracy Training and Validation Metrics # Validation Accuracy, Training Loss, Validation Loss SHAP Values # Tree SHAP is an algorithm used to compute exact SHAP values for Decision Tree-based models. SHAP (SHapley Additive exPlanation) is a game-theoretic approach to explain the output of any machine learning model. The goal of SHAP is to explain the prediction for any instance ( x_i ) as a sum of contributions from its individual feature values.\nAs explained in the first article, SHAP values are obtained from the following equation:\nSHAP Equation This method is part of the additive feature attribution methods class; feature attribution refers to the fact that the change of an outcome to be explained (e.g., a class probability in a classification problem) with respect to a baseline (e.g., average prediction probability for that class in the training set) can be attributed in different proportions to the model input features.\nSHAP Interaction Values # SHAP allows us to compute interaction effects by considering pairwise feature attributions. This leads to a matrix of attribution values representing the impact of all pairs of features on a given model prediction. SHAP interaction effect is based on the Shapley interaction index from game theory and is given by:\nSHAP Interaction Values The above equation indicates that the SHAP interaction value of the ( i )-th feature with respect to the ( j )-th feature can be interpreted as the difference between SHAP values of the ( i )-th feature with and without the ( j )-th feature. This allows us to use the algorithm for computing SHAP values to compute SHAP interaction values.\nChallenges # Training on a real dataset presented several additional challenges:\nComputational Resources: Training the model on a larger real-world dataset required substantial computational resources. Efficient use of GPU acceleration and parallel processing was necessary to manage training times. SHAP Integration: Integrating SHAP for model explainability was challenging due to the custom architecture of the neural decision forest. Ensuring compatibility and efficient computation of SHAP values required careful handling. Insights from SHAP # The integration of SHAP provided valuable insights into the model\u0026rsquo;s predictions. The SHAP summary plot highlighted the most influential features in the model\u0026rsquo;s decision-making process.\nFrom the plot, we can observe that the top features impacting the model\u0026rsquo;s predictions are:\nSystolic Blood Pressure (ap_hi) Cholesterol Levels Age Weight Diastolic Blood Pressure (ap_lo) These features have the highest mean absolute SHAP values, indicating their significant influence on the prediction of cardiovascular disease.\nSHAP Summary Plot # SHAP Summary Plot SHAP Beeswarm Plot # SHAP Beeswarm Plot Conclusion # Deep Neural Decision Forests offer a powerful combination of decision trees and neural networks, providing both high performance and interpretability. The addition of SHAP values further enhances the model\u0026rsquo;s transparency, making it easier to understand and trust the predictions.\nFor those interested in further exploration, the full implementation is provided, including the training process and SHAP integration for interpretability.\nHappy coding!\nReferences # Kaggle Dataset: Cardiovascular Disease Dataset GitHub: Neural Decision Forests Paper: Deep Neural Decision Forests Tree Shap Deep Neural Decision Forest "},{"id":7,"href":"/docs/groups/gradcam/","title":"Grad Cam","section":"Docs","content":"This work is made by Andrei Markov and Nikita Bogdankov\nGrad-CAM # What is it? # Grad-CAM (Gradient-weighted Class Activation Mapping) is a technique used in deep learning, particularly with convolutional neural networks (CNNs), to understand which regions of an input image are important for the network\u0026rsquo;s prediction of a particular class.\nThis method can be used for understanding how a CNN has been driven to make a final classification decision. Grad-CAM is class-specific, which means that it can produce a separate visualizations on the image for each class. If happens that there is a classification error, than Grad-CAM can help you to see what it did wrong, so we can say that it makes the algorithm more transparent to the developers.\nGrad-CAM consists of producing heatmaps which show the activation classes on the input images. Each activation class is associated with one of the output classes, which are used to indicate the importance of each pixel to the question by changing the intensity of the pixel.\nOn the picture above you can see how important objects for detection are highlighted on the image. And after we created heatmap, we should overlay image and this heatmap to get the final result.\nHow does it work? # Now let\u0026rsquo;s see get to how Grad-CAM works. I will give you a step-by-step explanation:\nForward Pass - First, let \\(A^{k}\\) be the activation map. It is passed through the subsequent layers of the neural network to compute the final class score \\(y_{c}\\) for the target class \\(c\\) . \\[y_{c}=\\sum_iw_{i}^c\\cdot A_i\\] where \\(w_{i}^c\\) represents the weight of the \\({i}\\) th feature map \\(A_i\\) for the class \\(c\\) . Backpropagation - The gradients of the class score \\(y_c\\) with respect to the activation map \\(A_k\\) are computed using backpropagation: \\[\\dfrac{\\partial y_c}{\\partial A^{k}}\\] Gradient Weighting - Grad-CAM assigns importance weights to each activation map based on the gradients gained in the backpropagation step. This is done by taking Global Average Pooling of the gradients: \\[\\alpha_{k}^{c}= \\dfrac{1}{Z} \\sum_{i} \\sum_{j} \\dfrac {\\partial y_c}{\\partial A_{ij}^{k}}\\] \\(\\alpha_{k}^{c}\\) is the importance weight and \\(Z\\) is the spatial dimension of the activation map. \\(i\\) and \\(j\\) are width and height dimensions, \\(y_c\\) is class score(before softmax). Heatmap Generation - Grad-CAM generates the heatmap \\(H_c\\) by linearly combining the activation maps weighted by their importance scores: \\[H^{c} =ReLU (\\sum_k \\alpha_{k}^{c} A^{k})\\] where \\(ReLU\\) is the rectified linear unit activation function, allowing only positive values contribute to the heatmap. Heatmap \\(H_c\\) highlights the regions in the input image that are most relevant for the network\u0026rsquo;s decision for the target class \\(c\\) . Brighter the region, more it contributes to the prediction. To clarify the process, I will attach the image explaining the described above: Where can it be used? # But now let\u0026rsquo;s talk about where Grad-CAM can be used. There are many fields, where it can be useful, so here are some examples:\nMedical Image Analysis - Grad-CAM can help clinicians to quicker check the patients to see if there are any problems. If so, it shows where the problems may be located. Autonomous Vehicles - Grad-CAM is used for object detection, lane detection, and pedestrian recognition. It can be used to visualize which parts of the input image are most relevant in the decision-making process of these systems. Security and Surveillance: Grad-CAM is used for activity recognition, intruder detection, and object tracking. It can help security personnel to notice the criminal action if they missed it for some reason.\nIndustrial Quality Control - Grad-CAM is used for defect detection, product classification, and quality assessment. Grad-CAM can help operators to faster detect problem details.\nRemote Sensing and Earth Observation - Grad-CAM is used for land cover classification, crop monitoring, and disaster detection. Grad-CAM can assist analysts in understanding which features in satellite or aerial imagery are indicative of certain land cover types or environmental conditions, aiding in resource management and disaster response.\nLink to collab\n"},{"id":8,"href":"/docs/groups/integrated-gradients/","title":"Integrated Gradients","section":"Docs","content":" Integrated Gradients Method for Image Classification # XAI Course Project | Anatoliy Pushkarev\nGoal # Develop a robust image classification model and analyze its behavior with the help of the integrated gradients method.\nIntegrated gradients paper\nIntegrated Gradients # Integrated Gradients is a technique for attributing a classification model\u0026rsquo;s prediction to its input features. It is a model interpretability technique: you can use it to visualize the relationship between input features and model predictions. It finds the importance of each pixel or feature in input data for a particular prediction of the model.\nIntegrated gradients original paper can be found here.\nSteps # Select baseline (uniform distribution for all classes). Evaluate the path from baseline to input data point by many iterations. Observe how changing input data affects gradients. Integrate all gradients. This is the original integrated gradients formula: This is the Riemann approximation of the original formula, which is always used. A very good article about integrated gradients and Riemann Approximation can be found here:\nGreat IG method explanation This is a good picture which shows, why exactly we need to sum gradients. Basically if we just take the gradients of the model wrt the inputs, we will get a lot of noise, which is illustrated on the right side of the picture. But if we scale the inputs, at the some point we get interesting gradients, which the method uses.\nMNIST Fashion Dataset # The dataset which I am using for the test purposes is Fashion-MNIST. Fashion-MNIST is a dataset of Zalando\u0026rsquo;s article images—consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28x28 grayscale image, associated with a label from 10 classes. Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. It shares the same image size and structure of training and testing splits.\nModel # Keras Sequential 3 Conv layers 2 Max pooling layers Dropouts 2 FC layers Training # Below you can find a screenshot of model training loop. It is pretty straightforward.\nResults of Training # As a result, I got a robust model with accuracy \u0026gt; 90%, which is good for explanation.\nThere are some classes with lower precision and recall, it will be interesting to check IGs of them (for example, Shirt classs).\nExamples of correctly predicted classes: Examples of incorrectly predicted classes: Applying Integrated Gradients # Below you can find maps for IGs, which I`ve got after the method execution. Basically, red means that these points are important for certain class and blue means that these are negative features for a particular class.\nMore Examples # This is and example of how IGs work with regression tasks. Basically x-axis is features and y-axis is weights of the feature to the output.\nChallenges # Lack of open-source stable solutions for any model. Most likely you have to implement your own solution for a complicated model. Lack of support and community – not so popular (SHAP and GradCAM paper cited 20K times, IG – 5K times). "},{"id":9,"href":"/docs/groups/kernel-shap/","title":"Kernel Shap","section":"Docs","content":" Unveiling black boxes with SHAP Values # Nowadays, correct interpretation of model predictions is crucial. It builds user confidence, helps to understand the process being modeled, and suggests how to improve the model. Sometimes simple models are preferred, e.g. in finance, because they are easy to interpret, but they usually do not achieve the same performance as complex ones. Therefore, to overcome this trade-off between accuracy and interpretability, various methods have been developed. In this post I want to talk about one of them, the SHAP framework.\nWhat are SHAP Values? # SHAP Values is a method that assigns each feature a value that reflects its contribution to the model prediction. These values are based on cooperative game theory, the concept of Shapley values, introduced by Lloyd Shapley. In this context, each attribute is treated as a player in the game, and the Shapley value measures the average marginal contribution of each attribute across all possible combinations of attributes\nThe Mathematics Behind SHAP Values # As already mentioned, Shapley values is a concept in the theory of cooperative games. For each such game, it specifies the distribution of the total payoff received by the coalition of all players.\nFormal Definition: # The Shapley value for player \\( i \\) in a cooperative game is defined as the average marginal contribution of the player to the coalitions. Formally, we have a set \\(N\\) of players and a characteristic function \\(\\mathcal{v}\\) representing gains, which maps subset of players to real numbers(gain). Also, \\(\\mathcal{v(\\emptyset)}=0\\) meaning that empty coaliation of players worths nothing. Then, the Shapley value \\(\\phi_i\\) for player \\(i\\) is given by: \\[\\phi_i{(\\mathcal{v})}=\\sum_{S \\subset N \\backslash \\left\\{i\\right\\}}{\\frac{|S|!(n-|S|-1)!}{|N|!} (\\mathcal{v}(S \\cup \\left\\{i\\right\\})-\\mathcal{v}(S))}\\] where:\n\\(S\\) is a subset of players excluding player \\(i\\) . \\(|S|\\) is the number of player in the coalition \\(S\\) \\(\\mathcal{v}(S)\\) is a total gain of the coalition \\(S\\) This formula calculates the marginal contribution of player \\(i\\) to each possible coalition and then averages it. Extension to SHAP Values and Properties: # SHAP combines the local interpretability methods(Linear LIME, for example) and Shapley values. It results in desired properties:\nLocal accuracy: \\[f(x)=g(x\u0026#39;)=\\phi_0\u0026#43;\\sum^M_{i=1}{\\phi_ix\u0026#39;_i}\\] The explanation model \\(g(x\u0026#39;)\\) matches the original model \\(f(x)\\) when \\(x=h_x(x\u0026#39;)\\) , where \\(\\phi_0=f(h_x(0))\\) represents the model output with all simplified inputs toggled off(missing) Missingness: \\[x\u0026#39;_i=0 \\to \\phi_i=0\\] Missing features have no attributed impact Consistency: Let \\(f_x(z\u0026#39;)=f(h_x(z\u0026#39;))\\) and \\(z\u0026#39;\\backslash i\\) denote setting \\(z\u0026#39;_i=0\\) . For any two models \\(f\\) and \\(f\u0026#39;\\) , if \\[f\u0026#39;_x(z\u0026#39;)-f\u0026#39;_x(z\u0026#39;\\backslash i)\\geq f_x(z\u0026#39;)-f_x(z\u0026#39;\\backslash i)\\] for all inputs \\(z\u0026#39; \\in \\left\\{0,1\\right\\}^M\\) , then \\(\\phi_i(f\u0026#39;,x)\\geq\\phi_i(f,x)\\) . It means that if a model changes so that the marginal contribution of a feature value increases or stays the same (regardless of other features), the Shapley value also increases or stays the same. Computation of SHAP Values # Kernel SHAP # This is a model-agnostic method for approximating SHAP values. This method uses a Linear LIME to locally approximate the original model.\nFirst, we need to heuristically choose the parameters for LIME: \\[\\Omega(g)=0,\\] \\[\\pi_{x}(z\u0026#39;)=\\frac{(M-1)}{(M \\text{ choose } |z\u0026#39;|)|z\u0026#39;|(M-|z\u0026#39;|)}\\] \\[L(f,g,\\pi_{x})=\\sum_{z\u0026#39;\\in Z}{[f(h_x(z\u0026#39;))-g(z\u0026#39;)]^2\\pi_{x}(z\u0026#39;)}\\] Then, since \\(g(z\u0026#39;)\\) is linear, \\(L\\) is a squared loss, the objective function of LIME: \\(\\xi=\\underset{g \\in \\mathcal{G}}{\\operatorname{argmin}}{L(f,g,\\pi_{x\u0026#39;})\u0026#43;\\Omega{(g)}}\\) can be solved using linear regression.\nIllustrative example # Model and Instance: Let\u0026rsquo;s say we have a predictive model f and a dataset with three features. We want to understand how each feature contributes to the model\u0026rsquo;s prediction for a specific data point \\(x = (x_1,x_2,x_3)\\) by computing SHAP values. Generating coalitions: To do it we need to consider all possible coalitions of features that could be used to make a prediction. Each coalition is a subset of the features used for prediction. The set of coalitions in our case: {0,0,0},{0,0,1},{0,1,0},{0,1,1},{1,0,0},{1,0,1},{1,1,0},{1,1,1}. Obtaining modeling results for coalitions: For each of these coalitions, we compute the model output. The missing features must be imputed. We obtain the following outputs: \\(f(\\emptyset), f(x_1),f(x_2),f(x_3),f(x_1,x_2),f(x_1,x_3),f(x_2,x_3),f(x_1,x_2,x_3)\\) Obtaining weights for coalitions: \\[\\pi_{x}(z\u0026#39;)=\\frac{(M-1)}{(M \\text{ choose } |z\u0026#39;|)|z\u0026#39;|(M-|z\u0026#39;|)}\\] For example, \\(\\pi_x({0,0,1})=\\frac{3-1}{\\frac{3!}{1!(3-1)!}1(3-1)}=\\frac{2}{6}=\\frac{1}{3}\\) We obtain the model \\(g\\) : Finally, we train a linear model (explanation model) \\(g\\) . This model is trained to match the outputs of our original model \\(f\\) . The weights of the model \\(g\\) are obtained by optimizing the following loss function \\[L(f,g,\\pi_{x})=\\sum_{z\u0026#39;\\in Z}{[f(h_x(z\u0026#39;))-g(z\u0026#39;)]^2\\pi_{x}(z\u0026#39;)}\\] The weights of model \\(g\\) are the Shapley values. Interpreting SHAP Values # Individual Instance Interpretation: # Feature Contribution: SHAP values give us the ability to measure how badly or good a feature is in making model predictions about an individual, instance. A positive value implies that the feature contributes towards increasing the prediction of the model while a negative value suggests that it reduces the prediction. Magnitude: The magnitude or absolute value of a SHAP number impacts on how much influence a particular attribute has in our model prediction, Larger numbers indicate more importance in shaping up the final outcome. Global Feature Importance: # Feature Importance Ranking: The average of the absolute SHAP values for each feature across all instances gives us a global ranking of feature importance. This ranking helps identify the features that consistently have the most significant impact on model predictions. Understanding Model Behavior: An insight into how well our model reacts to different levels of a given characteristic is only possible by studying distributions of its corresponding Shapley Values. In doing so, an expose can be made about any prejudices or non-linear aspects within. Applications of SHAP Values # SHAP values is a powerful tool that has several applications:\n1. Model Debugging: we can identify features which cause problems in predictions, that can indicate, for example, data leakage or correlations.\n2. Fairness and Bias Analysis: we can identify biases when the model making different unfair predictions based on some attributes like race, gender, and etc. Understanding the impact of these feature can help us develop fairer models.\nConclusion: # Main takeaways: # Shapley values is a measure of the average marginal contribution of each feature across all possible subsets of features. SHAP values are connecting Shapley values to the local interpretability methods, providing properties such as locality, missingness, and consistency. SHAP values enable us to better undestand both individual instance and global model behavior, as well as providing feature contribution analysis, feature importance ranking, and model behavior understanding. They are commonly used for model debugging allowing identifying problematic features and developing more unbiased and fair model References: # The blog post is mainly based on the original paper introducing SHAP values - A Unified Approach to Interpreting Model Predictions (nips.cc)\nThe original implementation of SHAP - shap/shap: A game theoretic approach to explain the output of any machine learning model. (github.com)\nMy implementation of Kernel SHAP - https://colab.research.google.com/drive/1TPHvns2psDNKknwubxCTHZprw3UGB-w1?usp=sharing\n"},{"id":10,"href":"/docs/groups/rag/","title":"Rag","section":"Docs","content":" Retrieval-augmented generation # Project by:\nVladislav Urzhumov Danil Timofeev What is RAG? # Retrieval-Augmented Generation is a framework in the explainable artificial intelligence field for improving the quality of LLM-generated responses by grounding the model on external sources of knowledge to supplement the LLM’s internal representation of information. By grounding an LLM on a set of external, verifiable facts, the model has fewer opportunities to pull information baked into its parameters. This reduces the chances that an LLM will leak sensitive data, or \u0026ldquo;hallucinate\u0026rdquo; incorrect or misleading information.\nAdvantages of RAG # Transparency: generated response is supported by the document chunks model referred to. A short and easy-to-go variant of references. Better accountability: RAG allows to trace back the information source to understand, whether the mistake in the response was due to incorrect source data or a flaw in the model’s processing Hallucinations avoided: knowledge can be directly revised and expanded, and accessed knowledge can be inspected and interpreted. Reasonably limited documents: users can limit the external resources to provide responses based only one the trusted source, enhanced by the pre-trained model’s ability to construct adequate connected sentences. Ease of implementation: modern frameworks allow embracing of Retrieval-Augmented Generation with ease and small adjustments of code. One can always add improvements on top to achieve better results, however, standard RAG is easy-to-go. Model-agnostic explainability of LMs: any LLM\u0026rsquo;s responses can be explained via RAG, despite of the model type and number of parameters. Responses based on trusted documents: there is an ability to construct a library of trusted documents to use while generating results, avoiding distrusted sources. Technical steps of RAG # Documents, which are provided for LLM to take information from, are firstly properly indexed. In vanilla (naive) RAG, user query is not pre-processed specifically, thus validated user query plus the indexed documents are sent to the retrieval stage.\nRelevant chunks of information are retrieved, included to prompt and then processed by LLM. Output is provided coupled with the indexed retrieved chunks of information.\nRAG improvements # Retrieval-Augmented Generation, as a popular framework, was highly researched. Thus, numerous improvement techniques are present to enhance the performance of the technique.\nBefore the retrieval part, Pre-retrieval is recommended (such as query routing or query expansion) to provide more relevant or sufficient context.\nAfter the retrieval part, Post-retrieval is used to summarize, fuse or rerank retrieved results.\nOur framework research and implementation embrace the pre-retrieval addition, leaving the post-retrieval for the readers to try out and make an experiment.\nImplementation details # LLM used is llama-3-8b-8192 from Groq. We picked the Groq provider for open-source models because it’s the quickest one out there and llama-3-8b is the current state-of-the-art for small language models. Embedding model embraced is bge-base-en-v1.5. The embedding model used is also open-source, it’s currently the SOTA for its current size. We could have used a bigger more capable multilingual model, but we decided to just keep the English support, so this was sufficient. RAG framework for our purpose should be well-maintained and easy-to-use. Thus, we have chosen llama-index (vector databases and retrieval); As an improvement, our team has used AutoContext to pre-retrieve the necessary information. We have used it to correctly summarize the main idea behind the whole document with specific details in separate chunks. Code # Best way to check code is interactive environment, such as Google Colab. Hence, our team provides one for any developer interested in trying by own hand.\nPlease, enjoy by following the link. Don\u0026rsquo;t forget to insert your own Groq API key here (it\u0026rsquo;s free):\nif \u0026#34;llm\u0026#34; not in st.session_state: st.session_state.llm = Groq( model=\u0026#34;llama3-8b-8192\u0026#34;, api_key=\u0026#34;\u0026#34;, kwargs={\u0026#34;\u0026#34;}, ) Settings.llm = st.session_state.llm Further improvements # Better pipelines: Retrieval can be improved by implementing more complex RAG pipelines, adding better data parsing (better chunking, for example) and adding a reranker, for example (a step in post-retrieval); Multilingual support: users are interested in explained results not only in english, but also in other languages; OCR support: Optical Character Recognition (OCR) is a technology that falls under the umbrella of machine learning and computer vision. It involves converting images containing written text into machine-readable text data. Being able to process images with infographics or screenshots along with non-copiable image-based pdfs is a big step towards versatility; Specific rules for scientific paper: Pre-retrieval for scientific paper should be focused on abstracts, summaries and conclusions written by human authors rather than generated ones, that will increase understanding and trustworthiness of documents and retrieval pipeline. Thanks for the attention! # "},{"id":11,"href":"/docs/groups/shap_darya_and_viktoria/","title":"Shap Darya and Viktoria","section":"Docs","content":" SHAP. Tree Explanations # Exploring SHAP Tree Explainer for Predictive Health Analytics: A Deep Dive into RandomForest Models # Predictive models are essential for managing healthcare resources, predicting disease outbreaks, and guiding public health policy in the quickly changing field of health analytics. RandomForest is a standout model among these due to its solid performance, interpretability, and simplicity. But it can be difficult to comprehend how these models\u0026rsquo; complex internal workings operate. Presenting SHAP (SHapley Additive exPlanations), a revolutionary tool that clarifies how machine learning models—like RandomForest—make decisions. In-depth discussion of the SHAP Tree Explainer, its use in RandomForest models, and its importance in predictive health analytics are provided in this article.\nWhat is SHAP? # The unified measure of feature importance known as SHAP, or SHapley Additive exPlanations, gives each feature a value based on how important it is for a certain prediction. Basing itself on the idea of Shapley values from cooperative game theory, it provides an explanation for any machine learning model\u0026rsquo;s output. After taking into consideration feature interactions, SHAP values indicate the relative contribution of each feature to the prediction for a particular instance.\nSHAP Tree Explainer for RandomForest # A customized version of the SHAP library called the SHAP Tree Explainer is intended to be used with tree-based models, such as RandomForest. Trees are more difficult to interpret since they are by nature complicated and nonlinear, in contrast to linear models. To tackle this difficulty, the SHAP Tree Explainer dissects a tree\u0026rsquo;s prediction into contributions from every leaf node and attributes the prediction to the qualities that gave rise to those nodes.\nHow Does It Work? # Imagine a RandomForest model predicting whether a person is infected with a disease based on various factors like age, symptoms, and medical history. The SHAP Tree Explainer starts by assigning a base value to the prediction, which is the average outcome of the model across all instances. Then, it iterates through the tree, adding the contribution of each feature along the path to the final prediction. Each feature\u0026rsquo;s contribution is calculated based on its impact on the prediction, taking into account the interactions with other features.\nBenefits of Using SHAP with RandomForest # Interpretability: SHAP provides clear, actionable insights into how each feature influences the prediction, making it easier to understand the model\u0026rsquo;s decisions. Fairness Analysis: By showing how each feature contributes to the prediction, SHAP helps identify potential biases in the model, aiding in fairness assessments. Model Debugging: It highlights areas where the model might be performing poorly or where additional data could improve predictions. How to use TreeExplainer? # Install the shap library !pip install shap # install Read the data df = pd.read_csv(\u0026#39;/content/AIDS_Classification.csv\u0026#39;) Initialize tree model\nmodel = RandomForestClassifier(n_estimators=200, random_state=121) Fit the model\nmodel.fit(X_train, y_train) Make a prediction\npredictions = model.predict(X_test) Initialize TreeExplainer\nexplainer = shap.TreeExplainer(model) Now, we are ready for construct graph. The first is visualise the shap values.\nshap_values = explainer.shap_values(X) # compute common shap values and visualize shap.summary_plot(shap_values, X, feature_names=X.columns, plot_type=\u0026#34;dot\u0026#34;, auto_size_plot=False, show=False) plt.show() 8.The second shown the SHAP interaction values between two features (\u0026ldquo;age\u0026rdquo; and \u0026ldquo;preanti\u0026rdquo;) are computed, and the features\u0026rsquo; dependence on the model\u0026rsquo;s predictions are displayed. This aids in comprehending the interplay between these two characteristics while forecasting.\nshap_interaction = explainer.shap_interaction_values(X) # compute interaction values and visualize shap.dependence_plot((\u0026#34;age\u0026#34;, \u0026#34;preanti\u0026#34;), new_shap_interaction, X, feature_names=X.columns, show=False) plt.show() This graph shows that those who use drugs and are over 40 have a higher risk of contracting an illness.\nConclusion # Our understanding of and confidence in predictive health analytics models is greatly improved by integrating SHAP with RandomForest models. An essential tool for healthcare decision-making, SHAP\u0026rsquo;s Tree Explainer provides comprehensive insights into how specific attributes affect forecasts. In addition to its interpretability, SHAP facilitates fairness analysis and debugging of models, which improves the model development process. The essay showcases SHAP\u0026rsquo;s transformative potential in turning raw data into actionable knowledge through its practical application, which primarily focuses on forecasting disease infection status. In order to achieve transparency and reliability in predictive models going forward, the field will need to adopt SHAP, which will be crucial.\nLink to our colab:\nCode example\nLink to the dataset:\nAIDS_classification\nLinks:\nDocumentation of SHAP\nPaper (Explainable AI for Trees: From Local Explanations to Global Understanding, Scott M. Lundberg et al.)\nOfficial exaple of usage\nSHAP on github\n"},{"id":12,"href":"/docs/groups/sverl_tac_toe/","title":"Sverl Tac Toe","section":"Docs","content":" Explaining Reinforcement Learning with Shapley Values | Tutorial # Introduction # https://www.toptal.com/machine-learning/deep-dive-into-reinforcement-learning Explainability and interpretability of AI models is a hot topic in the research community in recent times. With the growth of new technologies and methods in the neural networks field, the endeavour to understand the “black box” models is rising in popularity.\nIn this tutorial, you will learn 2 methods for explaining the reinforcement learning models:\nApplying Shapley values to policies Using SVERL-P method Before we start, this tutorial is based on the paper “Explaining Reinforcement Learning with Shapley Values” by Beechey et. al. [1], where the researchers present 2 approaches on how to explain reinforcement learning models with Shapley values.\nAlso, we assume that you are familiar with machine learning, reinforcement learning, but not familiar with explainable artificial intelligence.\nA quick reminder: reinforcement learning is a type of unsupervised learning technique, in which you train the agent in the environment, which can vary from a hide-and-seek game to a traffic simulation.\nOpenAI reinforcement learning research paper visualisation [2].\nFor the implementation, we will be using a ⚡blazingly fast⚡ and statically-typed programming language called 🦀 Rust 🦀. We will provide a few code snippets, but our main goal is to provide you with the idea on implementation, so you can try to implement these methods in your favourite programming language.\nExplainable Artificial Intelligence # Yearly growth trends. Papers before 2013 were omitted for readability [3].\nExplainable Artificial Intelligence (XAI) is a field of study that seeks to make AI systems more understandable and interpretable to humans [4].\nInterpretability is the ability of a human to understand the reasoning behind a decision made by an AI system [4].\nFair AI: AI systems can perpetuate and amplify existing societal biases if they are not carefully designed. Fair AI techniques can help to ensure that AI systems make decisions that are fair and unbiased for all people [4].\nThere are many techniques, methods, and algorithms invented in recent years to explain AI and design it to be fair. These techniques involve: LIME (Locally Interpretable Model-Agnostic Explanations), SHAP (Shapley Additive exPlanations), DeepDream, SVERL-P, and many more.\nShapley values # Imagine you and your friend are making a group project in a university course. At the end of the course, you get B and your friend gets A. You think this is unfair, but how to prove it? Well, to fairly assess your performance, you can calculate your contribution to the group project, i.e. to a coalition of two players using Shapley values. For example, if your friend would do the project alone, they would complete 45% of the project. However, together you complete 100% of the project.\nShapley value formula [5].\nShapley values are a concept from cooperative game theory that provides a way to fairly distribute the marginal gains among players in a coalition, ensuring that each player gets as much or more than they would have if they acted independently.\nIn the above formula, your friend’s val(S) = 75, and the value of both of you is val(S union {i}) = 100. Now, your marginal contribution to the coalition of two players (you and your friend) is 100 - 45 = 55. Now, you cannot say that your whole contribution is 55, you need to calculate the average of this number to fairly assess your performance. That is when weight comes into play. You can think of it as a way to normalise marginal contribution over all possible coalitions, even when it is an empty coalition, i.e. noone does the project.\nAfter all the calculations are done, you get a fair assessment of your performance, and prove to your teaching instructor that you deserve a better grade.\nOne more thing about Shapley values is that they satisfy the following 4 properties [6]:\n4 properties of Shapley values [5].\nEfficiency - the sum of the Shapley values of all agents equals the value of the grand coalition, so that all the gain is distributed among the agents. Symmetry - two players are considered interchangeable if they make the same contribution to all coalitions. Null player - if the player makes 0 contribution to all coalitions, then they have zero shapley value. Linearity (Additivity) - shapley value of coalition is equal to shapley values of individuals in this coalition. Environment # For this project, we have coded the Tic-Tac-Toe game in the Rust programming language using Geng game engine [7].\nIn the centre of the screen there is a grid of cells, where each cell contains either: nothing, cross, or circle. Players make moves in-turns, and one of them wins if they either make a consequent horizontal, vertical, or diagonal line of the same shape, corresponding to them (circle or cross).\nExact implementation of this environment is out of scope for this tutorial. However, you can find the full source code on our GitHub [8].\nIn this simple environment we will introduce 2 models/policies and try to interpret their actions. For that reason, we first need to describe the environment in terms of Markov Decision Process:\nState is the grid, consisting of 9 cells. Action is an input from the user to place a shape. In our case, we have 9 actions, one for each cell. Environment handles the turns automatically, so that is why you do not need to worry about which shape to put. Reward is either 0 or 1 (player either wins a game or looses) Policies: random and minimax. Random policy outputs a random legal move, i.e. it cannot place a shape on the occupied cell or make turns when the game is finished. Minimax calculates all possible game outcomes for both players, and compares them. As output it produces the action, which leads to a maximum available value. pub enum Tile { Empty, X, O, } pub struct Grid\u0026lt;T = Tile\u0026gt; { pub cells: [[T; 3]; 3], } pub type Policy = Box\u0026lt;dyn FnMut(\u0026amp;Grid) -\u0026gt; Grid\u0026lt;f64\u0026gt;\u0026gt;; // policies return the probabilty distribution over possible action pub type Action = vec2\u0026lt;Coord\u0026gt;; Shapley values applied to policy # Before applying the method, we need to define terms and try to understand them.\nPolicy takes a state and returns the probability distribution over possible actions. Observation is the state with some of the features (cells) hidden. The first method to interpret the reinforcement learning model is to apply shapley values directly to the policy,\nThe value function used to evaluate a policy, as defined in the paper, is the expected probability of each action over the distribution of possible states (given current partial observation).\n\\(v^\\pi(C) = \\pi_c(a|s) = \\sum_{s\u0026#39; \\in S} p^\\pi(s\u0026#39;|s_C) \\pi(a|s\u0026#39;) \\) Where C is the observation (with the features from the coalition), \\(p^\\pi(s’|s_C)\\) is the probability of seeing state \\(s’\\) given the observation \\(s_C\\) .\nSo, to calculate the Shapley value for each feature for every action, we first need to get all partial observations:\nimpl Grid { pub fn all_subsets(\u0026amp;self) -\u0026gt; Vec\u0026lt;Observation\u0026gt; { let positions: Vec\u0026lt;_\u0026gt; = self.positions().collect(); powerset(\u0026amp;positions) .into_iter() .skip(1) // Skip the set itself .map(|positions| Observation { positions, grid: self.clone(), }) .collect() } } Then, for every observation we calculate the value function for it and for the observation without the feature we\u0026rsquo;re calculating for:\nlet observations = grid.all_subsets(); let n = grid.positions().count(); // number of all features observations .iter() .cloned() .map(|observation| { let base_value = value(feature, \u0026amp;observation); let s = observation.positions.len(); let scale = factorial(s) as f64 * factorial(n - s - 1) as f64 / factorial(n) as f64; let featureless = observation.subtract(feature); let mut value = base_value - value(feature, \u0026amp;featureless); value *= scale; value }) .fold(Grid::zero(), Grid::add) The value function itself is the expected value for every action, and is easy to implement. But we also need to calculate possible states given the observation. We can do that by iterating over all hidden cells in the grid and checking all 3 possible states for that cell.\nimpl Observation { pub fn value(\u0026amp;self, policy: \u0026amp;mut Policy) -\u0026gt; Grid\u0026lt;f64\u0026gt; { let states = self.possible_states(); let scale = (states.len() as f64).recip(); let mut result = states .into_iter() .map(|state| policy(\u0026amp;state)) .fold(Grid::zero(), Grid::add); result *= scale; result } pub fn possible_states(\u0026amp;self) -\u0026gt; Vec\u0026lt;Grid\u0026gt; { let hidden: Vec\u0026lt;_\u0026gt; = self .grid .positions() .filter(|pos| !self.positions.contains(pos)) .collect(); (0..3usize.pow(hidden.len() as u32)) .map(|i| { let mut grid = self.grid.clone(); for (pos, cell) in hidden.iter().enumerate().map(|(t, \u0026amp;pos)| { let cell = match (i / 3_usize.pow(t as u32)) % 3 { 0 =\u0026gt; Tile::Empty, 1 =\u0026gt; Tile::X, 2 =\u0026gt; Tile::O, _ =\u0026gt; unreachable!(), }; (pos, cell) }) { grid.set(pos, cell); } grid }) .collect() } } SVERL-P # Instead of applying shapley values directly to policy, we can use a better approach, which is called Shapley Values for Explaining Reinforcement Learning Performance or in short SVERL-P.\nSVERL-P is divided into 2 methods: local and global ones.\nLocal SVERL-P is essentially a prediction of the reward given uncertainty of the current observation.\n\\(v^{local}(C) = E_{\\hat\\pi}[\\sum_{t=0}^\\infty \\gamma^t r_{t\u0026#43;1} | s_0=s]\\) Where\n\\(\\hat\\pi(a_t|s_t) = \\begin{cases} \\pi_C(a_t|s_t)\\ if\\ s_t=s, \\\\ \\pi(a_t|s_t)\\ otherwise \\end{cases}\\) \\(r\\) is the reward (winner gets the reward of 1), \\(\\gamma\\) is the discounting factor.\nGlobal extends the uncertainty to all future states, rather than just the starting one.\n\\(v^{global}(C) = E_{\\pi_C}[\\sum_{t=0}^{\\infty} \\gamma^t r_{t\u0026#43;1} | s_0=s]\\) \\(Ф_i(v^{global}) = E_{p^\\pi(s)}[\\phi_i(v^{global}, s)]\\) Where \\(\\phi_i\\) is the Shapley value.\nThe difference with the previous (regular Shapley) value function is the future prediction. So, after calculating the initial \\(\\pi_C\\) the same way (Observation::value), we recursively go through all the states in the future and calculate the expected reward, according to the policy action distribution.\nimpl Grid { fn predict(\u0026amp;self, gamma: f64, policy: \u0026amp;mut Policy) -\u0026gt; f64 { let Some(player) = self.current_player() else { return 0.0; // The game has ended }; let mut result = 0.0; // Expected reward let weights = policy(self); // action probability distribution for pos in self.empty_positions() { let prob = weights.get(pos).unwrap(); let mut grid = self.clone(); grid.set(pos, player.into()); let immediate_reward = grid.reward(player); let future_reward = gamma * grid.predict(gamma, policy); result += prob * (immediate_reward + future_reward); } result } } Demo # You can try out environment from this tutorial in the web for yourself: https://1adis1.github.io/xai-sverl/\nConclusion # As a result, we have implemented Shapley values and SVERL-P algorithm to Tic-Tac-Toe. While Shapley values are used for interpreting specific actions of the policy, it may be difficult to understand their impact on the game. And SVERL-P is designed to solve that problem by showing directly the contribution of each feature to the outcome of the game.\nReferences # [1] Beechey et. al. “Explaining Reinforcement Learning with Shapley Values” [2] OpenAI Hide-and-Seek simulation [3] Trends in Explainable AI by Alon Jacovi [4] Rustam Lukmanov, Explainable and Fair AI, Spring 24, Lecture 1 [5] The mathematics behind Shapley Values [6] Shapley values [7] Geng game engine [8] Our implementation with environment in Rust "},{"id":13,"href":"/docs/groups/torchprism/","title":"Torch Prism","section":"Docs","content":" TorchPRISM # Table of Contents\nTorchPRISM Introduction: Unlocking CNNs with PRISM Understanding CNNs Introducing PRISM: A Glimpse into CNN Decision-Making Implementation of PRISM How to use Examples VGG11 ResNet101 GoogleNet Introduction: Unlocking CNNs with PRISM # Convolutional Neural Networks (CNNs) have revolutionized computer vision, powering innovations from facial recognition to autonomous vehicles. Yet, their decision-making process remains a mystery, hindering trust and understanding.\nInspired by the paper \u0026ldquo;Unlocking the black box of CNNs: Visualising the decision-making process with PRISM,\u0026rdquo; our blog sets out to demystify CNNs\u0026rsquo; decisions.\nIn this short intro, we\u0026rsquo;ll touch on CNN basics, the need for transparency, and introduce PRISM as our tool of choice for visualizing CNN decisions. Get ready to see CNNs in a new light!\nUnderstanding CNNs # CNNs are the backbone of modern computer vision, mimicking the human visual system to recognize patterns and features in images. At their core, CNNs consist of layers of neurons organized in a hierarchical fashion, each layer extracting increasingly complex features from the input data.\nWhy Interpretability Matters: While CNNs excel at tasks like image classification and object detection, their inner workings often remain inscrutable. This lack of transparency raises concerns about bias, fairness, and reliability in AI systems. Understanding how CNNs arrive at their decisions is crucial for ensuring accountability and trust.\nVisualizing CNNs: Techniques like PRISM offer a window into CNN decision-making. By visualizing the activations of individual neurons and feature maps across different layers of the network, PRISM helps unravel the thought process behind CNN predictions.\nIntroducing PRISM: A Glimpse into CNN Decision-Making # Meet PRISM: Predictive, Interactive, Summarisation, and Modelling. PRISM isn\u0026rsquo;t just a tool; it\u0026rsquo;s a key to unlocking the black box of CNN decision-making.\nPredictive: PRISM enables us to predict and understand how CNNs arrive at their decisions by visualizing the activation patterns within the network.\nInteractive: With PRISM, exploring CNN decision-making is not a passive experience. It\u0026rsquo;s an interactive journey where we can manipulate inputs, observe neuron activations, and gain insights into the network\u0026rsquo;s inner workings.\nSummarisation: PRISM doesn\u0026rsquo;t overwhelm us with complex data. Instead, it distills the essence of CNN decision-making into intuitive visualizations that highlight the most influential features and neurons.\nModelling: Through PRISM, we model and interpret the decision-making process of CNNs, shedding light on their behavior and paving the way for more transparent and accountable AI systems.\nImplementation of PRISM # Implementing PRISM brings us closer to understanding the intricate decision-making processes of Convolutional Neural Networks (CNNs). Let\u0026rsquo;s explore how to harness the power of PRISM to visualize CNN activations and gain valuable insights.\nStep 1: Data Preparation: Prepare the input data and the trained CNN model you want to analyze. Ensure that the data is in a format compatible with your chosen deep learning framework.\n# crop image for the model input _crop = transforms.Compose([ transforms.ToPILImage(), transforms.Resize((224, 224)) ]) # normalize image for model input on which it was trained _normalize = transforms.Compose([ transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) ]) If you have different data preprocessing steps add them here.\nStep 2: Model Loading: Load the pre-trained CNN model into your preferred deep learning framework, such as PyTorch. This model will serve as the basis for visualizing activations with PRISM.\nmodel_name = \u0026#39;vgg11\u0026#39; model = models.get_model(model_name, weights=True) Step 3: PRISM:\nThe proposed technique uses PCA of features detected by neural network models to create an RGB coloured image mask that highlights the features identified by the model. PRISM can be used for better human interpretation of neural network representations and to automate the identification of ambiguous class features. The combination of PRISM with another method, Gradual Extrapolation, results in an image showing each segment of a classified object in different colours. PRISM can help identify indistinct classes and improve the real-world application of the model.\nGenerating PRISM results consists of simple matrix manipulation and computation of the PCA (Fig. 1). First, we transform the output from the chosen layer of the model into a two-dimensional matrix. Each channel becomes a single column in the resulting matrix. In this matrix, we computed the PCA and cut off all PCs beyond the first three. In the last step, we transform these three PCs back into channel matrices to assign later colours red, green, and blue to make them visually distinguishable.\nStep 4: PRISM Implementation:\nGet top three PC\u0026rsquo;s for RGB color.\ndef _get_pc(self, final_excitation): final_layer_input = final_excitation.permute(0, 2, 3, 1).reshape( -1, final_excitation.shape[1] ) normalized_final_layer_input = final_layer_input - final_layer_input.mean(0) u, s, v = normalized_final_layer_input.svd(compute_uv=True) self._variances = s**2/sum(s**2) # save the variance raw_features = u[:, :3].matmul(s[:3].diag()) return raw_features.view( final_excitation.shape[0], final_excitation.shape[2], final_excitation.shape[3], 3 ).permute(0, 3, 1, 2) Use PC to perform PRISM.\ndef prism(self, grad_extrap=True): if not self._excitations: print(\u0026#34;No data in hooks. Have You used `register_hooks(model)` method?\u0026#34;) return with torch.no_grad(): rgb_features_map = self._get_pc(self._excitations.pop()) if grad_extrap: rgb_features_map = self._upsampling( rgb_features_map, self._excitations ) rgb_features_map = self._normalize_to_rgb(rgb_features_map) return rgb_features_map Basic PRISM outputs RGB image according to last layer. To get accurate output we do upsampling the output to original image size. With upsampling it is called Gradual Extrapolated PRISM.\nGradual Extrapolation is based on the concept that a map considers the size of the preceding layer. This result is then multiplied by a matrix denoting the weights of the contributions from the current layer. When used on PRISM, this approach generates a sharp heat map focused on an object instead of the area where the object is present.\ndef _upsampling(self, extracted_features, pre_excitations): for e in pre_excitations[::-1]: extracted_features = interpolate( extracted_features, size=(e.shape[2], e.shape[3]), mode=\u0026#34;bilinear\u0026#34;, align_corners=False, ) extracted_features *= e.mean(dim=1, keepdim=True) return extracted_features To use Gradual Extrapolation PRISM set parameter grad_extrap=True (default True).\nHow to use # # load images into batch input_batch = load_images() prism = TorchPRISM() # choose your prefered model model = models.vgg11(weights=True).eval() prism.register_hooks(model) model(input_batch) prism_maps_batch = prism.prism() drawable_input_batch = input_batch.permute(0, 2, 3, 1).detach().cpu().numpy() drawable_prism_maps_batch = prism_maps_batch.permute(0, 2, 3, 1).detach().cpu().numpy() draw_input_n_prism(drawable_input_batch, drawable_prism_maps_batch) Code # You can find source code for this tutorial in this Colab Notebook.\nHere is the presentation for the tutorial in Google Slides.\nExamples # VGG11 # ResNet101 # GoogleNet # "},{"id":14,"href":"/docs/groups/xai_for_transformers/","title":"Xai for Transformers","section":"Docs","content":" XAI for Transformers. Explanations through LRP # Introduction # Transformers are becoming more and more common these days. But transformers are based on DNN that makes it harder to explain than other models. However, more and more ordinary users are starting to work with LLMs and to have more questions and doubts for its\u0026rsquo; work and decisions. Thus, there is a need for some explanation of Transformers. The method presented in the article \u0026ldquo;XAI for Transformers: Better Explanations through Conservative Propagation\u0026rdquo; by Ameen Ali et. al. serves this purpose.\nLRP method # Layer-wise Relevance Propagation method here are compared with Gradient×Input method presented in earlier article in this field.\nImg.1. Layer-wise Relevance Propagation principe The relevence in LRP is computing as $$R(x_i) = \\sum_{j} \\frac{\\delta y_j}{\\delta x_i} \\frac{x_i}{y_j} R(y_j)$$\nBut in some layers of transformer formulas look little different. For the attention-head layer and for normalization layers rules are look like\n\\[R(x_i)=\\sum_{j}\\frac{x_i p_ij}{\\sum_{i\u0026#39;} x_{i\u0026#39;} p_{i\u0026#39;j}}R(y_j) \\text{ and } R(x_i)=\\sum_{j}\\frac{x_i (\\delta_{ij} - \\frac{1}{N})}{\\sum_{i\u0026#39;} x_{i\u0026#39;} (\\delta_{i\u0026#39;j} - \\frac{1}{N})}R(y_j),\\] where \\(p_{ij}\\) is a gating term value from attention head and for the LayerNorm \\((\\delta_{ij} - \\frac{1}{N})\\) is the other way of writing the \u0026lsquo;centering matrix\u0026rsquo;, \\(N\\) is the number of tokens in the input sequence.\nBetter LRP Rules for Transformers # In practice authors observed that these rules do not need to be implemented explicitly. There are trick makes the method straightforward to implement, by adding detach() calls at the appropriate locations in the neural network code and then running standard Gradient×Input.\nSo improved rules will be \\[y_i = \\sum_i x_i[p_{ij}].detach()\\] for every attention head, and \\[y_i = \\frac{x_i - \\mathbb{E}[x]}{\\sqrt{\\varepsilon \u0026#43; Var[x]}}.detach()\\] for every LayerNorm, where \\( \\mathbb{E}\\) and \\(Var[x]\\) is mean and variance.\nResults # In the article different methods was tested on various datasets, but for now most inetersing is comparisom between old Gradient×Input (GI) method and new LRP methods with modifications in attention head rule (AH), LayerNorm (LN) or both (AH+LN).\nImg.2. AU-MSE (area under the mean squared error) The LRP with both modifications shows slightly better results in comparison with Gradient×Input method, but may make a huge difference in the future.\nThe results on SST-2 dataset that contains movie reviews and ratings are shown below. Both transformers was learned to classify review as positive or negative, and LRP shows slightly brighter and more concrete relevance values.\nReferences # [1] Ameen Ali et. al. “XAI for Transformers: Better Explanations through Conservative Propagation.” ICML, 2022\n[2] Hila Chefer et. al. “Transformer Interpretability Beyond Attention Visualization.” CVPR, 2021\nCode # All code for Transformer you can find in https://github.com/AmeenAli/XAI_Transformers\n"}]